[
  {
    "objectID": "posts/2021-02-17-canon.html",
    "href": "posts/2021-02-17-canon.html",
    "title": "Canon",
    "section": "",
    "text": "A collection of my favourite essays, posts, courses on data & analytics.\n\n\n\n\n\n\n\n\nModern data engineering teams are increasingly involved in selecting tools, integrating them, and keeping costs in check.\nThe industry is doubling down on templated SQL and YAML as a way to manage the “T” in ELT. > it feels like what early PHP was to web development. Early PHP would essentially be expressed as PHP code snippets living inside HTML files, and led to suboptimal patterns. It was clearly a great way to inject some more dynamisity into static web pages, but the pattern was breaking down as pages and apps became more dynamic. Similarly, in SQL+jinja, things increasingly break down as there’s more need for complex logic or more abstracted forms. It falls short on some of the promises of a richer programming model. > > I’ve been very interested in higher level abstractions sitting over the transform layer - namely “computation frameworks” or “parametric pipelines” as pieces of reusable data engineering machinery that can perform complex tasks. It’s pretty clear to me that combining SQL with Jinja templating doesn’t provide the proper foundation for these emerging constructs.\nThe metrics layer (popularized by Airbnb’s Minerva, Transform.co, and MetriQL), feature engineering frameworks (closer to MLops), A/B testing frameworks, and a cambrian explosion of homegrown computation frameworks of all shapes and flavors. Call this “data middleware”, “parametric pipelining” or “computation framework”, but this area is starting to take shape.\nWhether you call it data mesh or more generally “decentralized governance”, teams of domain experts are starting to own and drive data systems. Each team will start to be responsible for data quality SLA’s and publishing metrics and dimensions for the rest of the organization to consume. It’s a huge mes(s/h)!\nEvery product is becoming a data product\n\n\n\n\n\nhttps://roundup.getdbt.com/\nhttps://benn.substack.com/\n\n\n\n\n\n\n\nPipelines can be considered as a virtual view of source data, containing all the necessary extractions and transformations. Interdependent pipelines can be thought of as a graph of materialised views.\n\nAs long as the input data and pipeline transformations (i.e. the pipeline code) are preserved, the output can always be recreated. The input data is primary; if lost, it cannot be replaced. The output data, along with any intermediate stages in the pipeline, are derivative; they can always be recreated from the primary data using the pipeline.\n\n\nAny time someone queries the output of the pipeline, it’s logically equivalent to them running the entire pipeline on the source data to get the output they’re looking for. Of course, data pipelines don’t work this way in practice. Hence, the typical real-world pipeline materializes its output, and often also several of the intermediate datasets required to produce that final output.\n\n\nTo update a materialized view, there are two high-level properties you typically care about: the update trigger, and the update granularity.\n\n\n\n\nDan Romero’s start-up canon"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2021-09-12-multivariate-calculus_01.html",
    "href": "posts/2021-09-12-multivariate-calculus_01.html",
    "title": "Building an intuition for Calculus",
    "section": "",
    "text": "Mathematical language is like any other. You can’t enjoy poetry until you understand a lot of vocabulary and grammar. Don’t shy away from the “boring, rote” work.\n\n\nThe science of change\n\nCalculus, like other forms of mathematics, is much more than a language; it’s also an incredibly powerful system of reasoning.\nIt lets us transform one equation into another by performing various symbolic operations on them, operations subject to certain rules. Those rules are deeply rooted in logic, so even though it might seem like we’re just shuffling symbols around, we’re actually constructing long chains of logical inference. The symbol shuffling is useful shorthand, a convenient way to build arguments too intricate to hold in our heads.\nThe truly radical and distinctive move of calculus is that it takes the divide-and-conquer strategy to its utmost extreme - all the way out to infinity. Instead of cutting a big problem into a handful of bite-size pieces, it keeps cutting and cutting relentlessly until the problem has been chopped and pulverized into its tiniest conceivable parts, leaving infinitely many of them. Once that’s done, it solves the original problem for all the tiny parts, which is usually a much easier task than solving the initial giant problem.\nThus, calculus proceeds in two phases: cutting and rebuilding.\n\nIn mathematical terms, the cutting process always involves infinitely fine subtraction, which is used to quantify the differences between the parts. Accordingly, this half of the subject is called differential calculus. (2) The reassembly process always involves infinite addition, which integrates the parts back into the original whole. This half of the subject is called integral calculus.\n\nThis strategy can be used on anything that we can imagine slicing endlessly. Such infinitely divisible things are called continua and are said to be continuous, from the Latin roots con (together with) and tenere (hold), meaning uninterrupted or holding together.\nWhy should the universe respect the workings of any kind of logic, let alone the kind of logic that we puny humans can muster? This is what Einstein marvelled at, when he wrote, “The eternal mystery of the world is its comprehensibility. And it’s what Eugene Wigner meant in his essay”On the Unreasonable Effectiveness of Mathematics in the Natural Sciences” when he wrote, “The miracle of the appropriateness of the language of mathematics for the formulation of the laws of physics is a wonderful gift which we neither understand nor deserve.”\nInfinite Powers – Steven Strogatz\n\n\n\n\nFunctions are a relationship between an input and an output.\nIf we want the temperate of a room at a given time, our inputs might be the x, y, z co-ordinates of the room and t to represent time. Our function would then return the temperature.\n\\[ T(x, y, z, t) = temp \\]\n\\(f(x) = x^2 + 3\\), here, \\(f\\) is a function of \\(x\\)\nWithout context, this can get tricky. Take the example below;\n\\[ f(x) = g(x) + h(x - a) \\]\nWe can assume assume \\(g, h, a\\) are not variables, otherwise we would have to write \\(f(x, g, h, a)\\).\nBut without context it’s not 100% clearly if \\(g\\) is a function acting on \\(x\\), are \\(h\\) and \\(a\\) constants, or is \\(h\\) also a function.\nSelecting a candidate function or hypothesis to model the world is the creative essence of science.\n\nCalculus is the study of describing the relationship between a function and the change in its variables.\n\n\n\n\n\nImagine that you place one end of a water hose into a swimming pool and turn the tap on at the other end. Water then pours into the pool at a constant rate, causing the volume of water in the pool to increase at a constant rate.\n\nWhile the swimming pool is still filling up with water, what would we expect the plot of the function of volume of water in the pool with respect to time to look like?\n\n{:width=“300px”}\nAs water flows in at a constant rate, the volume increases at a constant rate, so the graph is just a straight line.\n\n\nThe tea is left to cool down. The speed of cooling depends on the temperature of the tea: when it is hot it cools down quickly and as it gets colder it cools down more and more slowly, until it approaches room temperature.\n\nWhich of the following graphs could represent the temperature of that cup of tea with time?\n\n{:width=“300px”}\nNewton’s law of cooling. This is an exponential function\n\n\nRahul drops a ball from the top of a ladder into a pit of sand.\n\nWhen the ball is released it begins to accelerate towards the ground, getting faster and faster until it hits the sand and quickly becomes stationary again. What would a plot of the speed of the ball against time look like?\n\n{:width=“300px”}\nThe plot has three distinct regions. When the ball is falling it speeds up at at a constant rate of acceleration, then it suddenly decelerates and has zero speed in the sandpit.\n\n\nBags of flour labelled 1 kg from a supermarket are weighed. Most of the weights measured are very close to 1 kg, with some a little more and others a little less. Those which are further away from 1 kg are found less and less often, with almost no bags more than 100 g out.\n\n\n\nWhat might we expect the plot of frequency (i.e. how often a type of bag is found) against mass to look like?\n\n{:width=“300px”}\nThe weights can be approximated by a bell-curve, called the “Normal Distribution”.\n\n\nA mass is attached to a string and hung from the ceiling. It is then pulled away from its natural hanging position (called equilibrium) and released, so that it swings backwards and forwards. Let’s assume there is no air resistance, so that when the mass swings back it returns all the way back to where it was originally released. It completes a full swing, away and back, every 2 seconds.\n\nWhat is a reasonable plot for the displacement of the mass from equilibrium with respect to time?\n\n{:width=“300px”}\nThis is called a simple harmonic oscillator - that is, we model the movement of the pendulum through time as a simple sine wave, with some amplitude (determined by the maximum distance of the pendulum to the equilibrium point) and some frequency (determined by the period of the swing).\nThe pendulum takes 2 seconds to complete a full revolution, which can also be described as swinging at a frequency of 0.5 Hz.\n\n\n\n\n\nWe can calculate the gradient of a straight line by looking at how much the function \\(f(x)\\) changes, divided by the amount the variable (x) changes.\nThis is often referred to as “rise over run”.\n{:width=“300px”}\nA classic example is graph showing the speed of a car over time.\n{:width=“300px”}\nWe can see the graph starts with an initial speed of 0, it accelerates, slows, speeds up again before coming to a sharp stop.\nA constant speed would be represented as a flat line.\nAcceleration is a function of time in our example.\n\nAcceleration can be defined as the local gradient (the gradient at a single point) of a speed-time graph\n\nWe illustrate the gradient at a single point but drawing a tangent line. The line will be the same gradient of the curve at that point.\n{:width=“300px”}\nWe can plot an entirely new graph which shows us acceleration against time, rather than speed, bt recording the slop of these tangent lines at each point.\n{:width=“300px”}\nAn acceleration of 0 is equivalent a constant speed, so a flat line in the speed-time graph.\nSpeed is, \\(\\frac{distance}{time}\\)\nAcceleration is, \\(\\frac{distance}{time^2}\\)\nVelocity is the amount of distance travelled during the amount of time, \\(\\frac{s}{t}\\). Acceleration is a change of velocity over time \\(\\frac{\\Delta v}{\\Delta t}\\) (this is why we square time)\n\nWhat we’ve just done by eye is the essence of calculus.\nWe took a continuous function and described its slope at every point by constructing a new function, which is its derivative.\nWe can in principle plot the derivative of the acceleration function following the same procedure, where we simply take the slope of the acceleration function at every point. This is the rate of change of acceleration which we can also think of as being the second derivative of the speed, and it’s actually referred to as the jerk of the car.\n\n{:width=“300px”}\nWe can also follow the inverse procedure. Take our baseline speed function, and imagine what function this would have been the gradient of. This is the anti-derivative.\nIn our example this would be the distance of the car from its starting position, or displacement.\n{:width=“300px”}\n\nThe anti-derivative is closely related to the integral\n\nthe fundamental object of calculus corresponding to summing infinitesimal pieces to find the content of a continuous region [ref].\n\n\n\n\n\nEstimate the gradient of the tangent to the non-linear, green function at the point \\((4, 2)\\)\n\n{:width=“300px”}\nThe run goes from 0 to 6, the rise goes from 2 to 8. Therefore, rise over run is \\(\\frac{6}{6} = 1\\)\n\nIt’s possible to have a reasonable guess at what the derivative of a function will look like by considering regions of the function with different gradients.\n\nTake the following example, we can see that there are three types of behaviour we might see in the gradient of a smooth function.\n{:width=“300px”}\n\nOn the left the function is increasing, so we have a positive gradient\nAt the turning point the gradient will be exactly \\(0\\)\nOn the right the function is decreasing, so we will have a negative gradient\n\n{:width=“300px”}\n\nThe derivative starts positive in the “increasing region” and becomes negative in the “decreasing region”, passing through zero at the “turning point”\n\n\nWhat representation would best describe the following function?\n\n{:width=“300px”}\nWe can see there are three turning points, so the derivative will be 0 three times.\n{:width=“300px”}\n\nWhat representation would best describe the following function?\n\n{:width=“300px”}\nAgain, we can see there are three turning points, so the derivative will be 0 three times.\n{:width=“300px”}\n\nand (4) have the same derivative. Shifting a function up or down does not change the gradient.\nReverse problem. What is the anti-derivative of the function below?\n\n{:width=“300px”}\nWe can see there are four points at which the derivative is 0, so there are four turning points.\nBoth these images could be represented by the derivative plot above.\n\nIf one function is a vertical shift of another function, then they have the same differential.\n\n{:width=“500”}\n\n\n\n\nRise over run is more formally expressed as, “the gradient of a line is equal to the amount of that function changes in this interval, divided by the length of the interval we’re considering”.\n\\[ gradient = \\frac{rise}{run} \\]\nThis helps us built the intuition that\n\nflat horizontal lines have a gradient of 0\nupward sloping lines have a positive gradient\ndownward sloping lines have a negative gradient\n\nA linear function will have a flat gradient, because no matter the interval (run) we’re considering the slope is the same. However, for more complex functions, where the slope is constantly changing, i.e. the gradient is different at different points, the rise over run depends on where we choose our points.\n{:width=“500”}\nImportant to note. For continuous functions, as delta x \\(\\Delta x\\) gets smaller and smaller it becomes a better approximation for the gradient at point \\(x\\).\nThis goes back to what Storgatz called the Infinity Principle.\n\nTo analyze something complicated, the Infinity Principle says you should first break it down into an infinity of simpler parts and analyze those. Putting those infinitely many analysed parts back together into an analysed whole can be difficult, but it can be easier than analysing the complicated whole directly. [ref]\n\nDelta x is infinitely small but non-zero.\nWe can express this concept formally, by using the limit notation scheme:\n\nas delta goes to zero, our expression will give us a function for our gradient at any point we choose\n\nDepending on the notation schema you prefer you can write f dash or f prime: \\(f'\\) or df by dx: \\(\\frac{df}{dx}\\)\n{:width=“500”}\nThis is what it means to differentiate a function. You substitute the function into this expression.\n\\[ f'(x) = lim_{\\Delta x \\to 0} \\left(\\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\\right)\\]\nDifferentiation is the process of finding a derivative. ### In practice, linear\nWe know a linear function will give us a constant derivative, so lets start there.\nOur linear function: $ f(x) = 3x + 2 $\nSubstitute in:\n\\[ f'(x) = lim_{\\Delta x \\to 0} \\left(\\frac{3(x + \\Delta x) +2 - (3x + 2)}{\\Delta x}\\right)\\]\nDo some algebra;\n\nstart by expanding the brackets\n\n\\[ lim_{\\Delta x \\to 0} \\left(\\frac{3x + 3\\Delta x +2 - 3x - 2}{\\Delta x}\\right)\\]\n\\(3x\\) and \\(-3x\\) cancel each other out, as do the \\(2\\) and \\(-2\\)\n\\[ lim_{\\Delta x \\to 0} \\left(\\frac{3\\Delta x}{\\Delta x}\\right)\\]\nThe \\(\\Delta x\\) themselves cancel out, so we end up with a limit of 3;\n\\[ lim_{\\Delta x \\to 0} (3)\\]\nBecause all the \\(\\Delta x\\) terms have been removed (solved for), the limit expression has no effect, so we can just ignore it.\nThe gradient of our linear function is a constant.\nWe know that gradient of a linear function \\(ax + b\\) is just \\(a\\) and now we see why.\n\n\nIn the example above, we differentiated \\(3x\\) and \\(+2\\) as the same time. We could do this things separately and then added the results.\n\nThis interchangeability of the approach is called The Sum Rule\n\n\\[ \\frac{d}{dx} \\left(f(x) + g(x)\\right) = \\frac{df(x)}{dx} + \\frac{dg(x)}{dx} \\]\nRemember \\(\\frac{df(x)}{dx}\\) is \\(\\Delta x\\)\n\n\n\nTake the function \\(f(x) = 5x^2\\)\nNote Remember the square of a sum\n\\[ (x + \\Delta x)^2 = x^2 + 2x\\Delta x + (\\Delta x)^2\\]\nSubstitute in:\n\\[ lim_{\\Delta x \\to 0} \\left(\\frac{5(x + \\Delta x)^2 - (5x^2)}{\\Delta x}\\right)\\]\nBecomes\n\\[ lim_{\\Delta x \\to 0} \\left(\\frac{5x^2 + 10x\\Delta x + 5\\Delta x^2 - 5x^2}{\\Delta x}\\right)\\]\n\n\\(5x^2\\) and \\(-5x^2\\) cancel out\nremove a \\(\\Delta x\\) from both top terms\n\n\\[ lim_{\\Delta x \\to 0} \\left(10x + 5\\Delta x\\right)\\]\nBecause \\(\\Delta x\\) is going to become infinitesimally small, i.e. going to 0, we can forget about it, so we get \\(10x\\)\n\\[ f'(x) = lim_{\\Delta x \\to 0} (10x) \\]\n__The derivative of the function \\(5x^2\\) is \\(10x\\)\n\n\n\nThe power rule allows us to differentiate polynomials quickly\nIf \\(f(x) = ax^b\\)\nThen \\(f'(x) = abx^{b-1}\\)\nIn our example above, \\((5*2)^1 = 10\\)\n{:width=“300”}\nRules like the power rule and the sum rule help speed up the process of differentiation. There are many of these in calculus.\n\n\n\nSpecial case functions that give interesting results when differentiated.\n\n\n\n\nThe gradient of this function is negative everywhere, except at the point, x = 0.\nWe can’t divide by 0 so we can’t see this point.\nAs we move right along the graph we see there is a break in our continuous function. \\(f(x)\\) drops towards negative infinity and then re-emerges on the positive side.\nThis sudden break is what is known as a discontinuity. This function doesn’t have a value at the point \\(x=0\\).\nWhat about the gradient? Let’s differentiate our function to find out;\n\\[ f'(x) = lim_{\\Delta x \\to 0} \\left(\\frac{\\frac{1}{x + \\Delta x} - \\frac{1}{x}}{\\Delta x}\\right) \\]\n\nFirst we need to combine our numerator into a single fraction, making the denominator of each the same.\n\n\\[ lim_{\\Delta x \\to 0} \\left(\\frac{\\frac{x}{x(x + \\Delta x)} - \\frac{x + \\Delta x}{x(x + \\Delta x)}}{\\Delta x}\\right) \\]\n\nThe \\(x\\) and \\(-x\\) on our new numerator cancel out\n\n\\[ lim_{\\Delta x \\to 0} \\left(\\frac{\\frac{-\\Delta x}{x(x + \\Delta x)}}{\\Delta x}\\right) \\]\n\nThe \\(-\\Delta x\\) and \\(\\Delta x\\) cancel out, giving us\n\n\\[ lim_{\\Delta x \\to 0} \\left(\\frac{-1}{x(x + \\Delta x)}\\right) \\]\n\\[ lim_{\\Delta x \\to 0} \\left(\\frac{-1}{x^2 + x \\Delta x}\\right) \\]\nWe now have the magic of limits to remove the term \\(x \\Delta x\\), leaving us with;\n\\[ \\frac{-1}{x^2} \\]\n\nThis derivative function is negative everywhere and undefined at \\(x = 0\\)\n\n\n\nIn this function, the value of the function is always equal to its gradient.\n\\[ f(x) = f'(x) \\]\nThis is also true for \\(f(x) = 0\\)\nOur function must always be negative or always be negative. Crossing the horizontal function would cause \\(f'(x)\\) to be 0. This property of always decreasing or always increasing also means it can never return to the same value again.\nPlenty of functions would fit this in the positive direction. But only Euler’s number \\(e\\), works in both positive and negative direction.\n\\(e = 2.71828...\\)\n\\[ f(x) = e^x = f'(x) \\]\n\\(e\\), like \\(\\pi\\) appears to be written all over the fabric of the universe.\n\nAs differentiating e to the x gives us e to the x, clearly, we can just keep differentiating this thing as many times as we’d like and nothing is going to change. This self similarity is going to come in very handy.\n\n\n\n\n\n\nYou may recall that for a right angled triangle, sine of angle x multiplied by the hypotenuse r gives you the length of the opposite side to the angle.\n\n\n\\(Cosine(x)\\) is the derivative of \\(Sin(x)\\)\n\nIf we differentiate \\(cos(x)\\) we get \\(-sin(x)\\)\n\nDifferentiating \\(sin(x)\\) four times, gives us \\(sin(x)\\)\n\nThe pattern repeats, again giving us a property of self-similarity.\nThat is because trigonometric functions are exponential functions in disguise.\n\\[ sin(x) = \\frac{e^{ix} - e^{-ix}}{2i} \\]\nDifferentiation may feel complicated at times, but the concept is simple. You are looking for the rise over run gradient at each point.\n\n\n\n\n(1) Using the power rule, \\(\\frac{d}{dx} \\left(ax^b\\right) = abx^{b-1}\\)\nDifferentiate \\(f(x) = x^{173}\\)\n\\(f'(x) = 173x^{172}\\)\n(2) Using the sum rule, \\(\\frac{d}{dx}\\left[f(x) + g(x)\\right] = \\frac{df(x)}{dx} + \\frac{gf(x)}{dx}\\)\nDifferentiate \\(f(x) = x^2 + 7 + \\frac{1}{x}\\)\nWe know\n\n\\(x^2\\) becomes \\(2x\\),\n\\(7\\) is a constant so we ignore it (remember the gradient of a line function \\(x = ab + c\\), is \\(a\\)),\nand \\(\\frac{1}{x}\\) we’ve been given\n\n\\(f'(x) = 2x - \\frac{1}{x^2}\\)\n(3) Find the second derivative of \\(f(x) = e^x + 2sin(x) + x^3\\)\nWe know\n\n\\(f''(x)\\) of \\(e^x\\) is \\(e^x\\)\n\\(f'(x)\\) of \\(sin(x)\\) is \\(cos(x)\\) and \\(f'(x)\\) of that is \\(-sin(x)\\)\n\\(x^3\\) we can use the power rule: \\(x^3 \\to 3x^2 \\to 6x\\)\n\n\\(f''(x) = e^x + -2sin(x) + 6x\\)\n(4) To find the anti-derivative we ask what function you’d need to differentiate to get \\(f'(x)\\). Consider the power rule in reverse. The anti-derivative of \\(abx^{b-1} is ax^b\\)\nWhat is the anti-derivative of \\(f'(x) = x^4 - sin(x) - 3e^x\\)?\n\n\\(x^4\\) becomes \\(\\frac{1}{5} x ^ 5\\),\n\ntip: 5 * 0.2 = 1\n\n\\(-sin(x)\\) becomes \\(cos(x)\\)\n\\(3e^x\\) is \\(3e^x\\)\n\n$ f(x) = x ^ 5 + cos(x) - 3e^x + c$\nWhen calculating anti-derivatives we can add any constant, since the derivative of a constant is zero.\n(5) The power rule can be applied for any real value of \\(b\\)\nUsing the facts that \\(\\sqrt{x} = x^{\\frac{1}{2}}\\) and \\(x^{-a} = \\frac{1}{x^a}\\)\n\nPower rule: multiply by the power, then reduce the power by one\n\nCalculate \\(\\frac{d}{dx}\\left(\\sqrt{x}\\right)\\)\n\nRewrite: \\((x^{\\frac{1}{2}})\\)\nPower law: \\((\\frac{1}{2}x^{-\\frac{1}{2}})\\)\nSecond fact given: \\((\\frac{1}{2}\\frac{1}{x^\\frac{1}{2}})\\)\nBecomes: \\((\\frac{1}{2}\\frac{1}{\\sqrt{x}})\\)\nBecomes: \\((\\frac{1}{2\\sqrt{x}})\\)\n\n\\(\\frac{d}{dx}\\left(\\sqrt{x}\\right) = \\frac{1}{2\\sqrt{x}}\\)\n\n\n\n\n\nA convenient shortcut for differentiating the product of two functions.\nSay we have a rectangle. The length of one side is the function \\(f(x)\\) and the other \\(g(x)\\). The product of these two function with give us the rectangle’s area, \\(A(x) = f(x)g(x)\\).\nIf we differentiate \\(A(x)\\), what we’re looking for is the change in area of our rectangle as we vary \\(x\\).\n\nWhat is the derivative of A with respect to x?\n\n{:width=“500”}\nNote: In the example chosen, both our functions increase with \\(x\\). This does not need to be the case for the product rule to be applied.\n\\(\\Delta A(x)\\) can be viewed as the sum of three triangles, each representing the change in height, width and diagonal\n{:width=“500”}\nAs \\(\\Delta x\\) goes to 0, it is the smallest rectangle that will shrink the fastest. We simply ignore its contribution to the area.\nOur rise over run, differentiation calculation is the limit of \\(\\frac{\\Delta A(x)}{\\Delta x}\\)\n\\[ lim_{\\Delta x \\to 0} \\left(\\frac{\\Delta A(x)}{\\Delta x}\\right) = lim_{\\Delta x \\to 0} \\left(\\frac{f(x)(g(x + \\Delta x) - g(x)) + g(x)(f(x + \\Delta x) - f(x))}{\\Delta x}\\right) \\]\nWe can rearrange this equation. (1) Splitting into two fractions and (2) Removing \\(f(x)\\) and \\(g(x)\\) out of the numerators.\n\\[ lim_{\\Delta x \\to 0} \\left(\\frac{\\Delta A(x)}{\\Delta x}\\right) = lim_{\\Delta x \\to 0} \\left(f(x)\\frac{(g(x + \\Delta x) - g(x))}{\\Delta x} + g(x)\\frac{(f(x + \\Delta x) - f(x))}{\\Delta x}\\right) \\]\nThis first fraction is now the derivative of \\(g(x)\\) and the second is the derivative of \\(f(x)\\)\n\\[ A'(x) = f(x)g'(x) + g(x)f'(x) \\]\n{:width=“500”}\n{:width=“300”}\n\n\n(1) Write the product rule \\(A'(x) = f'(x)g(x) + f(x)g'(x)\\) in \\(\\frac{d}{dx}\\) notation\n\\[ \\frac{dA(x)}{dx} = \\frac{df(x)}{dx} g(x) + f(x) \\frac{dg(x)}{dx}\\]\n(2) It can be useful to rewrite a function \\(f(x)\\) into two parts so that we can apply the product rule and differentiate more easily.\n\\(A(x) = (x + 2)(3x - 3)\\) can be thought of as \\(f(x) = (x + 2) and g(x) = (3x - 3)\\)\nRemember we can ignore constants, so it makes it easy to differentiate\nTherefore \\(A'(x) = 3(x + 2) + (3x - 3) = 6x + 3\\)\n(3) Combining previous rules and our knowledge of our trigonometric functions differentiate it we can easily differentiate the function \\(f(x) = x^3sin(x)\\)\n\\(f'(x) = 3x^2sin(x) + x^3cos(x)\\)\n(4) Differentiate \\(f(x) = \\frac{e^x}{x}\\)\nNote \\(\\frac{e^x}{x} = e^x\\frac{1}{x}\\) so can use the product rule\n\\(f'(x) = e^x (-\\frac{1}{x^2}) + e^x \\frac{1}{x} = e^x \\left(\\frac{1}{x} - \\frac{1}{x^2}\\right)\\)\n(5) We can extend the product rule to the products of more than two functions\n\nConsider the function \\(u(x) = f(x)g(x)h(x)\\)\nThen use the product rule twice to find the expression for \\(u'(x)\\)\nThis is the product rule for a product of three functions\n\nStart by substituting A(x) = f(x)g(x)\n\\(u'(x) = A(x)h'(x) + h(x)A'(x)\\)\nNow substitute out (A)\n\\(u'(x) = f(x)g(x)h'(x) + h(x)\\left(f(x)g'(x) + g(x)f'(x)\\right)\\)\nMultiply out \\(h(x)A'(x)\\) on the right hand-side\n\\(u'(x) = f(x)g(x)h'(x) + h(x)(f(x)g'(x) + h(x)g(x)f'(x)\\)\n(6) Now differentiate a function that is the product of three functions: \\(f(x) = x e^x cos(x)\\)\nSubstitute \\(t(x) = x e^x\\)\n\\(f'(x) = t(x)-sin(x) + cos(x)t'(x)\\)\n\\(t'(x) = (e^x x+1) + (e^x x)\\) – note the derivative of \\(x\\) is 1\n\\(f'(x) = e^x[(x + 1) cos(x) + x sin(x)]\\)\n\n\n\n\nFunctions can be used as the input to other functions.\nConsider the nested (or composite) function, h of p of m \\(h(p(m))\\)\nWe are relating the concept of \\(m\\) to \\(h\\) via the concept of \\(p\\). We are chaining concepts together.\nIn our example, let’s say \\(h\\) is happiness, which is a function of the number of pizzas we eat \\(p\\), which itself is a function of the money we have \\(m\\).\n\\[h(p) = -\\frac{1}{3}p^2 + p + \\frac{1}{5}\\]\nThis may look daunting but can be plotted as;\n{:width=“300”}\nWithout any pizza it is still possible to be happy. There is a point where some amount of pizza peaks our happiness, then it decreases until the amount of pizza consumed begins to negatively effect our happiness.\nPizza and money has a straight forward exponential relationship.\n\\[ p(m) = e^m - 1\\]\n\nWhat we’d like to know is, by considering how much money I have now, how much effort should I put into making more, if my aim is to be happy?\nTo work this out, we’re going to need to know what the rate of change of happiness is, with respect to money,\n\nThis is $ $.\nWe could directly substitute these two functions in:\n\\[ h(p(m)) = -\\frac{1}{3}(e^m - 1)^2 + (e^m - 1) + \\frac{1}{5}\\]\nThen we can differentiate this directly to\n\\[ \\frac{dh}{dm} = \\frac{1}{3}e^m(5 - 2e^m)\\]\nThe chain rules allows us to handle more complex examples where direct substitution may not be feasible.\nConsider the derivative of \\(h\\) with respect to \\(p\\), \\(\\frac{dh}{dp}\\), and of \\(p\\) with respect to \\(m\\) \\(\\frac{dp}{dm}\\).\nIn this notation convention, the derivatives are represented as quotients (a result obtained by dividing one quantity by another). The product of these two will give you the desired function \\(\\frac{dh}{dm}\\)\n\\[ \\frac{dh}{dp} * \\frac{dp}{dm} = \\frac{dh}{dm}\\]\n{:width=“300”}\nTake our example.\nWe don’t want \\(p\\) to appear in our final output, so we substitute \\(p\\) in terms of \\(m\\)\n{:width=“500”}\nWe can see the benefit of getting more money (the dotted white line) decreases dramatically, once you have enough pizza.\n{:width=“500”}\n\nWhat’s magic about the chain rule is that for some real-world applications, we may not have a nice analytical expression for our function. But we may still have the derivatives. So, being able to simply combine them with the chain rule becomes very powerful, indeed.\n\nCommon mistake: Not recognising whether a function is composite or not.\nFor example: \\(cos^2(x)\\) is shorthand for \\([cos(x)]^2\\)\n\nDescribed verbally, if \\(f(x) = g(h(x))\\)\nthe rules says that the derivative of the composite function \\(f'(x)\\), is - the inner function \\(h\\) within the derivative of the outer function \\(g'\\)\n\\(f'(x) = g'(h(x)) * h'(x)\\)\n\n\n\nThe chain rule allows us to differentiate functions of functions.\n(1) If \\(f(x) = g(h(x))\\) express \\(f'(x)\\) in terms of the chain rule notation\n\\[ \\frac{dg}{dx} = \\frac{dg}{dh} \\frac{dh}{dx} = f'(x) = g'(h(x)) \\cdot h'(x)\\]\n(2) Much like the product rule, the art of the chain rule lies in identifying the components of the function that allow you to apply the rule.\nConsider the function \\(f(x) = e^{x^2 - 3}\\)\nThis can be broken down to, \\(f(x) = g(h(x))\\),\nWhere\n\n\\(g(h) = e^h \\to g'(h) = e^{x^2-3}\\)\n\\(h(x) = x^2-3 \\to h'(x) = 2x\\)\n\n\\[ f'(x) = \\frac{dg}{dh} \\frac{dh}{dx} = 2xe^{x^2 - 3} \\]\n(3) Following the same process, given \\(f(x) = sin^3(x)\\)\nThis is shorthand for \\([sin(x)]^3\\)\n\\(g(h) = h^3 \\to g'(h) = 3sin^2(x)\\)\n\\(h(x) = sin(x) \\to h'(x) = cos(x)\\)\n\\(f'(x) = 3sin^2(x)cos(x)\\)\n(4) Calculate the derivative of \\(tan(x)\\) with respect to \\(x\\)\nNote\n\n\\(tan(x) = \\frac{sin(x)}{cos(x)}\\)\n\\(\\frac{1}{cos(x)} = [cos(x)]^{-1}\\)\n\n\\(1 + tan^2(x) = \\frac{1}{cos^2(x)}\\)\n\\[\\frac{d}{dx} tan(x) = 1 + tan^2(x)\\]\n(5) The chain rule can also be applied to functions of functions of functions.\nConsider a function \\(f(g(h(x)))\\)\nBy applying the chain rule twice, it’s possible to show that \\(\\frac{df}{dx} = \\frac{df}{dg}\\frac{dg}{dh}\\frac{dh}{dx}\\)\nUse this to find the derivative of \\(f(x) = e^{sin(x^2)}\\)\n\n\\(f(g) = e^g \\to f'(g) = e^{sin(x^2)}\\)\n\\(g(h) = sin^h \\to g'(h) = cos(x^2)\\)\n\\(h(x) = x^2 \\to h'(x) = 2x\\)\n\n\\[ f'(x) = 2xe^{sin(x^2)} cos(x^2)\\]\n\n\n\n\n\nIn this example, we’ll use a complex formula\n\\[ f(x) = \\frac{sin(2x^5 + 3x)}{e^{7x}} \\]\nWe need to break this down into manageable pieces that allow us to apply the rules we have learnt.\n\nThough expressed as a fraction, we can rewrite our function as a product, by raising the denominator to the power of -1\n\n\\[ f(x) = (sin(2x^5 + 3x))e^{-7x} \\]\nWe can now split \\(f(x)\\) into the two parts \\(g(x)\\) and \\(h(x)\\)\n\\[ g(x) = sin(2x^5 + 3x) \\]\nand\n\\[ h(x) = e^{-7x}\\]\nLet’s find an expression for \\(g'(x)\\):\nHere we have the trigonometric function \\(sin\\) applied to a polynomial. This is a classic target for the chain rule.\n\\[ g(u) = sin(u) \\to g'(u) = cos(u) \\]\n\\[ u(x) = 2x^5 + 3x\\to u'(x) = 10x^4 + 3 \\]\n\\[ \\frac{dg}{du} \\frac{du}{dx} = cos(u)(10x^4 + 3)\\]\nWe want a final expression that doesn’t include a \\(u\\).\nThe two \\(\\frac{du}{du}\\)s cancel each other out and substitute out \\(u\\).\n\\[ \\frac{dg}{dx} = cos(2x^5 + 3x)(10x^4 + 3) \\]\nLet’s find an expression for \\(h'(x)\\):\n\\[ h(v) = e^v \\to h'(v) = e^v\\]\n\\[ v(x) = -7x \\to v'(x) = -7\\]\n\\[ \\frac{dh}{dv} \\frac{dv}{dx} = -7e^{-7x}\\]\nSo we have\n\\[ f'(x) = (g'(x) * h) + (h'(x) * g) \\]\nWhere\n\\[ f'(x) = (cos(2x^5 + 3x)(10x^4 + 3)e^{-7x}) +  ((-7e^{-7x})sin(2x^5 + 3x)) \\]\nThis can be rearranged and factorised\n\\[ f'(x) = e^{-7x} [(10x^4 + 3) cos(2x^5 + 3x) - 7sin(2x^5 + 3x)] \\]\nOr expressed in terms of the original function\n\\[ f'(x) = \\frac{(10x^4 + 3) cos(2x^5 + 3x)}{e^{-7x}} - 7f(x)\\]\nPremature optimisation is the root of all evil\n\nDon’t spend time tidying things up and rearranging them until that you’re sure that you’ve finished making a mess.\n\n\n\n(1) \\(f(x) = x^{3/2} + \\pi x ^2 + \\sqrt{7}\\)\nWhat is the derivative of \\(f(x)\\) at the point \\(x = 2\\)?\n\n\\(x^{3/2} \\to \\frac{3}{2} x^{\\frac{1}{2}} = \\frac{3\\sqrt{2}}{2}\\)\n\\(\\pi x^2 \\to 2\\pi \\cdot 2 = 4\\pi\\)\n\n\\(f'(x) = \\frac{3\\sqrt{2}}{2} + 4\\pi\\)\n(2) What is the derivative of \\(f(x) = x^3cos(x)e^x\\)?\nNote product rule of three terms is \\(f'(x) g(x) h(x) + f(x) g'(x) h(x) + f(x) g(x) h'(x)\\)\n\n\\(3x^2 cos(x) e^x = 3e^xx^2cos(x)\\)\n\\(-sin(x) x^3 e^x\\)\n\\(x^3 cos(x) e^x\\)\n\n\\(f'(x) = 3e^xx^2cos(x) -sin(x) x^3 e^x + x^3 cos(x) e^x\\)\n(3) What is the derivative of the function \\(f(x) = e^{[(x+1)^2]}\\)?\n\n\\(g(h) = e^h\\)\n\\(h(x) = (x + 1)^2\\)\n\n\\(f'(x) = e^{[(x+1)^2]} \\cdot 2(x + 1)\\)\n(4) What is the derivative of the function \\(f(x) = x^2cos(x^3)\\)?\nProduct rule and chain rule on the second term\nTerm one\n\n\\(2x cos(x^3)\\)\n\nTerm two, first apply the chain rule\n\n\\(g(h) = cos(h) \\to -sin(x^3)\\)\n\\(h(x) = x^3 \\to 3x + 2\\)\n\\(-sin(x^3) \\cdot 3x + 2\\)\n\nApply the product rule\n\n\\(-sin(x^3) \\cdot 3x + 2 \\cdot 2x\\)\n\nTODO: check $ f’(x) = 2x cos(x^3) - sin(x^3) 3x + 2 2x$\n$ f’(x) = 2x cos(x^3) - 3x^4 sin(x^3)$\n(5) What is the derivative of the function \\(f(x) = sin(x)e^{cos(x)}\\) at the point \\(x = \\pi\\)\nTerm one, product rule\n\n\\(cos(x)e^{cos(x)}\\)\nnote \\(cos(\\pi) = -1\\)\n\\(-1e^{-1}\\)\n\nTerm two, first apply chain rule - \\(g(h) = e^h\\) - \\(h(x) = cos(x)\\) - \\(e^{cos(x)}-sin(x)\\)\nApply the product rule\n\n\\(e^{cos(x)}-sin(x) \\cdot sin(x) = e^{-1}-sin(x) \\cdot sin(x)\\)\n\nBring both terms together\n\\(-1e^{-1} + e^{-1}-sin(x) \\cdot sin(x)\\)\n\nnote \\(sin(\\pi) = 0\\)\n\nSo becomes: \\(-1e^{-1} = -\\frac{1}{e}\\)"
  },
  {
    "objectID": "posts/2021-02-20-linear-algebra.html",
    "href": "posts/2021-02-20-linear-algebra.html",
    "title": "An introduction to Linear Algebra",
    "section": "",
    "text": "Algebra is the generic term for the maths of equations in which numbers and operations are written as symbols.\n\nRené Descarte’s La Geometrie first introduced standard algebraic notation. When the book was being printed, the printer started to run out of letters. When asked if it mattered if x, y, or z was used to represent the unknowns, Descarte said it did not. As a result, x became fixed in maths - and the wider culture - as the symbol for the unknown quantity. - Alex Bellos, Numberland\n\nThe beauty of applied mathematics is that is gives us a new layer of abstraction. In the case of linear algebra that is a language to represent data and manipulate space. In many of the examples, we’ll use a geometric lens, i.e. using data to describe a 2D space (an arrow). But mathematics allows us to take these learnings and generalise to \\(n\\) number of dimensions and our data can describe not just space but objects.\nWe’ll look at building an intuition around the concepts and doing some basic work in linear algebra, before passing off the difficult stuff to computers. Though, we’ll be intentional about what we’re asking the computer to do.\nLinear algebra has evolved to a more general and abstract form, representing vector spaces. Vectors are used to described points in some finite-dimensional space. An n dimensional space will be described by a vector with n coordinates. In a 2D world this will be an arrow, with x, and y coordinates, the vector describes will be an arrow pointing to B.\nLinearity is the property of a mathematical relationship (function) that can be graphically represented as a straight line - Wikipedia. This statement will make sense when we start to do operations with vectors. But we’ll see how adding and scaling manipulating the size of vectors is done by moving in a straight line.\nIt’s important to note that linear algebra represents much of the trunk of knowledge required for modern data analysis and many machine learning techniques. Often the use of linear algebra will be implicit, you’ll forget you’re even using it. Think of it as a prerequisite, a string to your bow, which doesn’t get the same attention as it’s sexy cousins statistics and probability, but is just as essential.\nTake the time to refresh your memory or grasp the concepts here.\n\n\nVectors are the building blocks of linear algebra.\nThink of a vector as an object that moves us about space (physical or data), it could just be a list of attributes of an object.\nDepending on your world view, what exactly you deem to be a vector will vary. Vectors can be considered;\n\nVectors are arrows pointing in space\nWhat defines a vector is its length and the direction it’s pointing. You can move it anywhere in space, and it’s the same vector.\nA vector on the flat plane are two-dimensional, those in broader space at three-dimensional.\nVectors are a list of numbers\nVectors are usually viewed by computers as an ordered list of numbers which they can perform “operations” on - such as multiplying by scalars (numbers) to form new vectors.\nThe numbers in vectors represent data about an object. The number of dimensions is determined by the length of the vector.\n\nAt the start of this learning material, we’ll focus on applying a geometric lens to vectors. That is, think of them as an arrow describing dimensions of space, according to some co-ordinate system.\n\nVectors can be thought of in a variety of different ways - some geometrically, some algebraically, some numerically. In this way, there are a lot of techniques one can use to deal with vectors.\n\nConcretely, vectors are often a useful way to represent data.\n\n\n\nAddition \\((r + s == s + r)\\) -> vector addition is associative\n\nConceptually you add vectors by moving the tail origin of one vector to the tip of the other. Then drawing a new vector from the tail of the first to the tip of where the second now sits.\nYou’re adding the \\(x\\) and \\(y\\) components together.\n\\(r + s = [r_x + s_x, r_y + s_y]\\)\n{:width=“300px”} \nIn the example above \\(r = \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix}\\), \\(s = \\begin{bmatrix} 3 \\\\ -1\\end{bmatrix}\\) we can arrive at \\(t\\) geometrically, taking 1 step the right, 2 up, then 3 to the right, and one down.\nWe add the component parts (component wise), as add the \\(x\\) coordinates and the \\(y\\) coordinates to get our new vector.\n\nMultiplication by a scalar (i.e. number, because numbers scale vectors, so we use the terms interchangeably) negative numbers means reverse\n\n{:width=“250px”} \nWe multiple each component in the vector by that scalar.\n\\(r = \\begin{bmatrix} i \\\\ j\\end{bmatrix}\\) \\(3r = \\begin{bmatrix} 3i \\\\ 3j\\end{bmatrix}\\)\nVectors give us a language of space and the ability to manipulate space. They allow us to represent lots of lists of data together.\nTies to Machine Learning\nOne of the tasks of machine learning is to fit a model to data in order to represent the underlying distribution.\nA model allows us to predict the data in a distribution.\nWe can start with a parameter vector \\(\\mathbf{p}\\) and convert it to a vector of expected frequencies \\(\\mathbf{g}_\\mathbf{p}\\)\nWe need a way fit a model’s parameters to data and quantify how good that fit is.\nOne way of doing so is to calculate the “residuals”, which is the difference between the measured data and the modelled prediction for each histogram bin.\nA better fit would have as much overlap as it can, reducing the residuals as much as possible.\n{:width=“400px”}\nIn the example above, we’d improve the model in orange by decreasing or increasing \\(\\mu\\) (the height) and keeping \\(\\sigma\\) (the width) roughly the same.\nThe performance of a model can be quantified in a single number. One measure we can use is the Sum of Squared Residuals, \\(\\mathrm{SSR}\\). i.e. we square the difference for each value in actual and predicted, and add all those together;\n\\[ \\mathrm{SSR}_\\mathbf{p} = \\lVert \\mathbf{f} - \\mathbf{g}_\\mathbf{p} \\rVert ^2 \\]\n\n\nIn the below; orange is the observed, purple the predicts and green the overlap.\n\nμ = 160 ; σ = 6\np = [μ, σ]\nhistogram(p)\n{:width=“400px”}\nμ = 179 ; σ = 7\np = [μ, σ]\nhistogram(p)\n{:width=“400px”}\nSince each parameter vector \\(\\mathbf{p}\\) represents a different bell curve, each with its own value for the sum of squared residuals, \\(\\mathrm{SSR}\\), we can draw the surface of \\(\\mathrm{SSR}\\) values over the space spanned by \\(\\mathbf{p}\\), such as \\(\\mu\\) and \\(\\sigma\\) in this example.\nEvery point on this surface represents the SSR of a choice of parameters\n{:width=“400px”}\nWe can take a ‘top-down’ view of the surface, and view it as a contour map, where each of the contours (in green here) represent a constant value for the \\(\\mathrm{SSR}\\).\n{:width=“400px”}\nOften we can’t see the whole parameter space, so instead of just picking the lowest point, we have to make educated guesses where better points will be.\nWe can define another vector, \\(\\Delta\\mathbf{p}\\), in the same space as \\(\\mathbf{p}\\) that tells us what change can be made to \\(\\mathbf{p}\\) to get a better fit.\nFor example, a model with parameters \\(\\mathbf{p}'\\) = \\(\\mathbf{p}\\) + \\(\\Delta\\mathbf{p}\\) will produce a better fit to data, if we can find a suitable \\(\\Delta\\mathbf{p}\\).\n\nMoving at right angles to contour lines in the parameter space is the most effective way to move through the space. Moving along contonour lines has no effect. Moving perpendicular to them can significantly improve or reduce the quality of the fit.\nMoving along the contour lines does not produce the same model\n\nThe \\(\\Delta\\mathbf{p}\\) \\(\\begin{bmatrix}-2 \\\\ 2\\end{bmatrix}\\) will give the best improvement in the model below;\n{:width=“400px”}\n\\(\\mu\\) and \\(\\sigma\\) have to be different, decrease \\(\\mu\\) along the x-axis, increase \\(\\sigma\\) along the y-axis.\n\n\n\n\nSolving simultaneous equations is the process of finding the values of the variables (here \\(x\\) and \\(y\\)) that satisfy the system of equations.\nThe first goal when solving simple simultaneous equations should be to isolate one of the variables.\nFor example, using elimination, taking the second equation away from the first to solve the following pair of equations:\n\\(3x−2y=7\\)\n\\(2x - 2y = 2\\)\n\\(x = 5, y = 4\\)\nYou can use elimination even when the coefficients, the numbers in front of \\(x\\) and \\(y\\), aren’t the same.\nFor example, multiplying both sides of the first equation by 2, then solve using the same method as the last question.\n\\(3x−2y=4\\) \\(\\equiv\\) \\(6x - 4y = 8\\)\n\\(6x + 3y = 15\\)\n\\(x = 2, y = 1\\)\nA very similar technique can be used to find the inverse of a matrix.\nThere is also the substitution method, where we rearrange one of the equations to the form \\(x = ay+b\\) or \\(y = cx+d\\) and then substitute \\(x\\) or \\(y\\) into the other equation.\nSystems of simultaneous equations can have more than two unknown variables. Below there is a system with three; \\(x\\), \\(y\\) and \\(z\\).\nFirst try to find one of the variables by elimination or substitution, which will lead to two equations and two unknown variables.\nContinue the process to find all of the variables.\n\\(3x − 2y + z =7\\)\n\\(x + y + z = 2\\)\n\\(3x − 2y - z =3\\)\n\\(x = 1, y = -1, z = 2\\)\n\n\n\nThink of vectors within a co-ordinate system as mentioned previously.\nThink of the co-ordinates as scalars that describe but also allow us to manipulate vectors.\nThe basis vector for the \\(x\\) axis is typically \\(\\hat{i}\\) and \\(\\hat{j}\\) for the \\(y\\) axis.\nVector addition is associative we can do it component by component.\n\nFormally, what this means is that if we have three vectors, r, s and, t, it doesn’t matter whether we add r plus s and then add t, or whether we add r to s plus t, it doesn’t matter where we put the bracket.\n\nIn the image below \\(\\hat{i}\\) and \\(\\hat{j}\\) are our basis vectors, things that define the space. Vector addition can be thought of as combining two scaled vectors of \\(\\hat{i}\\) and \\(\\hat{j}\\)\n{:width=“400px”}\nMulitplication on vectors; \\(2r\\) is 2 * the components of \\(r\\)\n\\[r = \\begin{bmatrix}3 \\\\ 2\\end{bmatrix}\\]\n\\[2r = \\begin{bmatrix}2*3 \\\\ 2*2\\end{bmatrix} = \\begin{bmatrix}6 \\\\ 4\\end{bmatrix}\\]\nVector subtraction; minus \\(r\\) is not a shorter vector, that would be 0.5, rather it is in the oppositie direction. (Addition of negative one multiple the vector, ie vector + \\((s * -1)\\) -> this is a very handy way to think of subtraction)\n{:width=“400px”}\nThinking of vectors as representing attributes of objects\nWe don’t have to think of vectors in a geometric space.\nVecotrs can represent information about an object. In this example, each vector will hold data relating to a house. Each row or co-ordinate,represents a separate piece of information; square metres, the number of bedrooms, bathrooms, and the price of a house.\nOur vector operations still apply to these house objects, we can do operations on multiple houses, adding and multiplying (and introducing the concept of a negative house if there’s such a thing)\n{:width=“400px”}\nDoing operations with vectors\nWe have the following vectors;\n{:width=“400px”}\n\\(a\\) = \\(\\begin{bmatrix}2 \\\\ 2\\end{bmatrix}\\) \\(-b\\) = \\(\\begin{bmatrix}-1 \\\\ 2\\end{bmatrix}\\) \\(2c\\) = \\(\\begin{bmatrix}2 \\\\ 2\\end{bmatrix}\\) \\(d\\) = \\(\\begin{bmatrix}-1 \\\\ 2\\end{bmatrix}\\)\nCalculate\n\\(b + e\\) = \\(\\begin{bmatrix}-1 \\\\ -1\\end{bmatrix}\\)\n\\(d - b\\) = \\(\\begin{bmatrix}-2 \\\\ 4\\end{bmatrix}\\) -> think of it as \\(2d\\), you’re flipping \\(b\\) because it’s subtraction\n\n\n\nAddition and scaling by a number are two main vector operations and allow us define the mathematical properties that a vector has.\nWe can define a vector, say \\(r\\), without any reference to any coordinate system. We can treat it as a geometric object, with just two properties, it’s length (size) and its direction.\n{:width=“200px”}\nIf we wanted to calculated the size, or the length of \\(r\\), we could use a coordinate system with two axis that are orthogonal to each other.\n{:width=“200px”}\n\\(r = ai + bj\\) -> a number of i, and b number of j\nWe assume \\(i\\) and \\(j\\) are of length one, denoted by \\(\\hat{i}\\) \\(\\hat{j}\\)\nIn this case \\(r\\) could often be written as a column vector, ignoring i and j;\n\\(r = \\begin{bmatrix}a \\\\ b\\end{bmatrix}\\)\nLength of \\(r\\), also denoted as \\(\\lVert r \\rVert\\) is giving by the hypotenuse. Remember that thing? It’s the longest side of a right-angled triangle.\nThe size of a vector with two components is calculated using Pythagoras’ theorem\n\\(\\lVert r \\rVert = \\sqrt{a^2 + b^2}\\)\n{:width=“400px”}\nWe define the size of a vector through the sums of the squares of its components\nIn this case we’ve used two spatial directions (i and j) but it doesn’t matter if the different components of the vector are dimensions in space or even things that have different fiscal units like length, time and price.\nIn fact, this definition can be extended to any number of dimensions; the size of a vector is the square root of the sum of the squares of its components.\nThe size of vector \\(s = \\begin{bmatrix}1 \\\\ 3 \\\\ 4 \\\\2\\end{bmatrix}\\) is \\(\\lVert s \\rVert = \\sqrt{30}\\)\nThe size of a vector is equal to the square root of the dot product (we’ll get to this) of the vector with itself;\n\\[\\lVert r \\rVert = \\sqrt{r.r}\\]\nPractice\nLet \\(a = \\begin{bmatrix}3 \\\\ 0 \\\\ 4\\end{bmatrix}\\) and \\(b = \\begin{bmatrix}0 \\\\ 5 \\\\ 12\\end{bmatrix}\\)\nWhich is larger? \\(\\lVert a + b \\rVert\\) or \\(\\lVert a \\rVert + \\lVert b \\rVert\\)\n\\(\\lVert a + b \\rVert = \\sqrt{3^2 + 5^2 + 16^2} = 17.029\\)\n\\(\\lVert a \\rVert + \\lVert b \\rVert = 5 + 13 = 18\\)\nThis is known as triangle inequality\n\\[\\lVert a + b \\rVert \\le \\lVert a \\rVert + \\lVert b \\rVert\\]\nShort video from Khan Academy explaining the theorem\nAny one side of a triangle has to be less than the sum of the other two sides.\n\n\n\nSay we have two vectors, \\(r\\) and \\(s\\), \\(r\\) has the components \\(ri\\) and \\(rj\\), \\(s\\) has the components \\(si\\) and \\(sj\\)\n{:width=“300px”}\nWe define \\(r . s\\) (the dot product) as multiplying the \\(i\\) components and the \\(j\\) components and then summing them.\n\\(r.s = ri * si + rj * sj\\)\nAnother way of saying this is; the dot product of two vectors is the sum of their componentwise products;\n{:width=“400px”}\ndef dot(v: Vector, w: Vector) -> float:\n    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n    assert len(v) == len(w), \"vectors must be same length\n\n    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n\nassert dot([1, 2, 3], [4, 5, 6]) == 32 # 1 * 4 + 2 * 5 + 3 * 6\nIt’s the length of the vector you’d get if you projected v onto w.\n{:width=“400px”}\nThe dot product is therefore a scalar number not a vector. However, we get the vector of the dotted line above by multiplying \\(w\\) by the dot product.\n\\(r = \\begin{bmatrix}3 \\\\ 2\\end{bmatrix}\\) \\(s = \\begin{bmatrix}-1 \\\\ 2\\end{bmatrix}\\) dot product = 1\nTwo properties of the dot product;\n\nThe dot product is commutative ie r.s == s.r\nThe dot product is distributive over addition ie r.(s + t) == r.s + r.t - think in terms of unpacking\nThe dot product is associative over scalar multiplication, similar to addition, here \\(a\\) is a scalar number; r.(as) == a(r.s) we can just pull scalar numbers out\n\nThere is an interesting relationship between the size and the dot product of a vector.\nIf we dot a vector with its self \\(r.r\\) we get the sum of the squares of its components (the square of it’s size, it’s modulus);\n\\[r.r = r_1 r_1 + r_2 r_2\\] \\[  = r_1^2 + r_2^2\\] \\[  = (\\sqrt{r_1^2 + r_2^2})^2\\] \\[  = \\lVert r \\rVert^2\\]\nSo we can get the size of a vector by dotting it and taking the square root.\n\nThe dot product for two \\(n\\) component vectors \\(a, b = a_1b_1 + a_2b_2 + ... + a_nb_n\\)\n\nThe dot product of \\(r\\) and \\(s\\) below;\n\\(r = \\begin{bmatrix}-5\\\\3\\\\2\\\\8\\end{bmatrix} s = \\begin{bmatrix}1\\\\2\\\\-1\\\\0\\end{bmatrix}\\)\n\\(r.s = -5 + 6 + -2 + 0 = -1\\)\n\n\nCosine rule\nIf we have a triangle with sides a, b and c\n\\(c^2 = a^2 + b^2 - 2ab * cos\\theta\\)\nTheta being the angle between a and b\nThe cosine rule, when combined with the dot product can tell us the degree to which the two vectors are pointing in the same direction.\n{:width=“300px”}\nFirst, translate out the cosine rule using vector notation;\n{:width=“300px”}\nWe know that \\(\\lVert r-s \\rVert^2\\) is equal to \\((r-s).(r-s)\\)\n\\[(r-s).(r-s) = r.r \\;\\; r.-s \\; -s.r \\; -s.-s\\] \\[  = \\lVert r \\rVert^2 \\: -2s.r \\;\\; \\lVert s \\rVert^2 \\]\nNow we’ve multiplied out \\(\\lVert r-s \\rVert^2\\)\nWe can compare that to the right hand side.\n\\[\\lVert r \\rVert^2 \\: -2s.r \\;\\; \\lVert s \\rVert^2 = \\lVert r \\rVert^2 + \\lVert s \\rVert^2 - 2\\lVert r \\rVert\\lVert s \\rVertcos\\theta\\]\nEquivalent to;\n\\[ s.r = \\lVert r \\rVert\\lVert s \\rVertcos\\theta \\]\nThe dot product takes the size of the vectors and multiples by the angle between them.\nThis tells us the extent to which the two vectors go in the same direction.\n\ndot product of a vector is a scalar quantity describing only the magnitude of a particular vector\n\n{:width=“400px”}\nIf the vectors are 90 degrees from one another - they’re orothognal - (ie \\(\\theta\\) = 90), - \\(cos 90 = 0\\) - the dot product is 0\nIf the vectors are pointing in the same direction, - ie there is no angle - (\\(\\theta\\) = 0) - \\(cos 0 = 1\\) - the dot product equals the multiplication of the two sizes of the vectors (mod r \\(\\lVert r \\rVert\\) and mod s \\(\\lVert s \\rVert\\)) - a positive dot product tells us their moving in the same direction\nIf the vectors are pointing in opposite directions, - (\\(\\theta\\) = 180), - \\(cos 180 = -1\\) - the dot product equals the minus the multiplication of the two sizes of the vectors (mod r \\(\\lVert r \\rVert\\) and mod s \\(\\lVert s \\rVert\\)) - a negative dot product tells us their moving in opposite directions\nFrom this, we’ve derived a property in the dot product; \\[ r.s = \\lVert r \\rVert\\lVert s \\rVert cos\\theta\\]\nWe can also use this formula to find the angle (\\(\\theta\\)) between the two vectors\n\n\n\nCast your mind back to sohcahtoa;\nsoh -> Sine = Opposite / Hypotenuse\ncah -> Cosine = Adjacent / Hypotenuse\ntoa -> Tangent = Opposite / Adjacent\nHere, the hypotenuse is the size of \\(s\\)\n{:width=“200px”}\n{:width=“200px”}\nRemember that the scalar projection is the size of the green vector.\nIf the angle between \\(s\\) and \\(r\\) is greater than \\(\\pi/2\\), the projection will also have a minus sign.\nWe can substitute this in with our dot product\n\\[ r.s = \\lVert r \\rVert\\lVert s \\rVert cos\\theta\\]\n\\(\\lVert s \\rVert cos\\theta\\) is the adjacent side (dotted line in images above)\nThink of the projection as a light coming down from \\(s\\) at a right angle and the shadow cast onto \\(r\\). If \\(s\\) and \\(r\\) are at 90 degrees, it would be 0.\nThe projection of \\(s\\) onto \\(r\\) is not the same as \\(r\\) onto \\(s\\) - the light will be pointing at different angles.\nThe dot product gives us the projection multiplied by the size of r \\(\\lVert r \\rVert\\)\n{:width=“200px”}\nWe can get the adjacent size by diving the dot product by mod r\n\\[ \\frac{r.s}{\\lVert r \\rVert} = \\lVert s \\rVert cos\\theta\\]\nRemember, \\(r.s\\) is a number and \\(\\lVert r \\rVert\\) is a number so you get a number, known as the scalar projection\nThe dot product is also known as the projection product. It takes the projection of one vector onto another.\nWe can do projection in any number of dimensions. Consider two vectors with three components\n\\(r = \\begin{bmatrix}3\\\\-4\\\\0\\end{bmatrix} s = \\begin{bmatrix}10\\\\5\\\\-6\\end{bmatrix}\\)\nThe scalar projection of \\(s\\) on \\(r\\);\n\\[ proj_rs = \\frac{s.r}{\\lVert r \\rVert}\\]\n\\(s.r = 10;\\;\\; r.r = 5\\)\nThe vector projection allows us to encode something about which direction \\(r\\) was going, into the scalar projection\nVector projection; \\[ r\\; \\frac{r.s}{\\lVert r \\rVert \\lVert r \\rVert} = r\\; \\frac{r.s}{r.r}\\]\nWe’ve take the scalar projection (how much \\(s\\) goes along \\(r\\) or \\(\\frac{r.s}{\\lVert r \\rVert}\\) and multiplied it by \\(r\\) divided by its length or \\(r\\; \\frac{1}{\\lVert r \\rVert}\\) (this produces a vector going the direction of \\(r\\) but normalised to have a length 1)\nThe vector projection is a number multiplied by a unit vector (that goes in the direction of \\(r\\))\nTaking our example from above, given the the scalar projection is 2, the vector projection of \\(s\\) onto \\(r\\)\n\\(r = \\begin{bmatrix}3\\\\-4\\\\0\\end{bmatrix} s = \\begin{bmatrix}10\\\\5\\\\-6\\end{bmatrix}\\)\nThe scalar projection of \\(s\\) on \\(r\\);\n\\[ proj_rs = \\frac{s.r}{\\lVert r \\rVert} = 2\\]\n\\(s.r = 10;\\;\\; r.r = 5\\)\nThe vector projection is;\n\\[\\frac{s.r}{r.r}r\\]\nSo you can multiple \\(r\\) by the scalar projection and then divide by \\(r\\)\n\\(\\begin{bmatrix}3\\\\-4\\\\0\\end{bmatrix} *2 = \\begin{bmatrix}6\\\\-8\\\\0\\end{bmatrix}\\)\nnormalised by length r = \\(\\begin{bmatrix}6/5\\\\-8/5\\\\0\\end{bmatrix}\\)\n\n\n\n\nThe coordinate system is what we use to describe space. These are our basis vectors.\nWe describe our vectors in terms of our basis vectors.\nAny time we describe vectors numerically it implies implicitly on what basis vectors we’re using.\n\\(r\\) described in terms of \\(e_1\\) and \\(e_2\\) (hat -> \\(\\hat{e}\\) represents they are of unit length)\nIn a 2D plane we can describe all points with different combinations of our two basis vectors (as long as linearly independent i.e. not pointing in the same direction).\nThe “span” of \\(\\hat{e_1}\\) and \\(\\hat{e_2}\\) is the set of all their linear combinations; \\(a\\hat{e_1} + b\\hat{e_2}\\)\n{:width=“400px”}\n\\(r\\) exists independently of the basis vectors. It takes us from an origin to another point, but we use the coordinate system to describe \\(r\\).\nIf we want to do a transformation of axes i.e. change the coordinate system we use to describe \\(r\\), if the vectors are at a 90 degree angle from one another we can use the dot product, otherwise we need to use a matrix.\nHere, we know \\(b\\) in terms of \\(e\\) and the vectors are at a 90 degreee angle to one another (orthogonal), so we can work out \\(r\\) in terms of \\(b\\) using the dot product.\nYou check that two vectors are orthogonal by taking the dot product, multiply two vectors and divide by their length;\n\\[ cos\\theta = \\frac{b_1.b_2}{\\lVert b_1 \\rVert\\lVert b_2 \\rVert}\\]\nIf the dot product is 0, \\(cos\\theta\\) is 0 and they’re at 90 degrees to one another.\nUsing the example below;\n\\[b_1.b_2 = (2*-2) + (1*4) = 0\\]\n{:width=“400px”}\nYou can project \\(r\\) down onto \\(b_2\\) and calculate the vector projection. This will tell you “how much of that axis (\\(b_2\\)) you need”. You then do the same for \\(b_1\\)\nThe sum of those two vector projections, give you \\(r\\) in terms of \\(b\\) or \\(r_b\\).\nThe vector projection for \\(b_1\\);\n\\[r_e = \\begin{bmatrix}3\\\\4\\end{bmatrix}\\;\\; b_1 = \\begin{bmatrix}2\\\\1\\end{bmatrix}\\]\n\\[\\frac{r_e.b_1}{\\lVert b_1 \\rVert^2} = \\frac{(3*2)+(4*1)}{2^2 + 1^2} = \\frac{10}{5} = 2\\]\nAnd the same for the other axis\n\\[b_2 = \\begin{bmatrix}-2\\\\4\\end{bmatrix}\\]\n\\[\\frac{r_e.b_2}{\\lVert b_2 \\rVert^2} = \\frac{(3*-2)+(4*4)}{-2^2 + 4^2} = \\frac{10}{20} = 1/2\\]\n\\[r_b = \\begin{bmatrix}2\\\\1/2\\end{bmatrix}\\]\nWe can verify by multiplying our \\(b\\)s in terms of \\(e\\) by the scalar projections (normalising) to product \\(r_e\\)\n\\[b_1 = \\begin{bmatrix}2\\\\1\\end{bmatrix} *2 = \\begin{bmatrix}4\\\\2\\end{bmatrix}\\]\n\\[b_2 = \\begin{bmatrix}-2\\\\4\\end{bmatrix} *1/2 = \\begin{bmatrix}-1\\\\2\\end{bmatrix}\\]\n\\[\\begin{bmatrix}4\\\\2\\end{bmatrix} + \\begin{bmatrix}-1\\\\2\\end{bmatrix} = \\begin{bmatrix}3\\\\4\\end{bmatrix} \\]\nWe can re-describe \\(r\\) using a new set of basis vectors.\n\nPractice\n\n\n\nGiven the following vectors are written in the standard basis, represent v in terms of b;\n\\[v = \\begin{bmatrix}5\\\\-1\\end{bmatrix}\\;\\; b_1 = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\;\\; b_2 = \\begin{bmatrix}1\\\\-1\\end{bmatrix}\\]\nvb1 = ((5 * 1) + (-1 * 1)) / (1**2 + 1**2) # 2\n\nvb2 = ((5 * 1) + (-1 * -1)) / (1**2 + pow(-1, 2)) # 3\n\\[v_b = \\begin{bmatrix}2\\\\3\\end{bmatrix}\\]\n\n\n\n\\[v = \\begin{bmatrix}10\\\\-5\\end{bmatrix}\\;\\; b_1 = \\begin{bmatrix}3\\\\4\\end{bmatrix}\\;\\; b_2 = \\begin{bmatrix}4\\\\-3\\end{bmatrix}\\]\n\\[v_b = \\begin{bmatrix}2/5\\\\11/5\\end{bmatrix}\\]\n\n\n\n\\[v = \\begin{bmatrix}2\\\\2\\end{bmatrix}\\;\\; b_1 = \\begin{bmatrix}-3\\\\1\\end{bmatrix}\\;\\; b_2 = \\begin{bmatrix}1\\\\3\\end{bmatrix}\\]\n\\[v_b = \\begin{bmatrix}-2/5\\\\4/5\\end{bmatrix}\\]\n\n\n\n\\[v = \\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\;\\; b_1 = \\begin{bmatrix}2\\\\1\\\\0\\end{bmatrix}\\;\\; b_2 = \\begin{bmatrix}1\\\\-2\\\\-1\\end{bmatrix}\\;\\; b_3 = \\begin{bmatrix}-1\\\\2\\\\5\\end{bmatrix}\\]\n\\[v_b = \\begin{bmatrix}3/5\\\\-1/3\\\\-2/15\\end{bmatrix}\\]\n\n\n\n\\[v = \\begin{bmatrix}1\\\\1\\\\2\\\\3\\end{bmatrix}\\;\\; b_1 = \\begin{bmatrix}1\\\\0\\\\0\\\\0\\end{bmatrix}\\;\\; b_2 = \\begin{bmatrix}0\\\\2\\\\-1\\\\0\\end{bmatrix}\\;\\; b_3 = \\begin{bmatrix}0\\\\1\\\\2\\\\0\\end{bmatrix}\\;\\;\nb_4 = \\begin{bmatrix}0\\\\0\\\\0\\\\3\\end{bmatrix}\\]\n\\[v_b = \\begin{bmatrix}1\\\\0\\\\1\\\\1\\end{bmatrix}\\]\n\n\n\nA basis is a set of \\(n\\) vectors that;\n\nAre not linear combinations of each other  and are therefore linearly independent\nSpan the space\nOur space is therefore n-dimensional\n\n\\(b_3\\) is not a valid basis vector. Because I can take some combination of \\(b_2\\) and \\(b_1\\) to get \\(b_3\\)\nWe cannot write one of the vectors as a linear combination of the others\nie: \\(b_3 = a_1 b_1 + a_2 b_2\\)\n“It lies in the same plane as \\(b_1\\) and \\(b_2\\)”\nFormula required to show linear independence;\n{:width=“400px”}\nLinearly dependent;\n\\(a = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\;\\; b = \\begin{bmatrix}2\\\\2\\end{bmatrix}\\)\nas;\n\\(a = \\frac{1}{2}b\\)\nSimilarly, \\(\\mathbf{a} = q_1\\mathbf{b} + q_2\\mathbf{c}\\)\n{:width=“400px”}\nFind \\(q_1, q_2\\)\n\\(\\begin{bmatrix}2\\\\2\\end{bmatrix}\\;\\; = q_1 \\begin{bmatrix}1\\\\-2\\end{bmatrix} + q_2 \\begin{bmatrix}-1\\\\0\\end{bmatrix}\\)\n\\(q_1 = -1,\\;\\; q_2 = -3\\)\nLinearly independent; one is not a scalar multiple of the other\n\\(a = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\;\\; b = \\begin{bmatrix}2\\\\1\\end{bmatrix}\\)\nBasis vectors do not need to be; - Of unit length (length one) - Orthogonal - Though it’s much if they are both of these things\nLinearly independent\n\\(a = \\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}\\;\\; b = \\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}\\;\\; c = \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}\\)\nEasy to tell, as you need all three vectors to cover each element. Whereas in the following example, you do not;\nLinearly dependent\n\\(a = \\begin{bmatrix}1\\\\2\\\\0\\end{bmatrix}\\;\\; b = \\begin{bmatrix}-2\\\\1\\\\3\\end{bmatrix}\\;\\; c = \\begin{bmatrix}4\\\\3\\\\-3\\end{bmatrix}\\)\n\\(c = 2a - b\\)\nWhat happens when we map from one basis to another?\nThe original grid projects down onto the new grid.\nThough it will potentially have different values on that grid, the projection keeps the grid being evenly spaced.\nTherefore, any mapping we do from one set of basis vectors, from one coordinate system to another, keeps the vector space being a regularly spaced grid. Ensuring, our original vector rules of vector addition and multiplication by a scalar still work.\nIt doesn’t warp or fold space which is what the linear bit in linear algebra means. Things might be stretched or rotated or inverted, but everything remains evenly spaced and linear combinations still work.\n{:width=“400px”}\n\n\n\nChanging the basis is what we’re doing when we do linear regression.\nWe’re working out the distance of points from a vector (distance least squared).\nWe use the dot-product to do the projection to map the data from the x-y space onto the space of the line.\nThere’s some theoretical disputes, whether the distance should be measured straight (y-axis) or orthognally (the angle).\n{:width=“400px”}\nIn a neural network for face recognition, the goal of the learning process is going to be to somehow derive a set of basis vectors that extract the most information-rich features of the faces.\n\nPractice\nQ1\nA ship travels with the velcoity given by \\(\\begin{bmatrix}1\\\\2\\end{bmatrix}\\), with current flowing in the direction given by \\(\\begin{bmatrix}1\\\\1\\end{bmatrix}\\) with respect to some co-ordinate axes.\nWhat is the velocity of the ship in the direction of the current?\n\nvector projection of the velocity of the ship, onto the velocity of the current\n\n\\(\\frac{ship . current}{current . current} * current\\)\n\\(\\frac{\\begin{bmatrix}1\\\\2\\end{bmatrix} . \\begin{bmatrix}1\\\\1\\end{bmatrix}} {\\begin{bmatrix}1\\\\1\\end{bmatrix} . \\begin{bmatrix}1\\\\1\\end{bmatrix}} * \\begin{bmatrix}1\\\\1\\end{bmatrix}\\)\n\\(\\begin{bmatrix}3/2\\\\3/2\\end{bmatrix}\\)\nQ2\nA ball travels with the velocity given by \\(\\begin{bmatrix}2\\\\1\\end{bmatrix}\\), with wind blowing in the direction given by \\(\\begin{bmatrix}3\\\\-4\\end{bmatrix}\\) with respect to some co-ordinate axes.\nWhat is the size of the velocity of the ball in the direction of the wind?\nThis is the scalar projection of the velocity of the ball onto the velocity of the wind.\nIf you were to draw a straight line from the co-ordinate of the ball onto the vector of the wind, what is the size of that vector in terms of the wind vector.\n{:width=“200px”}\nRemember that the scalar projection is the size of the green vector.\n\\(\\frac{\\begin{bmatrix}2\\\\1\\end{bmatrix} . \\begin{bmatrix}3\\\\-4\\end{bmatrix}} {\\sqrt{3^2 + -4^2}}\\)\n\\(\\frac{2}{5}\\)\nQ3\nGiven vectors \\(v = \\begin{bmatrix}-4\\\\-3\\\\8\\end{bmatrix}\\; b_1 = \\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\; b_2 = \\begin{bmatrix}-2\\\\1\\\\0\\end{bmatrix}\\; b_3 = \\begin{bmatrix}-3\\\\-6\\\\-5\\end{bmatrix}\\)\nAll written in the standard basis, what is \\(v\\) in the basis defined by \\(b_1, b_2, b_3\\)? -> they are all pairwise orthogonal to one another.\nAnswer What manipulation do you need to do in order to produce \\(v\\). Simply add them together, so one of each vector.\nThis is a change of basis in 3 dimensions.\n\\(\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\)\nQ4\nAre the following vectors linearly independent?\n\\(a = \\begin{bmatrix}1\\\\2\\\\-1\\end{bmatrix}\\; b = \\begin{bmatrix}3\\\\-4\\\\5\\end{bmatrix}\\; c = \\begin{bmatrix}1\\\\-8\\\\7\\end{bmatrix}\\)\nNo, \\(b = 2a + c\\)\nQ5\nAt 12:00 pm, a spaceship is at position \\(\\begin{bmatrix}3\\\\2\\\\4\\end{bmatrix} km\\) away form the origin with respect to some 3 dimensional co-ordinate system. The ship is travelling with velocity \\(\\begin{bmatrix}3\\\\2\\\\4\\end{bmatrix} km/h\\). What is the location of the spaceship after 2 hours have passed?\nAnswer Multiply the velocity by 2 and then add it to the current position to get the new position.\n\\(\\begin{bmatrix}1\\\\6\\\\-2\\end{bmatrix}\\)\n\n\n\n\n\nUnfortunately, no one can be told what the Matrix is. You have to see if for yourself - Morpheus\n\nMorpheus may have a point. Like the proverbial fish who has no idea what water is, we are swimming in matrices.\n\nMatrices are everywhere; anything that can be put in an Excel spreadsheet is a matrix, and language and pictures can be represented as matrices as well. fast ai\n\nA matrix is a two-dimensional collection of numbers, a list of lists. If a matrix has \\(n\\) rows and \\(k\\) columns, we refer to it as an \\(n \\times k matrix\\). We can think of each column in a matrix as a vector of length \\(n\\).\nWhen working with matrices to represent data, we need to think of them slightly differently to tabular data structures. However, they can to some extent both follow the tidy data philosophy where each column represents an attribute and each row a record.\nMatrices can also be used to represent binary relationships, where if A[i][j] = 1 then nodes \\(i\\) and \\(j\\) are connected (see hot encoding).\nMatrices can also be thought of as objects that rotate and stretch vectors. They also help us solve simultaneous equations.\n\\[2a + 3b = 8\\] \\[10a + 1b = 13\\]\nCan be rewritten as;\n\\[\\begin{pmatrix} 2a + 3b \\\\ 10a + 1b \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 13 \\end{pmatrix}\\]\nOr\n\\[\\begin{pmatrix} 2 & 3 \\\\ 10 & 1 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 13 \\end{pmatrix}\\]\nThe matrix \\(\\begin{pmatrix} 2 & 3 \\\\ 10 & 1 \\end{pmatrix}\\) acts on the vector \\(\\begin{bmatrix}a\\\\b\\end{bmatrix}\\)\nSo we then ask, what vector transforms to produce \\(\\begin{pmatrix} 8 \\\\ 13 \\end{pmatrix}\\)\nThis is the heart of linear algebra.\n\nNow we can see what we mean now by the term linear algebra.\nLinear algebra is linear, because it takes input values, our a and b, and multiplies them by constants. So everything is linear. And it’s algebra, that is it’s a notation describing mathematical objects and a system of manipulating those notations.\nSo linear algebra is a mathematical system for manipulating vectors in the spaces described by vectors.\nSo this is interesting. There seems to be some kind of deep connection between simultaneous equations, these things called matrices, the vectors. And it turns that the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices, which is the heart of linear algebra.\n\nWe can multiply the matrix \\(\\begin{pmatrix} 2 & 3 \\\\ 10 & 1 \\end{pmatrix}\\) by some basis vectors, for x we have \\(e_1 = \\begin{bmatrix}1\\\\0\\end{bmatrix}\\) and for y we have \\(e_2 = \\begin{bmatrix}0\\\\1\\end{bmatrix}\\)\nWhen multiplying vectors;\n\\[\\begin{pmatrix} 2 & 3 \\\\ 10 & 1 \\end{pmatrix} * \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2*1 + 3*0 \\\\ 10*1 + 1*0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 10 \\end{pmatrix}\\]\n\\[\\begin{pmatrix} 2 & 3 \\\\ 10 & 1 \\end{pmatrix} * \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2*0 + 3*1 \\\\ 10*0 + 1*1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\]\nSo the matrix \\(\\begin{pmatrix} 2 & 3 \\\\ 10 & 1 \\end{pmatrix}\\) is transforming our basis vectors, it’s a function that operates on input vectors and gives us output vectors.\nA set of simultaneous equations is asking what vector I need in order to get a transformed product at the position 8 13.\n\n\nTransformation is a another term for function \\(f(x)\\), it takes an input and produces an output.\nIn linear algebra a matrix transformation takes in a vector and produces another vectors. Transformation suggests movement, again thinking geometrically.\nWe can think of every corresponding input vector within a space, moving to its corresponding output vectors.\nThink of vectors as points (i.e. where the arrow points) and think of that point moving to some other point.\n{:width=“400”}\nWe have the ability to move around all the points in space. Though the types of transformations are limited to linear transformations.\nVisually, what this means is that all lines in our original coordinate system must remain lines, and our origin remains fixed.\n{:width=“400”}\nIt’s not just the vertical and horizontal lines that must remain lines, but all angles, so diagonals as well. Grid lines remain parallel and evenly spaces.\nThe implications of this are important.\nA vector, say \\(v = \\begin{bmatrix} -1\\\\2 \\end{bmatrix}\\)\nWe know is actually, \\(\\begin{bmatrix} -1\\hat{i}\\\\2 \\hat{j}\\end{bmatrix}\\)\nWhen do we the transformation, we simply need to understand what happens \\(\\hat{i}\\) and \\(\\hat{j}\\).\n\\(v\\) will still be the same linear combination of \\(\\hat{i}\\) and \\(\\hat{j}\\).\ntransformed \\(v\\) = -1(transformed \\(\\hat{i}\\)) + 2(transformed \\(\\hat{j}\\))\nIn a 2 dimensional matrix, we need just four numbers to know where any vector will now land. The coordinates for \\(\\hat{i}\\) and \\(\\hat{j}\\).\n{:width=“400”}\nAny vector can then be translated from the old basis vectors to the new transformation.\n{:width=“400”}\nin matrix multiplication, remember; rows times cols\nThe intuition here is to think of the two columns in the matrix as where your basis vectors end up, and your vector as a linear combination of your new basis vectors.\nA matrix is therefore a transformation of space.\n\n\n\nSpace changes include;\n\nstretches,\ninversions,\nmirrors,\nshears,\nrotations\n\n\nWe can think of a matrix multiplication being the multiplication of the vector sum of the transformed basis vectors.\n\nThe identity matrix \\((I)\\), the matrix that does nothing. It is composed of the basis vectors.\n\\[\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\]\nThe “Identity Matrix” is the matrix equivalent of the number “1”. It is “square” (has same number of rows as columns)\nGives us an axis of 1 x, 1 y –> \\(\\begin{bmatrix}x\\\\y\\end{bmatrix}\\)\nThe matrix; \\[\\begin{pmatrix} 3 & 0 \\\\ 0 & 2 \\end{pmatrix}\\]\nWould expand our basis vectors by 3x and 2y.\n{:width=“200”}\nA fraction would shrink space.\nA negative reverses the space. “Changing the sense of the co-ordinate system”\n\\[\\begin{pmatrix} -1 & 0 \\\\ 0 & 2 \\end{pmatrix}\\]\n{:width=“200”}\nTwo negatives will invert everything. Inversion.\nYou can have a matrix that is akin to having a mirror, where it shifts both axses\n\\[\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\]\n{:width=“400”}\nThe following matrix acts as a vertical mirror plane \\(\\begin{pmatrix} -1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\)\nThe following matrix acts as a horizontal mirror plane \\(\\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\\)\nYou can also have shears\n\nIn plane geometry, a shear mapping is a linear map that displaces each point in fixed direction, by an amount proportional to its signed distance from the line that is parallel to that direction and goes through the origin. This type of mapping is also called shear transformation, transvection, or just shearing.\n\nFor instance, keeping \\(\\hat{e}_1\\) in place but transforming \\(\\hat{e}_2\\) to \\(\\prime{e}_2\\) (e-prime)\n\\(\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) to \\(\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\)\nCreates a parallelogram;\n{:width=“400”}\nA 90 degree anticlockwise rotation will look like;\n\\(\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) to \\(\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\)\n{:width=“400”}\nA general expression for a rotation in 2d can be written as;\n\\(\\begin{bmatrix} cos\\theta & sin\\theta \\\\ -sin\\theta & cos\\theta \\end{bmatrix}\\)\nRotations are less relevant in most data science applications of linear algebra. But image classification and facial recognition would use this transformations in order to get all the images in a certain state before analysing them - ie remove the camera angles and make the faces portrait.\n\n\n\nIf you apply one transformation (a rotation), and then another (a shear), you live get a linear transformation that is distinct from the rotation and shear.\nApplying multiple transformations forms a composition. The composition is the product of multiple matrices.\nThe composition can be described by its own matrix.\nLike function notation, we read from inside out \\(f(g(x))\\)\n{:width=“400”}\nYou can see why we do rows times cols.\n\nThe matrix first applied is where the new basis vectors land.\n\\(\\hat{i}\\) and \\(\\hat{j}\\) are represented as columns in the matrix on the right.\nWe then need to multiply each vector (column) by the new matrix.\n\nThe image below gives us the first column of our composition.\n{:width=“400”}\nThe intuition is to think of applying one matrix transformation after another. This is why the order in which the transformations are applied matters. To rotate and then shear has a different effect than if you shear and then rotate.\nMatrix multiplication is associative \\(AB(C) \\equal A(BC)\\), because this doesn’t change the order in which the transformations are applied.\nIf we take the basis vectors \\(\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\)\n\\(A_1\\) represents a 90 degree clockwise rotation \\(\\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\\)\n\\(A_2\\) represents a mirror along the vertical plane (ie flips the x axis, with no change to y) \\(\\begin{bmatrix} -1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\)\nWe can calculate the result of applying the two transformation \\(A_2(A_1 r)\\) by first applying \\(A_1\\) to our basis vectors and then applying \\(A_2\\) to the result.\n\\[A_2 A_1 = \\begin{bmatrix} -1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\\]\nWe can multiply all possible rows and columns, without having to draw it out.\n\nTop left value = the first row of \\(A_2\\) and multiply it by the first column in \\(A_1\\),\nBottom left value = second row, first column\nTop right = first row, second column\nBottom right = second row, second column\n\n\\[ = \\begin{bmatrix} (-1 * 0) + (0 * -1) & (-1 * -1) + (0*0)\\\\ (-1 * 1) + (0 * 0) & (0 * 1) + (1 * 0) \\end{bmatrix}\\]\n\\[ = \\begin{bmatrix} 0 & -1 \\\\ -1 & 0 \\end{bmatrix}\\]\nThe order of transformations matter, doing \\(A_2\\) and then \\(A_1\\) gives us a different result.\n\\[A_1 A_2 = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix} \\begin{bmatrix} -1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\]\n\\[A_1 A_2 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\\]\nMatirx multiplication isn’t commutative\nWe can do them in any order, meaning they are associative. \\(A_3(A_2 A_1)\\) is the same as \\((A_3 A_2) A_1\\) but we cannot interchange the order.\n\n\n\nMatrices make transformations on vectors, potentially changing their magnitude and direction.\nIf we have two unit vectors (in orange) and another vector, \\(r = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}\\) (in pink), before any transformations - these look like this:\n{:width=“200”}\nThe matrix, \\(A = \\begin{pmatrix} 1/2 & -1 \\\\ 0 & 3/4 \\end{pmatrix}\\) will transform them;\n{:width=“200”}\nQ1\nr prime, or \\(A\\) applied to \\(r\\) can be written as;\n\\(Ar = \\begin{pmatrix} 1/2 & -1 \\\\ 0 & 3/4 \\end{pmatrix} \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}\\)\n\\(\\prime{r} = \\begin{bmatrix} -1/2 \\\\ 3/2 \\end{bmatrix}\\)\n(1/2 * 3) + (-1 * 2) = -1/2\n(0 * 3) + (3/4 * 2) = 3/2\nQ2\n\\(s = A \\begin{bmatrix} -2 \\\\ 4 \\end{bmatrix}\\)\n(1/2 * -2) + (-1 * 4) = -5\n(0 * -2) + (3/4 * 4) = 3\nQ3\n\\(M = \\begin{bmatrix} -1/2 & 1/2 \\\\ 1/2 & 1/2 \\end{bmatrix}\\)\nThoughts; - Everything gets smaller. - Top lefts inversion should flip the horizontal axis. - initially thought that because diagonals did not align it would be a parallelogram rather than a square, but not the case because it is a sheer and scale transformation - The axis have been rotated and flipped\nUsing unit vectors \\(\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\)\ntop left -0.5 bottom left 0.5 top right 0.5 bottom right 0.5\nBest corresponds to;\n{:width=“200”}\nQ4\nQuick hack - anticlockwise rotation requires top right to be negative, all else positive.\nQ5\n\\(M = \\begin{bmatrix} 1 & 0 \\\\ 0 & 8 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ -1/2 & 1 \\end{bmatrix}\\)\ntop left 1*1 + 0*-1/2 = 1\nbottom left 0*1 + 8*-1/2 = -4\ntop right 1*0 + 0x1 = 0\nbottom right 0x0 + 8x1 = 8\n\n\n\nThe apples and bananas problem\nI walked into a shop and bough 2 apples and 3 bananas for a price of £8. Another day I bought 10 apples and 1 banana for a price of £13.\nThis is a matrix (fruits \\(A\\)) * a vector (prices \\(r\\)) and produces an output vector (purchase \\(s\\)).\n\\(A\\) operates on \\(r\\) and gives \\(s\\)\n\\[ \\begin{pmatrix} 2 & 3 \\\\ 10 & 1 \\end{pmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 13 \\end{bmatrix}\\]\n\\(A^{-1}\\), the inverse of \\(A\\) does exact opposite of what \\(A\\) does.\nBy multiplying \\(A\\) by \\(A^{-1}\\), we reverse whatever \\(A\\) does and we get the identity matrix \\(I\\), the matrix that does nothing.\n\\(Ar = s\\) becomes \\(A^{-1} Ar = A^{-1} s\\)\n\\(A^{-1} A\\) is the identity matrix (\\(A^{-1} A = I\\)), the matrix that does nothing, so you just have \\(r\\)\n\\(r = A^{-1} s\\)\n\n\n\nAll of sudden we’ve moved away from planning with geometry and vector spaces and back to linear equations.\nSystems of equations are a list of variables (things you don’t know) and a list of equations relating to them.\nThis is one of the core applications of linear algebra, it allows us to solve “systems of equations”.\nThe following system of equations;\n\\[4x_1 = 5x_2 = - 13\\] \\[-2x_1 + 3x_2 = 9\\]\nIn matrix notation, we can write the system more compactly;\n\\(Ax = b\\) with;\n\\[A = \\begin{bmatrix} 4 & -5 \\\\ -2 & 3 \\end{bmatrix}, b = \\begin{bmatrix} -13 \\\\ 9 \\end{bmatrix}\\]\nIf the equations can be solved using linear algebra then (1) the variable in each equation is being scaled by some constant and (2) those variables are being added to each other in the equation. There are no exponents or multiplying variables together.\nArranging our systems of equations like this, sheds some geometric light on the problem;\n{:width=“400”}\nWe’re looking for a vector \\(\\bf{x}\\), that after applying the transformation \\(A\\), lands on \\(\\bf{v}\\)\nIf \\(A\\) manipulated \\(\\bf{x}\\) such that it rotated 90 degrees counter-clockwise, then we need to find the matrix that reverses that transformation, i.e. a 90 degree clockwise rotation.\nIf \\(A = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}, A^{-1} = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\\)\nIf you first apply \\(A\\), then \\(A^{-1}\\) you end up where you started.\n{:width=“200”}\n\\(A^{-1}A\\) gets you back to where you started; multiplied out it gives you the identity matrix, the equivalent of multiplying by 1 in matrix multiplication.\nTherefore;\n{:width=“400”}\nIf the determinant of \\(A\\) is zero, it has manipulated space into a lower dimension, as a result we cannot use it’s inverse to understand what matrix was applied onto \\(\\bf{x}\\) to produce \\(\\bf{v}\\).\nThe equation can be solved, but only with the vector \\(\\bf{v}\\) lives on the new dimension (in the case of 3D to 2D, the vector must live on that line).\n{:width=“300”}\nWhen the output of a transformation is a line, i.e. it’s one-dimensional, we say the transformation has a rank of 1. If all vectors land on a two-dimensional place, the transformation has a rank of 2, and so on.\nThe set of outputs for the transformation matrix is called the column space. The columns tell you where your vectors land, and the span of the columns gives you all possible outputs.\n{:width=“300”}\nThe rank is the number of dimensions in the column space. If a rank mantains it’s number of dimensions, then it is regarded as “full rank”.\nWhen a matrix has full rank, only the point on the origin is the original origin. When a matrix loses dimensions, many points will now fall on the origin. Think of a line coming down to a single point. Lots of lines now sit at the origin. The set of vectors that land on the origin is called the “null space” or the “kernel” of your matrix. The space of all vectors that become null.\n\n\n\nElimination\nWith a more complex example. We can simplify the problem by removing row one from the other two rows\n\\[ \\begin{pmatrix} 1 & 1 & 3 \\\\ 1 & 2 & 4 \\\\ 1 & 1 & 2 \\end{pmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} = \\begin{bmatrix} 15 \\\\ 21 \\\\ 13 \\end{bmatrix}\\]\nRemoving row one gives us;\n\\[ \\begin{pmatrix} 1 & 1 & 3 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & -1 \\end{pmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} = \\begin{bmatrix} 15 \\\\ 6 \\\\ -2 \\end{bmatrix}\\]\nWe’ve now solved \\(c\\), \\(-c = -2\\), \\(c = 2\\)\nA matrix like this, where everything below the body diagonal is 0 is known as a triangular matrix.\nThe matrix has been reduced to what’s known as Echelon form. All the numbers below the leading diagonal is zero.\nWe can now use back substitution. Using the value of \\(c\\) nad plugging back into the first two rows.\nRemoving \\(c\\) from the rows we get;\n\\[ \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} = \\begin{bmatrix} 9 \\\\ 4 \\\\ 2 \\end{bmatrix}\\]\nThen getting to the final solution we remove \\(b\\) from row 1;\n\\[ \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 4 \\\\ 2 \\end{bmatrix}\\]\nHowever, we found \\(r\\) for a specific output of \\(s\\), we didn’t compute the general. Calculating the inverse allows us to ensure we have the general case. We could find \\(r\\) for any \\(s\\).\nElimination and back substitution are extremely computational efficient.\nFinding the inverse matrix\n\\(A B = I\\) where \\(B\\) is the inverse of \\(A\\), \\(B = A^{-1}\\). You can apply the inverse to the right or the left, it’s commutative.\n\\[ A = \\begin{pmatrix} 1 & 1 & 3 \\\\ 1 & 2 & 4 \\\\ 1 & 1 & 2 \\end{pmatrix} B = \\begin{pmatrix} b_{11} & b_{12} & b_{13} \\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{pmatrix}\\]\nThe identity matrix is \\(I = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\)\nYou could perform back substitution, by taking each column in the inverse matrix and use the output of the identity matrix.\n\\[ \\begin{pmatrix} 1 & 1 & 3 \\\\ 1 & 2 & 4 \\\\ 1 & 1 & 2 \\end{pmatrix} * \\begin{pmatrix} b_{11}\\\\ b_{21}\\\\ b_{31} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\]\nHowever, we can use elimination to do it more efficiently.\nSubtracting the first row from rows two and three.\n\\[A* \\begin{pmatrix} 1 & 1 & 3 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ -1 & 0 & 1 \\end{pmatrix}\\]\nIf we multiply the bottom row by -1 we can get the matrix in echelon form;\n\\[A* \\begin{pmatrix} 1 & 1 & 3 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ 1 & 0 & -1 \\end{pmatrix}\\]\nThen back substitute, make the third column 0 in both rows one and two.\nTake the bottom row and subtract it from the second row and multiply by three and subtract from row one;\n\\[A* \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} -2 & 0 & 3 \\\\ -2 & 1 & 1 \\\\ 1 & 0 & -1 \\end{pmatrix}\\]\nThen back substitute b (the middle column) from the first row;\n\\[A* \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} -2 & -1 & 2 \\\\ -2 & 1 & 1 \\\\ 1 & 0 & -1 \\end{pmatrix}\\]\nThis provides us with the inverse matrix.\n\\[ \\begin{pmatrix} 1 & 1 & 3 \\\\ 1 & 2 & 4 \\\\ 1 & 1 & 2 \\end{pmatrix} * \\begin{pmatrix} -2 & -1 & 2 \\\\ -2 & 1 & 1 \\\\ 1 & 0 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\]\nComputationally this approach is much easier, particularly when the dimensions increase. There are computationally faster methods to do a decomposition process.\nMost software packages come with a solver function that we simply have to call on the matrix, and it will apply the most efficient process inv(A).\nThe method above is a general method that we could write ourselves.\n\n\n\nQ1\nYou go to the shops on Monday and buy 1 apple, 1 banana, and 1 carrot; the whole transaction totals €15. On Tuesday you buy 3 apples, 2 bananas, 1 carrot, all for €28. Then on Wednesday 2 apples, 1 banana, 2 carrots, for €23.\n\\[ A * \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} =\\begin{pmatrix} sMon \\\\ sTue \\\\ sWed \\end{pmatrix}\\]\nA = [[1, 1, 1],\n     [3, 2, 1],\n     [2, 1, 2]]\n\ns = [15, 28, 23]\nQ2\nGiven another system, \\(Br = t\\)\n{:width=“450”}\nWe need to time the first row by three, then minus that from the second row. This will ensure the first value of the row is 0.\nThen we need to multiply by minus 2 to ensure the second value of the row is 1.\n\\(2{''} = [2{'} - (3*1{'})] * -2\\)\nQ3\nFrom our previous question;\n\\[ \\begin{pmatrix} 1 & 3/2 & 1/2 \\\\ 0 & 1 & 1 \\\\ 2 & 8 & 13 \\end{pmatrix} * \\begin{bmatrix} a\\\\ b\\\\ c\\end{bmatrix} = \\begin{pmatrix} 9/4 \\\\ -1/2 \\\\ 2 \\end{pmatrix}\\]\nFix row 3 to be a linear combination of the other two and provide a matrix in echelon form.\n\nrow_3 = [2, 8, 13] = 2\n\n# Multiply row 1 by 2\n[1, 3/2, 1/2] * 2 = [2, 3, 1]\n9/4 * 2 = 4.5\n\n# Subtract from 3\n[2, 8, 13] - [2, 3, 1] = [0, 5, 12]\n2 - -2.5 = -2.5\n\n# Multiply row 2 by 5\n[0, 1, 1] * 5 = [0, 5, 5]\n-0.5 * 5 = -2.5\n\n# Subtract from 3'\n[0, 5, 12] - [0, 5, 5] = [0, 0, 7]\n-2.5 - -2.5 = 0\n\n# Divide by 7\n[0, 0, 7] / 7 = [0, 0, 1]\n0/7 = 0\n\\[ \\begin{pmatrix} 1 & 3/2 & 1/2 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix} * \\begin{bmatrix} a\\\\ b\\\\ c\\end{bmatrix} = \\begin{pmatrix} 9/4 \\\\ -1/2 \\\\ 0 \\end{pmatrix}\\]\nWe can then compute a, b, c\nb = -1/2 (ignore c as 0)\na = 9/4 - (-1/2 * 1.5) = 3 (multiply row 2 by 1.5 then remove it, ignore row 3 as 0)\n\\(r = \\begin{bmatrix} 3 \\\\ -1/2 \\\\ 0 \\end{bmatrix}\\)\nQ5/6\nReturning to Q1, convert the system to echelon form\nA = [[1, 1, 1],\n     [3, 2, 1],\n     [2, 1, 2]]\n\ns = [15, 28, 23]\n\n# Answer;\n# Row 1 * 2\n[2, 2, 2] , 30\n\n# Sub from Row 3\n[0, -1, 0] , -7\n# Multiply by -1\nR3` = [0, 1, 0], 7\n\n# Add R2 to R3`\n[3, 3, 1], 35\n\n# Row 1 * 3 and sub from R3``\n[0, 0, -2], -10\n# Multiply by -0.5\nR3` = [0, 0, 1], 5\n\n# Row 1 * 3 and sub from R2\n[0, -1, -2], -17\n# Multiply by -1\nR2` = [0, 1, 2], 17\n\nA = [[ 1 , 1, 1],\n     [ 0,  1 , 2],\n     [ 0,  0 , 1]]\ns = [15, 17, 5]\n\n# Price of individual elements;\ns = [3, 7, 5]     \nQ7\nFind the inverse of the matrix you used in Question 1\n# Answer;\nAinv = [[-1.5,  0.5,  0.5],\n       [ 2.0,  0.0, -1.0],\n       [ 0.5, -0.5,  0.5]]\nQ8\nIn practice, for larger systems, one never solves a linear system by hand as there are software packages that can do this for you - such as numpy in Python.\nimport numpy as np\n\nA = [[1, 1, 3],\n     [1, 2, 4],\n     [1, 1, 2]]\n\nAinv = np.linalg.inv(A)\nIn general, one shouldn’t calculate the inverse of a matrix unless absolutely necessary. It is more computationally efficient to solve the linear algebra system if that is all you need.\nNumpy can also do this for you.\nimport numpy as np\nA = [[4, 6, 2],\n     [3, 4, 1],\n     [2, 8, 13]]\n\ns = [9, 7, 2]\n\nr = np.linalg.solve(A, s)\n\n\n\n\nThe determinant of a linear transformation measures how much areas/volumes change during the transformation.\n\nThe matrix scales space. Creating \\(e'_1\\) and \\(e'_2\\) from our original basis vectors.\nThe space has been expanded a factor of \\(a\\) horizontally and a factor of \\(d\\) vertically. The total space has been expanded by a factor \\(ad\\)\nEverything in the space has grown by a factor \\(ad\\)\nThis is the determinant of the transformation matrix. the determinant is how much we grow/shrink space. More precisely, it is the factor by which a given area increases or decreases.\n{:width=“400”}\nA more concrete example,\n{:width=“400”}\nHowever, these transformations are not always equal.\nAdding \\(b\\) into the matrix;\n{:width=“400”}\nThe area, the determinant, is still the same \\(ad\\)\n{:width=“400”}\nTo get the area of a general matrix, such as;\n\\[ A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\]\nThe maths gets a little complex; but is \\(ad - bc\\)\n{:width=“450”}\nThis is finding the determinant of A\n\\(\\lVert a \\rVert = ad - bc\\)\nIf a 2D matirx transforms the space such that all points fit onto a single line or even a single point, then the determinant will be 0.\n{:width=“450”}\nIf a transformation has a determinant of 0, it is transforming the space into a smaller dimension. For a 2-dimenisonal space this is putting everything onto a single line, but the concept applies to \\(n\\) dimensions.\nWhen we “lose a dimensions”, the columns must be linearly dependent.\nOrientation\nYou can scale an area by a negative amount. As we’ve seen with vector operations, this does not mean shrinking the area but rather reversing it’s orientation. “flipping space”. They “invert the orientation of space.\n{:width=“450”}\nIn a 3D space, the determinant tells us how much volume gets scaled.\n\n\n\nTo get the inverse of a 2x2 matrix we flip the terms on the leading diagonal and take the minus of the off-diagonal terms\n\\[\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\\]\nMultiply these out to get the determinant;\n\\[\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix} = \\begin{pmatrix} ad-bc & 0 \\\\ 0 & ad-bc \\end{pmatrix}\\]\nThen multiplying by \\(\\frac{1}{ad-bc}\\) gives us the identity matrix\n\\[\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\]\n\nThe determinant here is what we need to divide the inverse matrix by in order for it to probably be an inverse.\n\nKnowing how to find the determinant in a general sense doesn’t add much to the learning process. Unlike the row echelon process we followed previously. Follow QR decomposition if interested.\nLinear independence\nTransformations can also remove linear independence.\n\\(A = \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix}\\) will transform our identity matrix to a single line\n\\(e'_1\\) and \\(e'_2\\) become multiples of one another.\nThe matrix is transforming every point in space along a line. Therefore the determinant is going to be 0.\n{:width=“450”}\nIf we’re working in 3D space, a 3x3 matrix and one of the new basis vectors is just a linear multiple of the other two (ie not linearly independent), the new space is a plane.\n\na plane is a flat, two-dimensional surface that extends infinitely far. A plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension)\n\nAgain, as turning a 2D shape into a single dimensional line, the determinant, would be zero.\nFor example,\nIn our matirx - row 3 = row1 + row2 - column 3 = 2*column1 + column2\n\\[\\begin{pmatrix} 1 & 1 & 3 \\\\ 1 & 2 & 4 \\\\ 2 & 3 & 7 \\end{pmatrix} \\begin{bmatrix} a\\\\b\\\\c\\end{bmatrix} = \\begin{pmatrix} 12\\\\17\\\\29\\end{pmatrix}\\]\nThis matrix doesn’t describe three independent basis vectors. It doesn’t describe any 3D space but collapses into a 2D space.\nAs a result, we cannot reduce this to row echelon form.\nIf you take away the first row from the second, and then the first and second row from the third row, you end up with;\n\\[\\begin{pmatrix} 1 & 1 & 3 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{bmatrix} a\\\\b\\\\c\\end{bmatrix} = \\begin{pmatrix} 12\\\\5\\\\0\\end{pmatrix}\\]\n\\(0c = 0\\) isn’t very helpful\nThere are essentially an infinite number of solutions for \\(c\\), any value of \\(c\\) would work.\nAs a result, we cannot solve this.\nIf you think of this from a simultaneous equation point of view, in trying to solve “apples, bananas, and carrots”. It’s equivalent to the third time going to the copy and ordering a copy of the first two orders. You gathered no new information.\n\nwhere the basis vectors describing the matrix are not linearly independent, then the determinant is zero, and that means I can’t solve the system of simultaneous equations. Which means I can’t invert the matrix because I can’t take one over the determinant either. That means I’m stuck, this matrix has no inverse\n\nDoing a transformation that collapses the number of dimensions in space comes at a cost.\nThe inverse of a matrix, allows us to un-do our transformation. You cannot un-do, moving from a 3D space to a 2D plane.\n\n\n\nWrite a function that will test if a 4×4 matrix is singular, i.e. to determine if an inverse exists, before calculating it.\nYou shall use the method of converting a matrix to echelon form, and testing if this fails by leaving zeros that can’t be removed on the leading diagonal.\nimport numpy as np\n\ndef isSingular(A) :\n    B = np.array(A, dtype=np.float_) \n    \"\"\"Make B as a copy of A. We're going to alter it's values\"\"\"\n    try:\n        fixRowZero(B)\n        fixRowOne(B)\n        fixRowTwo(B)\n        fixRowThree(B)\n    except MatrixIsSingular:\n        return True\n    return False\n\n\"defines our error flag\"\nclass MatrixIsSingular(Exception): pass    \n\"\"\"\nFor Row Zero, all we require is the first element is equal to 1.\nWe'll divide the row by the value of A[0, 0].\nThis will get us in trouble though if A[0, 0] equals 0\n    , so first we'll test for that,\nIf this is true, we'll add one of the lower rows to the first one before the division.\nWe'll repeat the test going down each lower row until we can do the division.\nThere is no need to edit this function.\n\"\"\"\ndef fixRowZero(A) :\n    if A[0,0] == 0 :\n        A[0] = A[0] + A[1]\n    if A[0,0] == 0 :\n        A[0] = A[0] + A[2]\n    if A[0,0] == 0 :\n        A[0] = A[0] + A[3]\n    if A[0,0] == 0 :\n        raise MatrixIsSingular()\n    A[0] = A[0] / A[0,0]\n    return A\n\"\"\"First we'll set the sub-diagonal elements to zero, i.e. A[1,0].\nNext we want the diagonal element to be equal to one.\nWe'll divide the row by the value of A[1, 1].\nAgain, we need to test if this is zero.\nIf so, we'll add a lower row and repeat setting the sub-diagonal elements to zero.\nThere is no need to edit this function.\"\"\"\ndef fixRowOne(A) :\n    A[1] = A[1] - A[1,0] * A[0]\n    if A[1,1] == 0 :\n        A[1] = A[1] + A[2]\n        A[1] = A[1] - A[1,0] * A[0]\n    if A[1,1] == 0 :\n        A[1] = A[1] + A[3]\n        A[1] = A[1] - A[1,0] * A[0]\n    if A[1,1] == 0 :\n        raise MatrixIsSingular()\n    A[1] = A[1] / A[1,1]\n    return A\ndef fixRowTwo(A) :\n    \"\"\"Insert code below to set the sub-diagonal elements of row two to zero (there are two of them).\"\"\"\n    A[2] = A[2] - A[2,0] * A[0]\n    A[2] = A[2] - A[2,1] * A[1]\n    \n    \"Next we'll test that the diagonal element is not zero.\"\n    if A[2,2] == 0 :\n        \"Insert code below that adds a lower row to row 2.\"\n        A[2] = A[2] + A[3]\n        \"Now repeat your code which sets the sub-diagonal elements to zero.\"\n        A[2] = A[2] - A[2,0] * A[0]\n        A[2] = A[2] - A[2,1] * A[1]\n        \n    if A[2,2] == 0 :\n        raise MatrixIsSingular()\n    \"Finally set the diagonal element to one by dividing the whole row by that element.\"\n    A[2] = A[2] / A[2,2]\n    return A\n\ndef fixRowThree(A) :\n    \"\"\"Insert code below to set the sub-diagonal elements of row three to zero.\"\"\"\n    A[3] = A[3] - A[3,0] * A[0]\n    A[3] = A[3] - A[3,1] * A[1]\n    A[3] = A[3] - A[3,2] * A[2]\n    \n    \"Complete the if statement to test if the diagonal element is zero.\"\n    if A[3,3] == 0:\n        raise MatrixIsSingular()\n    \"Transform the row to set the diagonal element to one.\"\n    A[3] = A[3] / A[3,3]\n    return A\n\n\n\n\n\n\nThe Einstein summation convention is a way to write matrix transformations. It writes down what the actual operations are on the elements of the matrix.\nWhen we started, we said that multiplying a matrix by a vector or with another matrix is a process of taking every element in each row in turn, multiplied with corresponding element in each column in the other matrix, and adding them all up and putting them in place.\n{:width=“400”}\nEinstein convention,says, you have a sum over some elements \\(j\\), for all the possible combinations of \\(i\\) and \\(k\\). As this is a repeated index, don’t bother with the sum;\n\\[ ab_{ik} = \\sum_{j} a_{ij} b_{jk} = a_{ij} b_{jk}\\]\nWe’d only have to run for loops over \\(i\\), \\(j\\) and \\(k\\). Then use an accumulator on the \\(j\\)’s to find the elements of the product matrix \\(AB\\).\nWe can multiply matrices that are not square (ie same numbers of rows in A as the same number of columns in B)\nAs long as you have the same number of \\(j\\)s you can multiply them together.\n{:width=“400”}\n\nNow, all sorts of matrix properties that you might want, inverses and so on, determinants, all start to get messy and mucky, and you sometimes can’t even compute them when you’re doing this sort of thing.\n\nRevisiting the dot product\nIf we have two column vectors (a single column with n elements);\n\\(U = \\begin{bmatrix} u_i \\end{bmatrix}\\) and \\(V = \\begin{bmatrix} v_i \\end{bmatrix}\\)\nThe summation convention is \\(u_i v_i\\), we repeat over all the \\(i\\)s and add.\nThat’s the same as doing a matrix transformation, where \\(u\\) is a row and \\(v\\) a column.\n{:width=“400”}\nProjection is symmetric. Projection is the dot product.\nIf you project \\(\\hat{u}\\) down onto \\(\\hat{e_1}\\) and vice versa, the two triangles split by the pink line are identical.\n{:width=“400”}\n\nthere is this connection between this numerical thing, matrix multiplication, and this geometric thing, projection\n\nThat’s why we talk about a matrix multiplication with a vector as being the projection of that vector onto the vectors composing the matrix, the columns of the matrix\n\n\n\nIn traditional notion we might write \\(\\sum^3 A_{ij} v_j = A_{i1} v_1 + A_{i2} v_2 + A_{i3} v_3\\)\nWith Einstein summation convention we can simply write \\(A_{ij}v{j}\\) and know that we sum over \\(j\\) because it appears twice.\nWe can multiply any matrices together as long as the terms which we sum over have the same number of elements - need to clarify\nWe can multiply an \\(m \\times n\\) matrix with an \\(n \\times k\\) matrix, and the resultant matrix will be an \\(m \\times k\\) matrix.\n\\[A=\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 0 & 1\\end{bmatrix} B=\\begin{bmatrix} 1 & 1 & 0\\\\0 & 1 & 1\\\\1 & 0 & 1\\end{bmatrix}\\]\n\\[C_{mn} = A_{mj} B{jn}\\]\n\\[C_{21} = A_{2j} B_{j1} = 5\\]\nEquivalent to \\(C_{21} = A_{2j} B_{j1} = A_{21} B_{11} + A_{22} B_{21} + A_{23} B_{31}\\)\n(4*1) + (0*0) + (1*1) = 5\nelements in second in A with elements in first column in B\n\\(C_{11}\\) for example is;\n(1*1) + (2*0) + (3*1) = 4\n\\[C=\\begin{bmatrix} 4 & 3 & 5 \\\\ 5 & 4 & 1\\end{bmatrix}\\]\nCalculate the product\nQuestion\n\\[\\begin{bmatrix} 2 & 4 & 5 & 6\\end{bmatrix} \\begin{bmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 1\\end{bmatrix}\\]\nAnswer 30\nQuestion\n\\[\\begin{bmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 1\\end{bmatrix} \\begin{bmatrix} 2 & 4 & 5 & 6\\end{bmatrix}\\]\nAnswer, produces a 4x4\nQuestion\n\\[\\begin{bmatrix} 2 & 4 & 5 & 6 \\\\6 & 12 & 15 & 18\\\\4 & 8 & 10 & 12\\\\2 & 4 & 5 & 6\\end{bmatrix}\\]\n- the row is repeated each time\n- multiplied by the relevant element in the first vector\nQuestion\n\\[\\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\\\ 1 & 0\\end{bmatrix} \\begin{bmatrix} 0 & 1 & 4 & -1 \\\\ -2 & 0 & 0 & 2\\end{bmatrix}\\]\n# Answer\n# Column1;\n(2*0) + (-1*-2) = 2\n(0*0) + (3*-2) = -6\n(1*0) + (0*-2) = 0\n\n# Column2;\n(2*1) + (-1*0) = 2\n(0*1) + (3*0) = 0\n(1*1) + (0*0) = 1\n\n# Column3;\n(2*4) + (-1*0) = 8\n(0*4) + (3*0) = 0\n(1*4) + (0*0) = 4\n\n# Column3;\n(2*-1) + (-1*2) = -4\n(0*-1) + (3*2) = 6\n(1*-1) + (0*2) = -1\n\\[\\begin{bmatrix} 2 & 2 & 8 & -4 \\\\ -6 & 0 & 0 & 6 \\\\ 0 & 1 & 4 & -1\\end{bmatrix}\\]\nQuestion\n\\(D = ABC\\) where - A is a 53 matrix - B is a 37 matrix - C is a 7*4 matrix\nWhat are the dimensions of \\(D\\)?\nThe size of \\(AB\\) is 5*7\nThe size of \\((AB)C\\) == \\(A(BC)\\) therefore the matrices can be multiplied together.\n\\(D\\) is a 5*4 matrix\nQuestion\nCalculate the product;\n\\[\\begin{bmatrix} 1 & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix}\\]\nThe identity matrix doesn’t change the second matrix, so remains \\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix}\\)\nLet \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors with \\(n\\) elements. Write the dot product;\n–TODO\n\n\n\nShadows are an example of a transformation that reduces the number of dimensions. For example, 3D objects in the world cast shadows on surfaces that are 2D.\n{:width=“400”}\nThe sun is sufficiently far away that effectively all of its rays come in parallel to each other. We can describe their direction with the unit vector \\(\\hat{\\mathbf{s}}\\)\nWe can describe the 3D coordinates of points on objects in our space with the vector \\(\\mathbf{r}\\). Objects will cast a shadow on the ground at the point \\(\\mathbf{r}'\\) along the path that light would have taken if it hadn’t been blocked at \\(\\mathbf{r}\\), that is, \\(\\mathbf{r}' = \\mathbf{r} + \\lambda \\hat{\\mathbf{s}}\\)\nThe ground is at \\(\\mathbf{r}'_3 = 0\\); by using \\(\\mathbf{r}'.\\hat{\\mathbf{e}}_3 = 0\\), we can derive the expression, \\(\\mathbf{r}.\\hat{\\mathbf{e}}_3 + \\lambda s_3 = 0\\), (where \\(s_3 = \\hat{\\mathbf{s}}.\\hat{\\mathbf{e}}_3\\)).\nRearrange this expression for \\(\\lambda\\) and substitute it back into the expression for \\(\\mathbf{r}'\\), in order to get \\(\\mathbf{r}'\\) in terms of \\(\\mathbf{r}\\).\n\\[ \\mathbf{r}' = r - \\hat{s}(r.\\hat{e_3})/s_3\\]\nRemember that; The scalar projection of \\(s\\) on \\(r\\);\n\\[ proj_rs = \\frac{s.r}{\\lVert r \\rVert}\\]\n\\(\\mathbf{r}'\\) can be written as a linear transformation of \\(r\\). This means we should be able to write \\(\\mathbf{r}' = Ar\\) for some matrix \\(A\\)\nUsiing Einstein summation convention, we can rewrite our previous answer as; \\[r'_i = r_i - s_i[\\hat{e_3}]_jr_j/s_3\\] or \\[r'_i = r_i - s_ir_3/s_3\\] or \\[r'_i = (I_{ij} - s_i[\\hat{e_3}]_j/s_3)r_j\\] or \\[r'_i = (I_{ij} - s_iI_{3j}/s_3)r_j\\]\nWe can now give an expression for \\(A\\) in its component form by evaluating the components A_{ij} for each row \\(i\\) and column \\(j\\).\n\\(A\\) takes a 3D vector \\(r\\) and transforms it into a 2D vector \\(r'\\). Therefore the matrix will be a 2x3. - remember; the columns of a matrix are the vectors in the new space that the unit vectors of the old space transform to\n\\[ A = \\begin{bmatrix} 1 & 0 & -s_1/s_3\\\\ 0 & 1 & -s_2/s_3\\end{bmatrix}\\]\nIf you were to evaluate it’s third row it would be all zeros as \\(r'\\) has no value in the third dimension; \\(A3 = [0, 0, 0]\\)\nAssume the Sun’s rays come in the direction; \\[ \\hat{s} = \\begin{bmatrix} 4/13 \\\\ -3/13 \\\\ -12/13 \\end{bmatrix}\\]\nConstruct the matrix A, apply it to a point, on an object in our space to find the coordinate of that point’s shadow;\n\\(r = \\begin{bmatrix} 6 \\\\ 2 \\\\ 3 \\end{bmatrix}\\), \\(A = \\begin{bmatrix} 1 & 0 & -4/13 / -12/13\\\\ 0 & 1 & 3/13 / -12/13\\end{bmatrix}\\)\n\\(r' = A_{ij}r{j}\\)\n# r'\n(1 * 6) + (0 * 2) + (0.333 * 3)\n(0 * 6) + (1 * 2) + (-0.25 * 3)\n\n[7, 1.25]\nAnother use of non-square matrices is applying a matrix to a list of vectors.\n{:width=“600”}\nObserve that it’s the same result as treating the columns as separate vectors and calculating them individually.\nUsing \\(\\hat{s} = \\begin{bmatrix} 4/13 \\\\ -3/13 \\\\ -12/13 \\end{bmatrix}\\), apply A to the matrix;\n\\(R = \\begin{bmatrix} 5&-1&-3&-7 \\\\ 4&-4&1&-2 \\\\ 9&3&0&12\\end{bmatrix}\\)\n\\(A = \\begin{bmatrix} 1 & 0 & 0.333\\\\ 0 & 1 & 0.25\\end{bmatrix}\\)\n# R`\nRp = [[a,  b,  c,  d],\n      [e,  f,  g,  h]]\n\na = (1 * 5) + (0 * 4) + ((-4/13)/(-12/13) * 9)\ne = (0 * 5) + (1 * 4) + (-0.25 * 9)\n\nb = (1 * -1) + (0 * -4) + ((-4/13)/(-12/13) * -3)\nf = (0 * -1) + (1 * -4) + (-0.25 * -3)\n\nc = (1 * -3) + (0 * 1) + ((-4/13)/(-12/13) * 0)\ng = (0 * -3) + (1 * 1) + (-0.25 * 0)\n\nd = (1 * 7) + (0 * -2) + ((-4/13)/(-12/13) * 12)\nh = (0 * 7) + (1 * -2) + (-0.25 * 12)\n\nRp = [[8.0, -2.0, -3.0, 11.0],\n      [1.75, -3.25, 1.0, -5.0]]\n\n\n\n\nThe columns of a transformation matrix, are the axes of the new basis vectors of the mapping in my coordinate system.\nThe lines in yellow describe the world of Panda Bear. To him, these vectors are [1,0] and [0,1] but in my frame, they are [3,1] and [1,1]\n{:width=“500”}\nBear’s transformation matrix is therefore; \\(\\begin{bmatrix} 3&1 \\\\ 1&1 \\end{bmatrix}\\)\nNow if we take a vector in Bear’s world, we can understand it in my coordinate system.\nThe transformation matrix is Bear’s basis vectors in my coordinate system.\nThe vector 1/2[3, 1] or [3/2, 1/2] in Bear’s world becomes;\n[(3 * 3/2) + (1 * 1/2), (1 * 3/2) + (1 * 1/2)] or [5, 2]\n{:width=“500”}\nHowever, we need to figure out how to go the other way. Translating my world to Bear’s world.\nTo get my basis vectors in Bear’s coordinates, we need to take the inverse of the transformation matrix; - flip on the leading diagonal, and put a minus on the off diagonal terms - Recall that when a matrix is transformed into its diagonal form, the entries along the diagonal are the eigenvalues of the matrix - this can save lots of calculation!\n\n\\(\\begin{bmatrix} 1&-1 \\\\ -1&3 \\end{bmatrix}\\)\nthen divide by the determinant (three minus one over one, 3-1/1 = 2), so multiply by a half\n\\(\\frac{1}{2} \\begin{bmatrix} 1&-1 \\\\ -1&3 \\end{bmatrix}\\)\nremember - the determinant is how much we grow/shrink space\n\nExample\nHere Bear’s world is going to be an orthonormal basis vector set (they form a v)\nyou can to a dot product to verfiy they are at 90 degrees to one another (orthogonal)\n{:width=“400”}\nBear’s transformation matrix, ie the matrix that converts a vector in Bear’s world to my coordinate system;\n\\(B = \\frac{1}{\\sqrt2} \\begin{bmatrix} 1&-1 \\\\ 1&1 \\end{bmatrix}\\)\nThe inverse, or \\(B^{-1}\\)\n\\(B = \\frac{1}{\\sqrt2} \\begin{bmatrix} 1&1 \\\\ -1&1 \\end{bmatrix}\\)\nBecause Bear’s vectors are orthogonal, we can do this using projections, rather than having to calculate the transformation matrices.\nTake my version of the vector and dot it with Bear’s axis, then we get the answer of the vector in Bear’s world;\nfirst component, the vector * the first axis; \\[\\frac{1}{\\sqrt2} \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} . \\frac{1}{\\sqrt2} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{2} 4 = 2\\]\n1/sqrt{2} * 1/sqrt{2} = 1/2\n(3 * 1) + (1 * 1) = 4\nsecond component, the vector * the second axis; \\[\\frac{1}{\\sqrt2} \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} . \\frac{1}{\\sqrt2} \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} = 1\\]\n1/sqrt{2} * 1/sqrt{2} = 1/2\n(3 * 1) + (1 * -1) = 2\nIn this case the lengths are one. When using projections normally we would need to normalise by their lengths (see scalar projections).\nIf Bear’s vectors are orthogonal to one another, we don’t have to use the matrix transformations, we can use the dot product.\n\n\nDoing a 45 degree rotation in my world is done using the following matrix;\n\\[\\frac{1}{\\sqrt2}  \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix}\\]\n{:width=“400”}\nHowever, we don’t know what a 45 degree transformation looks like in Bear’s world.\nFirst, we need to transform the vector into our world using the matrix \\(B\\), which is the Bear’s basis vector in our world;\n\\(B = \\begin{bmatrix} 3&1 \\\\ 1&1 \\end{bmatrix}\\)\nThen apply the transformation,\nThen reverse the result using \\(B^{-1}\\)\n\\(B^{-1} = \\frac{1}{2} \\begin{bmatrix} 1&-1 \\\\ -1&3 \\end{bmatrix}\\)\nOr written as; \\(B^{-1} R B = R_B\\)\n{:width=“400”}\n\nIf we want to transform to normal form normal coordinate systems, then the translation matrices also change. We have to be mindful of that\nWe’ve got the transformation matrix R, wrapped around by B, B is the minus 1, that does the translation from my world to the world of the new basis system.\n\nPractice\nFinish off the calculation;\n\\[\\frac{1}{2} \\begin{bmatrix} 3&-1 \\\\ -1&1\\end{bmatrix} * \\frac{1}{\\sqrt2} \\begin{bmatrix} 2&0 \\\\ 4&2\\end{bmatrix} = \\frac{1}{\\sqrt2} \\begin{bmatrix} 1&-1 \\\\ 1&1\\end{bmatrix}\\]\n# working\n# first matrix row * second matrix column\nx1y1 = (1.5 * 2) + (-0.5 * 4) = 1\nx1y2 = (1.5 * 0) + (-0.5 * 2) = -1\nx2y1 = (-0.5 * 2) + (0.5 * 4) = 1\nx2y2 = (-0.5 * 0) + (0.5 * 2) = 1\n\n\n\nWe can transpose a matrix, where we interchange all the elements of the rows and columns of the matrix; \\(A^T_{ij} = A_{ij}\\)\n\\[\\begin{bmatrix} 1&2 \\\\ 3&4\\end{bmatrix}^T = \\begin{bmatrix} 1&3 \\\\ 2&4\\end{bmatrix}\\]\nWe interchange the elements that are off the diagonal. So the one and the four stay where they are.\nBecause if is, I find \\(i\\) and \\(j\\) are the same; elements at the coorindates 1,1 would stay the same, the same for 2,2. But the element 1,2 interchange to the element 2,1.\nImagine we have a square matrix \\(A\\), with dimensions n x n. This defines a transformation (ie we apply to vectors to transform them), the columns in this matrix are the basis vectors in a new space.\nIn this matrix;\n\nthe vectors are orthogonal to each other and of unit length\n\n\\(a_i . a_j = 0\\) if \\(i \\neq j\\)\n\\(a_i . a_j = 1\\) if \\(i = j\\)\n\n\nIf we multiply this matrix by it’s transpose \\(A^T\\), the columns \\(a_1\\) to \\(a_n\\) become rows.\nThe result of the multiplication is an identity matrix;\n{:width=“400”}\n\\(A^T\\) is a valid inverse of \\(A\\)\n\nA set of unit length basis vectors that are all perpendicular to each other are called an orthonormal basis set\n\nThey must meet the criteria; - \\(a_i . a_j = 0\\) if \\(i \\neq j\\) - \\(a_i . a_j = 1\\) if \\(i = j\\)\n\nThe matrix composed of them is called an orthogonal matrix\n\nBecause all the basis vectors in an orthogonal matrix are of unit length, it must scale space by a factor of one.\nThe determinant of an orthogonal matrix must be either plus or minus one (depending on if you do \\(A^T * A or A * A^T\\)).\nWe want our transformation matrix to be an orthogonal matrix, we want the basis vectors to be orthonormal.\n\nthe inverse is easy to compute\nthe transformation is reversible as it doesn’t collapse the space\nthe projection is the dot product\nif arranged in the right order the determinant is one\n\n\n\n\nIf we assume we already have some linearly independent vectors that span the space we’re interested in, we can construct an orthonormal basis vector set.\nWe can check linear independence by calculating the determinant. If there are linearly independent the determinant will be 0.\n{:width=“400”}\nHowever, these vectors are not orthogonal to one another and are not of unit length.\nThe Gram-Schmidt process allows us to transform this vector set into an orthonormal set.\nTake the first vector \\(v_1\\) and normalise it to be on unit length;\n\\(e_1 = \\frac{v_1}{\\lVert v_1 \\rVert}\\)\n\\(v_2\\) can now be thought of as (1) a component of that’s in the direction of \\(e_1\\) plus (2) a component that is perpendicular to \\(e_1\\), which we can find by taking the projection of \\(v_2\\) onto \\(e_1\\)\n{:width=“250”}\nThe vector projection is \\(\\frac{(v_2 . e_1)}{\\lVert e_1 \\rVert}\\) or simply (\\(v_2 . e_1\\)) as \\(e_1\\) has already been normalised, it has size 1. To get this as a vector, not just a number we multiply by \\(e_1\\)\n\\(v_2 = (v_2 . e_1) e_1\\) + \\(u_2\\)\nRewritten as;\n\\(u_2 = v_2 - (v_2 . e_1) e_1\\)\nThen normalised to unit length\n\\(e_2 = \\frac{u_2}{\\lVert u_2 \\rVert}\\)\n{:width=“400”}\n\\(v_3\\) is not linear combination of \\(v_1\\) and \\(v_2\\), therefore it is not in the plane defined by \\(v_1\\) and \\(v_2\\) and following that, will not be defined by \\(e_1\\) and \\(e_2\\).\nAs a result, we need to project \\(v_3\\) down onto the plane of \\(e_1\\) and \\(e_2\\) and the projection will be some vector in the plane composed of \\(e_1\\)s and \\(e_2\\)s\nThe perpendicular vector \\(u_3\\) is what is left when you remove the elements of \\(v_3\\) made up of \\(e_1\\) and \\(e_2\\)\n\\(u_3 = v_3 - (v_3 . e_1)e_1 - (v_3 . e_2)e_2\\)\nThen normalise;\n\\(e_3 = \\frac{u_3}{\\lVert u_3 \\rVert}\\)\n\\(e_1, e_2, e_3\\) now form an orthonormal basis set\nWe can write a function to perform the Gram-Schmidt procedure. Taking in a list of vectors and forming an orthonormal basis set.\n# Remember we access elements in matrix as;\n# individual elements\nA[n, m]\n# rows\nA[n]\n# columns\nA[:, m]\n\n# calculate the dot product using @\nu @ v\nSay we have 4 basis vectors;\nimport numpy as np\nimport numpy.linalg as la\n\nverySmallNumber = 1e-14 # That's 1×10⁻¹⁴ = 0.00000000000001\n\n# Our first function will perform the Gram-Schmidt procedure for 4 basis vectors.\n# We'll take this list of vectors as the columns of a matrix, A.\n# We'll then go through the vectors one at a time and set them to be orthogonal\n# to all the vectors that came before it. Before normalising.\ndef gsBasis4(A) :\n    B = np.array(A, dtype=np.float_) # Make B as a copy of A, since we're going to alter it's values.\n    # The zeroth column is easy, since it has no other vectors to make it normal to.\n    # All that needs to be done is to normalise it. I.e. divide by its modulus, or norm.\n    B[:, 0] = B[:, 0] / la.norm(B[:, 0])\n    # For the first column, we need to subtract any overlap with our new zeroth vector.\n    B[:, 1] = B[:, 1] - B[:, 1] @ B[:, 0] * B[:, 0]\n    # If there's anything left after that subtraction, then B[:, 1] is linearly independant of B[:, 0]\n    # If this is the case, we can normalise it. Otherwise we'll set that vector to zero.\n    if la.norm(B[:, 1]) > verySmallNumber :\n        B[:, 1] = B[:, 1] / la.norm(B[:, 1])\n    else :\n        B[:, 1] = np.zeros_like(B[:, 1])\n    # Now we need to repeat the process for column 2\n    # Subtract the overlap with the zeroth vector,\n    B[:, 2] = B[:, 2] - B[:, 2] @ B[:, 0] * B[:, 0]\n    # Subtract the overlap with the first.\n    B[:, 2] = B[:, 2] - B[:, 2] @ B[:, 1] * B[:, 1]\n    # Again we'll need to normalise our new vector.\n    if la.norm(B[:, 2]) > verySmallNumber :\n        B[:, 2] = B[:, 2] / la.norm(B[:, 2])\n    else :\n        B[:, 2] = np.zeros_like(B[:, 2])    \n    # Finally, column three:\n    # Subtract the overlap with the first three vectors.\n    B[:, 3] = B[:, 3] - B[:, 3] @ B[:, 0] * B[:, 0]\n    B[:, 3] = B[:, 3] - B[:, 3] @ B[:, 1] * B[:, 1]\n    B[:, 3] = B[:, 3] - B[:, 3] @ B[:, 2] * B[:, 2]    \n    \n    # Now normalise if possible\n    if la.norm(B[:, 3]) > verySmallNumber :\n        B[:, 3] = B[:, 3] / la.norm(B[:, 3])\n    else :\n        B[:, 3] = np.zeros_like(B[:, 3])     \n    \n    # Finally, we return the result:\n    return B\nHowever, we can generalise the procedure;\ndef gsBasis(A) :\n    B = np.array(A, dtype=np.float_) # Make B as a copy of A, since we're going to alter it's values.\n    # Loop over all vectors, starting with zero, label them with i\n    for i in range(B.shape[1]) :\n        # Inside that loop, loop over all previous vectors, j, to subtract.\n        for j in range(i) :\n            B[:, i] = B[:, i] - B[:, i] @ B[:, j] * B[:, j]\n        # do the normalisation test for B[:, i]\n        if la.norm(B[:, i]) > verySmallNumber :\n            B[:, i] = B[:, i] / la.norm(B[:, i])\n        else :\n            B[:, i] = np.zeros_like(B[:, i])\n            \n    # Finally, we return the result:\n    return B\n\n# This function uses the Gram-schmidt process to calculate the dimension\n# spanned by a list of vectors.\n# Since each vector is normalised to one, or is zero,\n# the sum of all the norms will be the dimension.\ndef dimensions(A) :\n    return np.sum(la.norm(gsBasis(A), axis=0))\n\n\n\nSay we want to know what a vector looks like when reflected in some plane.\nWe have three vectors, the first two are within the plane of the mirror, the third is outside the plane.\n\\[v_1 = \\begin{bmatrix} 1\\\\1\\\\1\\end{bmatrix} v_2 = \\begin{bmatrix} 2\\\\0\\\\1\\end{bmatrix} v_3 = \\begin{bmatrix} 3\\\\1\\\\-1\\end{bmatrix}\\]\n{:width=“200”}\nUsing the Grahm-Schimdt procedure to get some orthonormal basis vectors that desrribe the plane and its normal \\(v_3\\)\n\\(e_1\\) is the normalised version of \\(v_1\\). \\(v_1\\) is a length 3 (\\(\\sqrt{1^2 + 1^2 + 1^2}\\))\n\\[ e_1 = \\frac{v_1}{\\lVert v_1 \\rVert} = \\frac{1}{\\sqrt3} \\begin{bmatrix} 1\\\\1\\\\1\\end{bmatrix}\\]\n\\(u_2\\) is \\(v_2\\) minus some number of \\(e_1\\)s -> more precisely; the projection of \\(v_2\\) onto \\(e_1\\) (\\(v_2 . e_1\\)) multiplied by \\(e_1\\)\n\\[u_2 = v_2 - (v_2 . e_1)e_1 = \\begin{bmatrix} 2\\\\0\\\\1\\end{bmatrix} - (\\begin{bmatrix} 2\\\\0\\\\1\\end{bmatrix} . \\frac{1}{\\sqrt3} \\begin{bmatrix} 1\\\\1\\\\1\\end{bmatrix}) \\frac{1}{\\sqrt3} \\begin{bmatrix} 1\\\\1\\\\1\\end{bmatrix}\\]\nthe root threes come outside, so become 1/3 \n[2, 0, 1] dotted with [1, 1, 1] is 3\nso they cancel out to 1\n\nso [2, 0, 1] - [1, 1, 1]\n\\[ = \\begin{bmatrix} 1\\\\-1\\\\0\\end{bmatrix}\\]\n\\(e_2\\) is equal to the normalised version of \\(u_2\\)\n\\[e_2 = \\frac{u_2}{\\lVert u_2 \\rVert} = \\frac{1}{\\sqrt2} \\begin{bmatrix} 1\\\\-1\\\\0\\end{bmatrix}\\]\nThen we need \\(u_3\\)\n\\[u_3 = v_3 - (v_3 . e_1)e_1 - (v_3 . e_2)e_2 = \\begin{bmatrix} 1\\\\1\\\\-2\\end{bmatrix}\\]\n\\[e_3 = \\frac{1}{\\sqrt6} \\begin{bmatrix} 1\\\\1\\\\-2\\end{bmatrix}\\]\nOur new transformation is \\(E\\), described by our new basis vectors\n\\[E = \\begin{pmatrix} \\begin{bmatrix}e_1\\end{bmatrix} \\begin{bmatrix}e_2\\end{bmatrix} \\begin{bmatrix}e_3\\end{bmatrix} \\end{pmatrix}\\]\nThis contains the plane (\\(e_1\\) and \\(e_2\\) and then the normal to the plane \\(e_3\\). It’s the bit of v3 that we can’t make by projecting on to v1 and v2, then of unit length)\nSay we have some vector \\(r\\) that we want to reflect down through the pane, and get \\(r'\\) on the other side;\n{:width=“400”}\nWe can think of \\(r\\) as composed of some vector within the pane (composed of \\(e_1\\)s and \\(e_2\\)s) - this the dotted line perpendicular to \\(e_1\\) and \\(e_2\\) and some vector that is normal (ie made up of \\(e_3\\)s) - this is the dotted line up to \\(r\\)\nWhen we reflect through the pane, the bit made up of \\(e_1\\) and \\(e_2\\) will be unchanged and the bit composed of \\(e_3\\)s will be inverted.\n\\[T_E = \\begin{pmatrix} \\begin{bmatrix}e_1\\end{bmatrix} \\begin{bmatrix}e_2\\end{bmatrix} \\begin{bmatrix}e_3'\\end{bmatrix} \\end{pmatrix}\\]\nGetting from \\(r\\) to \\(r'\\) is hard. As we saw with Bear.\n\nFirst we need to transform \\(r\\) into the basis plane using the inverse of our orthogonal basis matrix,\nThen transform it, do the reflection in the basis of the plane\nThen read that back into my basis using the orthogonal basis matrix.\n\nNote in this example we’re changing from our basis vectors to the plane’s and then translating back into our basis. So the \\(E\\) and the \\(E^{-1}\\) are flipped compared to when we were working with Bear.\n{:width=“250”}\n\\(E T_E E^{-1}r = r'\\)\nWritten out the whole thing looks a little ungodly, but that’s more down to the volume of arthimetic required than the complexity\n{:width=“400”}\nGeneralised, this is the process of reflecting a point in space in a mirror (we can transform something through the looking glass).\nIn the practical world of machine learning, this will be the technique used when transforming images of forces for the purpose of doing facial recognition. You transform a face from being side on to profile, and then using some form of neural network to do the recognition.\nPractice reflections\nPerform a transformation that is easy in a particular basis, but complicated in our starting basis.\nNamely we shall help Panda Bear determine what his reflection will look like in a mirror that he has placed at an angle.\nThe mirror lies along the first axis. But, as is the way with bears, his coordinate system is not orthonormal: so what he thinks is the direction perpendicular to the mirror isn’t actually the direction the mirror reflects in\nWrite a Python function that will produce a transformation matrix for reflecting vectors in an arbitrarily angled mirror.\n\\(T = E T_E E^{-1}\\)\nnote > the @ operator is used to combine vectors and/or matrices in the expected linear algebra way, i.e. it will be either the vector dot product, matrix multiplication, or matrix operation on a vector, depending on it’s input.\n\nThis is in contrast to the \\(*\\) operator, which performs element-wise multiplication, or multiplication by a scalar.\n\nimport numpy as np\nfrom numpy.linalg import norm, inv\nfrom numpy import transpose\nfrom readonly.bearNecessities import *\n\ndef build_reflection_matrix(bearBasis):\n\"\"\"Return the transformation T\"\"\"\n# built using an orthonormal basis set (E), created from Bear's Basis\n# and a transformation matrix (TE) in the mirror ccoordinates\"\"\"\n\n    # Use the gsBasis function on bearBasis to get the mirror's orthonormal basis\n    E = gsBasis(bearBasis) # bearBasis is a 2×2 matrix\n\n    # Write a matrix in component form that performs the mirror's reflection in the mirror's basis\n    # the mirror operates by negating the last component of a vector - one axis doesn't change\n    TE = np.array([0, 1],\n                   [0, -1]])\n\n    # Combine the matrices E and TE to produce your transformation matrix.\n    T = E @ TE @ inv(E) \n\n    return T\n\n\n\n\nWe’ll start by using geometric expressions (shapes in 2d) to conceptually understand “eigen-ness”\nEigen is perhaps most usefully translated from German to mean charactersitic. When we talk about an eigenproblem, we’re talking about finding the charactersitic properities of something.\nWe’ve seen that we can express linear transformations using matrices. These transformation operations include scalings, rotations and shears.\n\nA transformation in which all points along a given line L remain fixed while other points are shifted parallel to L by a distance proportional to their perpendicular distance from L. Shearing a plane figure does not change its area. The shear can also be generalized to three dimensions, in which planes are translated instead of lines. - Wolfram\n\n{:width=“250”}\nTypically, we have thought about how these transformations change a single vector. What about if the matrix was applied to all vectors in the space?\nWe can think of this by having a square, centred in the middle of our basis vectors and seeing how the shape is transformed.\nApplying a scaling of 2 in the vertical direction, the square becomes a rectangle. Applying a horizontal sheer gives us a parrallelegram.\n{:width=“400”}\nThe square helps us to understand what is happening to many vectors. However, some vectors remain on the same line they started on, while others do not.\nTake our initial square, with three vectors drawn on;\nIf we scale vertically, the diagonal vector will not be pointing in the same direction. Any other vector’s direction would have changed (apart from the horizontal and vertical, their angle and size will have changed)\n{:width=“250”}\nThe horizontal and vertical vectors are charactersitic of this particular trnasformation. They are the only ones that do not change. They are referred to as eigenvectors.\nBecuase the horiztonal’s length was unchanged, we say it has a “corresponding eigenvalue of one”, whereas the vertical eigenvector doubled in length, so it has a “corresponding eigenvalue of two”.\nEigenvectors are those laying on the same span as before the transformation. Then we measure how much their length has changed.\nTake a pure shear (where there is no rotation or scaling so the area is unchanged), here we would have one eigenvector, along the horiztonal;\n{:width=“250”}\nIn rotation, there are no eigenvectors.\n{:width=“250”}\nPractice\nIn all examples, we’ll start with the following vectors and apply a transformation \\(T\\)\n{:width=“250”}\n\\(T_1= \\begin{bmatrix} 2&0\\\\0&2 \\end{bmatrix}\\)\n\\(T_1\\) scales each vectors by 2, so all three can be considered eigenvectors.\n\\(T_2= \\begin{bmatrix} 3&0\\\\0&2 \\end{bmatrix}\\)\n\\(T_2\\) scales the x axis by 3 and the y axis by 2, so our purple vector will no long be on the same plane.\n\\(T_3= \\begin{bmatrix} 1&2\\\\0&1 \\end{bmatrix}\\)\n\\(T_3\\) is a sheer, the x axis is unchanged, but the angle and size of the other two vectors is changed along the x-axis\n\\(T_4= \\begin{bmatrix} 0&-1\\\\1&0 \\end{bmatrix}\\)\n\\(T_4\\) is an anti-clockwise rotation, so there will be no vectors that remain pointing along the same pane.\n\\(T_4= \\begin{bmatrix} 0&-1\\\\1&0 \\end{bmatrix}\\)\n\\(T_4\\) is an anti-clockwise rotation, so there will be no eigenvectors\n\\(T_5= \\begin{bmatrix} -1&0\\\\0&-1 \\end{bmatrix}\\)\n\\(T_5\\) is a reflection, all vectors will be pointing along the same pane, just in the opposite direction\n\\(T_6= \\begin{bmatrix} 2&1\\\\0&2 \\end{bmatrix}\\)\n\\(T_6\\) scales all vectors by 2, but alters the angle at which the orange and purple vectors are pointing\n{:width=“650”}\nTo summaise, eigenvectors are those that lay along the same path after applying a linear transformation to a space. Eigenvalues are the amount that each of those vectors has been stretched in the process (or negative if flipped).\nIn the practice examples, it appeared that rotation would leave us without any eigenvectors, however, 180 degree rotation, is equivalent to a reflection, where the vectors are pointing in the opposite direction. All vectors will be eigenvectors will an eigven value of -1.\nA transformation that is some combination of horizontal shearing and vertical scaling does have two eigenvectors. The first which is most obvious is the horiztonal vector. The second is between the organge and the pink vector. Though the concept is straight forward, there are not always easy to spot.\n{:width=“400”}\nThis problem is amplified in three or more dimensions, where we can’t simply use geometric representations to spot eigenvectors.\nIn 3D scaling and shearing work much the same way, but rotation works differently. The eigenvector represents the axis of rotation.\n{:width=“400”}\n\n\nAlegrabically eigenvectors (\\(x\\)) can be represented as;\n\\(Ax = \\lambda x\\)\nOn the lefthand side \\(A\\) represents a transformation matrix being applied to the vector \\(x\\). On the righthand side we are stretching the vector by some scalar factor lambda.\n\\(A\\) must be a square transformation (\\(n \\times n\\)) and \\(x\\) must be an \\(n\\) dimensional vector. Otherwise it’s shape would change, it wouldn’t just scale.\n\\((A - \\lambda I) x = 0\\) I represents an identity matrix, the same size as \\(A\\)\n\nWe didn’t need this in the first expression we wrote, as multiplying vectors by scalars is defined. However, subtracting scalars from matrices is not defined\n\nEither \\(x\\) is 0 or the contents of the brackets. However, we’re not interested we \\(x = 0\\), that means the vector has no length or direction, it’s a trivial solution.\nWe can test if a matrix operation will result in a 0 output by calculating its determinant.\n\\(det(A - \\lambda I) = 0\\)\nIn the case of a 2x2;\n\\[det\\begin{pmatrix} \\begin{pmatrix} a&b\\\\c&d \\end{pmatrix} - \\begin{pmatrix} \\lambda&0 \\\\ 0&\\lambda \\end{pmatrix} \\end{pmatrix} = 0\\]\nEvaluating this determinant, we get what is referred to as the characteristic polynomial\n\\[\\lambda^2 - (a+d)\\lambda + ad - bc = 0\\]\nOur eigenvalues are simply the solutions of this equation, and we can then plug these eigenvalues back into the original expression to calculate our eigenvectors.\nThis gets complex in high dimensions, but that’s why we have computers (they truly are a bicycle for the mind).\nLet’s take the example of a vertical scaling of 2\n\\(A = \\begin{pmatrix} 1&0\\\\0&2 \\end{pmatrix}\\)\nTake the determinant of A minus lambda I (\\(A - \\lambda I\\)) and set it to zero\n\\[det\\begin{pmatrix} 1-\\lambda&0\\\\0&2-\\lambda \\end{pmatrix} = 0 = (1-\\lambda)(2-\\lambda)\\]\nOur equation has solutions at lambda = 1 and lambda = 2, so we can substitute back in;\n\\[@\\lambda=1: \\begin{pmatrix} 1-1&0\\\\0&2-1\\end{pmatrix} \\begin{bmatrix} x_1\\\\x_2 \\end{bmatrix} = \\begin{pmatrix} 0&0\\\\0&1\\end{pmatrix} \\begin{bmatrix} x_1\\\\x_2 \\end{bmatrix} = \\begin{bmatrix} 0\\\\x_2\\end{bmatrix} = 0\\]\n\\[@\\lambda=2: \\begin{pmatrix} 1-2&0\\\\0&2-2\\end{pmatrix} \\begin{bmatrix} x_1\\\\x_2 \\end{bmatrix} = \\begin{pmatrix} -1&0\\\\0&0\\end{pmatrix} \\begin{bmatrix} x_1\\\\x_2 \\end{bmatrix} = \\begin{bmatrix} -x_1\\\\0\\end{bmatrix} = 0\\]\nAt lambda equals one, the \\(x_2\\) term must = 0. Our x axis \\(x_1\\) can equal anything, as long as there’s 0 in the vertical direction.\nWe express this as; \\(@\\lambda=1: x = \\begin{bmatrix} t\\\\0 \\end{bmatrix}\\)\nAt lambda equals two, we can express our eigen vector as not moving in the horizontal direction, any scaling along the vertical axis.\nWe express this as; \\(@\\lambda=2: x = \\begin{bmatrix} 0\\\\t \\end{bmatrix}\\)\nLet’s take the example of a 90 degree rotation, where we expect no eigenvectors.\n\\(A = \\begin{pmatrix} 0&-1\\\\1&0 \\end{pmatrix}\\)\nOur characteristic polynomial is;\n\\[\\lambda^2 - (a+d)\\lambda + ad - bc = 0\\]\nIn this case \\(\\lambda^2 + 1\\) as \\(a+d = 0\\), as is \\(a\\times d\\), and \\(b\\times c = -1\\), so minus -1 gives us \\(+1\\).\n\\(\\lambda^2 + 1 = 0\\)\nDoesn’t have any real numbered solutions at all. Hence, no real eigenvectors.\n\nWe saw that our approach required finding the roots of a polynomial of order n, i.e., the dimension of your matrix. Which means that the problem will very quickly stop being possible by analytical methods alone. So when a computer finds the eigensolutions of a 100 dimensional problem it’s forced to employ iterative numerical methods.\n\nPractice\nPractice calculating and solving the characteristic polynomial to find the eigenvalues of simple matrices.\nQ1\n\\(A = \\begin{pmatrix} 1&0\\\\0&2 \\end{pmatrix}\\), what is the characteristic polynomial, and the solutions to the characteristic polynomial?\n\\(\\lambda^2 - 3\\lambda + 2 = 0\\)\n\\(\\lambda_1 = 1, \\lambda_2 = 2\\)\nSelect the eigenvectors;\n\\(\\begin{bmatrix} 0\\\\2 \\end{bmatrix}\\) \\(\\begin{bmatrix} 0\\\\3 \\end{bmatrix}\\) \\(\\begin{bmatrix} 1\\\\0 \\end{bmatrix}\\)\n\nRecall that if a vector is an eigenvector of a matrix, then so is any (non-zero) multiple of that vector.\n\n\nOne way to check that a vector is an eigenvector is to simply apply the matrix transformation and see if this is the same as multiplying by a scalar. Another way is to calculate the eigenvector by hand.\n\nQ2\n\\(A = \\begin{pmatrix} 3&4\\\\0&5 \\end{pmatrix}\\), what is the characteristic polynomial, and the solutions to the characteristic polynomial?\n\\(\\lambda^2 - 8\\lambda + 15 = 0\\)\n\\(\\lambda_1 = 3, \\lambda_2 = 5\\)\nSelect the eigenvectors;\n\\(\\begin{bmatrix} 2\\\\1 \\end{bmatrix}\\) \\(\\begin{bmatrix} 3\\\\0 \\end{bmatrix}\\) \\(\\begin{bmatrix} -1\\\\-\\frac{1}{2} \\end{bmatrix}\\) \\(\\begin{bmatrix} 0\\\\0 \\end{bmatrix}\\)\nFor example\n\\(A \\times \\begin{bmatrix} 2\\\\1 \\end{bmatrix} = \\begin{bmatrix} 10\\\\5 \\end{bmatrix}\\) equivalent to scaling by 5\n\\(A \\times \\begin{bmatrix} -1\\\\-\\frac{1}{2} \\end{bmatrix} = \\begin{bmatrix} -5\\\\-2.5 \\end{bmatrix}\\) equivalent to scaling by 5\nQ3\n\\(A = \\begin{pmatrix} 1&0\\\\-1&4 \\end{pmatrix}\\), what is the characteristic polynomial, and the solutions to the characteristic polynomial?\n\\(\\lambda^2 - 5\\lambda + 4 = 0\\)\n\\(\\lambda_1 = 1, \\lambda_2 = 4\\)\nSelect the eigenvectors;\n\\(\\begin{bmatrix} 3\\\\1 \\end{bmatrix}\\) \\(\\begin{bmatrix} 0\\\\1 \\end{bmatrix}\\)\nQ4\n\\(A = \\begin{pmatrix} -3&8\\\\2&3 \\end{pmatrix}\\), what is the characteristic polynomial, and the solutions to the characteristic polynomial?\n\\(\\lambda^2 -25 = 0\\)\n\\(\\lambda_1 = -5, \\lambda_2 = 5\\)\nSelect the eigenvectors;\n\\(\\begin{bmatrix} -1\\\\-1 \\end{bmatrix}\\) \\(\\begin{bmatrix} 1\\\\1 \\end{bmatrix}\\) \\(\\begin{bmatrix} 4\\\\-1 \\end{bmatrix}\\)\nFor example\n\\(A \\times \\begin{bmatrix} 4\\\\-1 \\end{bmatrix} = \\begin{bmatrix} -20\\\\5 \\end{bmatrix}\\) equivalent to scaling by 5 and inversion\nIn this case \\(\\begin{bmatrix} 0\\\\2 \\end{bmatrix}\\) is not;\n\\(A \\times \\begin{bmatrix} 0\\\\2 \\end{bmatrix} = \\begin{bmatrix} 10\\\\6 \\end{bmatrix}\\) not very eigen\nQ5\n\\(A = \\begin{pmatrix} 5&4\\\\-4&-3 \\end{pmatrix}\\), what is the characteristic polynomial, and the solutions to the characteristic polynomial?\n\\(\\lambda^2 - 2\\lambda + 1 = 0\\)\n\\(\\lambda_1 = \\lambda_2 = 1\\)\nThis matrix has one repeated eigenvalue - which means it may have one or two distinct eigenvectors (which are not scalar multiples of each other).\nQ6\n\\(A = \\begin{pmatrix} -2&-3\\\\1&1 \\end{pmatrix}\\), what is the characteristic polynomial, and the solutions to the characteristic polynomial?\n\\(\\lambda^2 + \\lambda + 1 = 0\\)\nNo real solutions\nThis matrix has no real eigenvalues, so any eigenvalues are complex in nature. This is beyond the scope of this course, so we won’t delve too deeply on this.\n\n\n\nDiagonalisation is when we perform efficient matrix operations using eigenvectors as our basis.\nThere will be times when we want to apply the same matrix transformation multiple times. For instance, in a time series, once an event occurs.\nWe start at \\(v_0\\), then apply \\(T\\) to get to \\(v_1\\), then apply \\(T\\) again, to get to \\(v_2\\)\n{:width=“400”}\n\\(v_2\\) is equivalent to applying \\(T\\) to \\(v_1\\) or applying \\(T\\) to \\(v_0\\) twice\n\\[v_2 = Tv_1 = T(Tv_0) = T^2v_0\\]\nWe can generalise this to \\(n\\) times, \\(v_n = T^nv_0\\)\nMatrix multiplication can be comuputationally expensive, particularly as the dimensions increase.\nIf all the terms in a matrix are zero, except for those along the leadig diagonal, we refer to it as a diagonal matrix. When raising matrices to powers, i.e. as in the example above, applying the same transformation continuously, diagonal matrices make things a lot easier.\n\\(A = \\begin{bmatrix} 2&0\\\\0&2 \\end{bmatrix}\\)\n\\(A^3 = \\begin{bmatrix} 8&0\\\\0&8 \\end{bmatrix}\\)\n\\(T^n = \\begin{pmatrix} a^n&0&0\\\\0&b^n&0\\\\0&0&c^n \\end{pmatrix}\\)\nHowever, if \\(T\\) is not a diagonal matrix, we transform it to an eigenbasis, which will be diagonal.\nAs we saw in the section on changing basis, each column of our transform matrix simply represents the new location of the transformed unit vectors.\nTo build our eigen-basis conversion matrix (\\(C\\)), we plug in each of our eigenvectors as columns (\\(ev\\)).\n\\(C = \\begin{pmatrix} ev_1\\\\ev_2\\\\ev_3 \\end{pmatrix}\\)\nOur diagonal matrix \\(D\\), which is what we’ll scale up, contains the corresponding eigenvalues of the matrix \\(T\\)\n\\(T^n = \\begin{pmatrix} \\lambda_1&0&0\\\\0&\\lambda_2&0\\\\0&0&\\lambda_3 \\end{pmatrix}\\)\nWe convert our matrix \\(T\\) to our eigenbasis, applying our diagonalised matrix and then convert back again.\n\\(T = CDC^{-1}\\)\n\\(T^2 = CDC^{-1}CDC^{-1}\\)\nMultiplying a matrix by it’s inversion directly, as we do in the middle of \\(T^2\\) is the same as doing nothing at all, so we can remove this expression;\n\\(T^2 = CDDC^{-1} = CD^2C^{-1}\\)\nThis becomes generalisable;\n\\(T^n = CDDC^{-1} = CD^nC^{-1}\\)\nWe now have a method which lets us apply a transformation matrix as many times as we’d like without paying a large computational cost.\n{:width=“400”}\n\n\n\nUsing the transformation matrix \\(T = \\begin{pmatrix} 1&1\\\\0&2\\end{pmatrix}\\)\nAs the first column is just [1, 0], this means that our \\(\\hat{i}\\) vector will be unchanged. However, the second column tells us that \\(\\hat{j}\\), the second vector will be moving to the point [1, 2].\n\nThis particular transform could be decomposed into a vertical scaling by a factor of 2, and then a horizontal shear by a half step.\n\nThe diagonal vector of [1, 1] will become [2, 2] working below;\n# T rows * columns\n(1 * 1) + (1 * 1) = 2\n(0 * 1) + (2 * 1) = 2\nApplying T\nSay we apply \\(T\\) to [-1, 1], here we are applying \\(T\\) for the first time\n\\(\\begin{pmatrix} 1&1\\\\0&2\\end{pmatrix} \\begin{pmatrix} -1\\\\1\\end{pmatrix} = \\begin{pmatrix} -1+1\\\\0+2\\end{pmatrix} = \\begin{pmatrix} 0\\\\2\\end{pmatrix}\\)\nApply \\(T\\) again, here we are applying \\(T\\) for the second time\n\\(\\begin{pmatrix} 1&1\\\\0&2\\end{pmatrix} \\begin{pmatrix} 0\\\\2\\end{pmatrix} = \\begin{pmatrix} 0+2\\\\0+4\\end{pmatrix} = \\begin{pmatrix} 2\\\\4\\end{pmatrix}\\)\nHowever, we could have started with finding \\(T^2\\);\n\\(\\begin{pmatrix} 1&1\\\\0&2\\end{pmatrix} \\begin{pmatrix} 1&1\\\\0&2\\end{pmatrix} = \\begin{pmatrix} 1&3\\\\0&4\\end{pmatrix}\\)\nAnd applying that to our vector [-1, 1]\n\\(\\begin{pmatrix} 1&3\\\\0&4\\end{pmatrix} \\begin{pmatrix} -1\\\\1\\end{pmatrix} = \\begin{pmatrix} -1+3\\\\0+4\\end{pmatrix} = \\begin{pmatrix} 2\\\\4\\end{pmatrix}\\)\nWe can do the whole process using our eigenbasis approach;\nThe conversion matrix is made up of the eigen vectors;\neigenvectors and eigenvalues;\n\\(ev_1 = [1, 0], \\lambda = 1\\), x axis vector (\\(\\hat{i}\\)) does not alter\n\\(ev_2 = [1, 1], \\lambda = 2\\), diagonal vector scales by 2\n\\(C = \\begin{pmatrix} 1&1\\\\0&1\\end{pmatrix}\\)\nThe inverse of \\(C\\) can be calculated mentally, we shift to the left, rather than the right, though best to compute computationally;\n\\(C^{-1} = \\begin{pmatrix} 1&-1\\\\0&1\\end{pmatrix}\\)\n\\(T^2 = CD^2C^{-1}\\)\nremember \\(D\\) is the diagonal matrix of \\(T\\)\n\\[T^2 = \\begin{pmatrix} 1&1\\\\0&1\\end{pmatrix} \\begin{pmatrix} 1&0\\\\0&2\\end{pmatrix}^2 \\begin{pmatrix} 1&-1\\\\0&1\\end{pmatrix}\\]\nstart from the in, and work our way out\n\\[T^2 = \\begin{pmatrix} 1&1\\\\0&1\\end{pmatrix} \\begin{pmatrix} 1&-1\\\\0&4\\end{pmatrix}\\]\none more step\n\\[T^2 = \\begin{pmatrix} 1&3\\\\0&4\\end{pmatrix}\\]\nthen apply to our vector [-1, 1]\n\\[\\begin{pmatrix} 1&3\\\\0&4\\end{pmatrix} \\begin{pmatrix} -1\\\\1\\end{pmatrix} = \\begin{pmatrix} 2\\\\4\\end{pmatrix}\\]\nPractice\nQ1\nGive the matrix \\(T = \\begin{pmatrix} 6&-1\\\\2&3\\end{pmatrix}\\) and a change basis matrix of \\(C = \\begin{pmatrix} 1&1\\\\1&2\\end{pmatrix}\\)\nCalculate \\(D = C^{-1}TC\\)\n\\(D = \\begin{pmatrix} 1&1\\\\1&2\\end{pmatrix}\\)\n# start w T * C\na = (6 * 1) + (-1 * 1) = 5\nb = (6 * 1) + (-1 * 2) = 4\nc = (2 * 1) + (3 * 1) = 5\nd = (2 * 1) + (3 * 2) =  8\n\n[[5, 4], [5, 8]]\n\n# verify\nnp.array([[6, -1], [2, 3]]) @ np.array([[1, 1], [1, 2]])\n\\(C^{-1}\\) flip the off diagonal and make the leading diagonal negative; \\(\\begin{pmatrix} 2&-1\\\\-1&1\\end{pmatrix}\\)\n\nRecall that when a matrix is transformed into its diagonal form, the entries along the diagonal are the eigenvalues of the matrix - this can save lots of calculation!\n\nfrom numpy.linalg import inv\n\nC^-1 = inv([[1, 1], [1, 2]])\n\nc_inv @ np.array([[5, 4], [5, 8]])\n\narray([[5., 0.],\n       [0., 4.]])\nQ2\nGive the matrix \\(T = \\begin{pmatrix} 2&7\\\\0&-1\\end{pmatrix}\\) and a change basis matrix of \\(C = \\begin{pmatrix} 7&1\\\\-3&0\\end{pmatrix}\\)\nCalculate \\(D = C^{-1}TC\\)\n# start w T * C\na = (2 * 7) + (7 * -3) = -7\nb = (2 * 1) + (7 * 0) = 2\nc = (0 * 7) + (-1 * -3) = 3\nd = (0 * 1) + (-1 * 0) = 0\n\n[[-7, 2], [3, 0]]\n\n# verify\nnp.array([[2, 7], [0, -1]]) @ np.array([[7, 1], [-3, 0]])\n\\(C^{-1}\\) flip the off diagonal and make the leading diagnoal negative; \\(\\begin{pmatrix} 0&1\\\\-3&7\\end{pmatrix}\\) does not seem to work here\nfrom numpy.linalg import inv\n\nc_inv = inv(np.array([[7, 1], [-3, 0]])) = array([[0, -0.333], [1,  2.333]])\n\nc_inv @ np.array([[-7, 2], [3, 0]])\n\narray([[-1., 0.],\n       [4.4409, 2]]) # something wrong bottom left\n\n# should be\narray([[-1., 0.],\n       [0, 2]])    \nQ3\nGive the matrix \\(T = \\begin{pmatrix} 1&0\\\\2&-1\\end{pmatrix}\\) and a change basis matrix of \\(C = \\begin{pmatrix} 1&0\\\\1&1\\end{pmatrix}\\)\nCalculate \\(D = C^{-1}TC\\)\n# start w T * C\na = (1 * 1) + (0 * 1) = 1\nb = (1 * 0) + (0 * 1) = 0\nc = (2 * 1) + (-1 * 1) = 1\nd = (2 * 0) + (-1 * 1) = -1\n\n[[1, 0], [1, -1]]\n\n# verify\nnp.array([[1, 0], [2, -1]]) @ np.array([[1, 0], [1, 1]])\n\\(C^{-1}\\) flip the off diagonal and make the leading diagonal negative; \\(\\begin{pmatrix} 1&0\\\\-1&1\\end{pmatrix}\\)\nfrom numpy.linalg import inv\n\nc_inv = inv(np.array([[1, 0], [1, 1]]))\n\n# array([[1, 0], [-1, 1]])\n\nc_inv @ np.array([[1, 0], [1, -1]])\n\narray([[1, 0], [0, -1]])\nQ4\nGive the matrix \\(T = \\begin{pmatrix} a&0\\\\0&a\\end{pmatrix}\\) and a change basis matrix of \\(C = \\begin{pmatrix} 1&2\\\\0&1\\end{pmatrix}\\) and \\(C^{-1} = \\begin{pmatrix} 1&-2\\\\0&1\\end{pmatrix}\\)\nCalculate \\(D = C^{-1}TC\\)\n\\(T = \\begin{pmatrix} a&0\\\\0&a\\end{pmatrix}\\)\nTODO return and do properly with eigenvectors\nQ5\nGive the matrix \\(T = \\begin{pmatrix} 6&-1\\\\2&3\\end{pmatrix} = \\begin{pmatrix} 1&1\\\\1&2\\end{pmatrix} \\begin{pmatrix} 5&0\\\\0&4\\end{pmatrix} \\begin{pmatrix} 2&-1\\\\-1&1\\end{pmatrix}\\)\nCalculate the matrix \\(T^3\\)\nimport numpy as np\n\nnp.array([[6, -1], [2, 3]]) @ \n    np.array([[6, -1], [2, 3]]) @ \n    np.array([[6, -1], [2, 3]])\n\n[[186, -61], [122, 3]]\nTODO return and do properly with eigenvectors\nQ6\nGive the matrix \\(T = \\begin{pmatrix} 2&7\\\\0&-1\\end{pmatrix} = \\begin{pmatrix} 7&1\\\\-3&0\\end{pmatrix} \\begin{pmatrix} -1&0\\\\0&2\\end{pmatrix} \\begin{pmatrix} 0&-\\frac{1}{3}\\\\1&\\frac{7}{3}\\end{pmatrix}\\)\nCalculate the matrix \\(T^3\\)\nimport numpy as np\n\nnp.array([[2, 7], [0, -1]]) @ \n    np.array([[2, 7], [0, -1]]) @ \n    np.array([[2, 7], [0, -1]])\n\n[[8, 21], [0, -1]]\nTODO return and do properly with eigenvectors\nQ7\nGive the matrix \\(T = \\begin{pmatrix} 1&0\\\\2&-1\\end{pmatrix} = \\begin{pmatrix} 1&0\\\\1&1\\end{pmatrix} \\begin{pmatrix} 1&0\\\\0&-1\\end{pmatrix} \\begin{pmatrix} 1&0\\\\-1&1\\end{pmatrix}\\)\nCalculate the matrix \\(T^5\\)\nimport numpy as np\n\nnp.array([[1, 0], [2, -1]]) @ \n    np.array([[1, 0], [2, -1]]) @ \n    np.array([[1, 0], [2, -1]]) @ \n    np.array([[1, 0], [2, -1]]) @ \n    np.array([[1, 0], [2, -1]])\n\n[[1, 0], [2, -1]]\nTODO return and do properly with eigenvectors\n\n\n\n\nThe central assumption underpinning page rank is that the importance of a website is related to its links to and from other websites.\nWe’re trying to build an expression that tells us, based on this network structure, which of these webpages is most relevant to the person who made the search.\n{:width=“200”}\nBy mapping all the possible links, we can build a model to estimate the amount of time we would expect Procrastinating Pat to spend on each webpage.\nWe can describe the links present on page A as a vector, where each row is either a one or a zero based on whether there is a link to the corresponding page. And then normalise the vector by the total number of the links, such that they can be used to describe a probability for that page.\n\\(A = \\begin{bmatrix}0,&1,&1,&1\\end{bmatrix}\\) \\(A\\) has a link to each of \\(B\\), \\(C\\), and \\(D\\) but not itsself.\nWe’ll normalise by a third, as there are three links.\nDon’t forget, \\(L_A\\) is a column in our final \\(L\\) matrix\n\\(L_A = \\begin{bmatrix}0,&\\frac{1}{3},&\\frac{1}{3},&\\frac{1}{3}\\end{bmatrix}\\)\n\\(L_B = \\begin{bmatrix}\\frac{1}{2},&0,&0,&\\frac{1}{2}\\end{bmatrix}\\)\n\\(L_C = \\begin{bmatrix}0,&0,&0,&1\\end{bmatrix}\\)\n\\(L = \\begin{pmatrix}0&\\frac{1}{2}&0&0 \\\\ \\frac{1}{3}&0&0&\\frac{1}{2} \\\\ \\frac{1}{3}&0&0&\\frac{1}{2} \\\\ \\frac{1}{3}&\\frac{1}{2}&1&0\\end{pmatrix}\\)\nThe only way to get to \\(A\\) is by being at \\(B\\), which depends on being at \\(A\\) or \\(D\\). This problem is self-referential, as the ranks on all the pages depend on all the others.\n\nAlthough we built our matrix from columns of outward links, we can see that the rows describe inward links normalized with respect to their page of origin.\n\nThe vector \\(R\\) will store the rank of all web pages.\nTo calculate the rank of page A, you need to know three things about all other pages on the Internet.\n\nWhat’s your rank?\nDo you link to page A?\nHow many outoging links do you have in total?\n\n\\(r_A = \\sum^n_{j=1} L_{A, j} r{j}\\)\n\\(r_A\\) is the sum of all the locations * multiplied by their rank\nIn the expression above the \\(\\sum\\) means from where \\(J = 1\\) to \\(n\\), \\(n\\) is the number of web pages.\nAll positions in the link matrix relevent to webpage \\(A\\) at location \\(j\\), multiplied by the rank at location \\(j\\)\nThe rank of \\(A\\) is the sum of all the pages linked to it, weighted by their specific link probability (taken from \\(L\\))\nWe can re-write this as a matrix multiplication;\n\\(r = Lr\\)\n\nNow as we start off not knowing \\(r\\) we assume that all the ranks are equally and normalise them by the total number of webpages in our analysis, in this case is 4\n\n\\(r = \\begin{pmatrix}\\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\end{pmatrix}\\)\nEach time you multiply r by our matrix L, this gives us an updated value for r\n\\(r^{i+1} = Lr^i\\)\nWe solve the problem by iterating through the matrix \\(L\\) until \\(r\\) stops changing\n\\(r\\) ban be thought of as an eigenvector of \\(L\\) with an eigenvalue of 1\nWe can’t use the diagonal method, as it requires us knowing all the eigenvectors, which is what we’re trying to calculate.\n\nAlthough there are many approaches for efficiently calculating eigenvectors that have been developed over the years, repeatedly multiplying a randomly selected initial guest vector by a matrix, which is called the power method, is still very effective\nThe power method gives you one eigenvector, even though there will be n for an n webpage system, because of how we’ve structured our link matrix \\(L\\), it will always give you an eigenvector with a value of 1 -> this will be the largest eigenvalue\nIn an example with lots of web pages, most pages won’t link to one another, there will be lots of 0s in L, this is referred to as a sparse matrix, allowing us to perform very efficient multiplication.\n\nThe damping factor \\(d\\) adds an additional form to our iterative formula;\n\\(r^{i+1} = d(Lr^i) + \\frac{1-d}{n}\\)\n\\(d\\) is a value between 0 and 1\nYou can think of it as 1 minus the probability with which Procrastinating Pat suddenly, randomly types in a web address, rather than clicking on a link on his current page.\n\n\n\nThe PageRank algorithm is based on an ideal random web surfer who, when reaching a page, goes to the next page by clicking on a link. The surfer has equal probability of clicking any link on the page and, when reaching a page with no links, has equal probability of moving to any other page by typing in its URL. In addition, the surfer may occasionally choose to type in a random URL instead of following the links on a page. The PageRank is the ranked order of the pages from the most to the least probable page the surfer will be viewing.\nPageRank as a linear algebra problem Let’s imagine a micro-internet, with just 6 websites (Avocado, Bullseye, CatBabel, Dromeda, eTings, and FaceSpace). Each website links to some of the others, and this forms a network as shown,\n{:width=“400”}\nImagine we have 100 Procrastinating Pats on our micro-internet, each viewing a single website at a time. Each minute the Pats follow a link on their website to another site on the micro-internet. After a while, the websites that are most linked to will have more Pats visiting them, and in the long run, each minute for every Pat that leaves a website, another will enter keeping the total numbers of Pats on each website constant. The PageRank is simply the ranking of websites by how many Pats they have on them at the end of this process.\nWe represent the number of Pats on each website with the vector, \\[\\mathbf{r} = \\begin{bmatrix} r_A \\\\ r_B \\\\ r_C \\\\ r_D \\\\ r_E \\\\ r_F \\end{bmatrix}\\] And say that the number of Pats on each website in minute \\(i+1\\) is related to those at minute \\(i\\) by the matrix transformation\n\\[ \\mathbf{r}^{(i+1)} = L \\,\\mathbf{r}^{(i)}\\]\nwith the matrix \\(L\\) taking the form, \\[ L = \\begin{bmatrix}\nL_{A→A} & L_{B→A} & L_{C→A} & L_{D→A} & L_{E→A} & L_{F→A} \\\\\nL_{A→B} & L_{B→B} & L_{C→B} & L_{D→B} & L_{E→B} & L_{F→B} \\\\\nL_{A→C} & L_{B→C} & L_{C→C} & L_{D→C} & L_{E→C} & L_{F→C} \\\\\nL_{A→D} & L_{B→D} & L_{C→D} & L_{D→D} & L_{E→D} & L_{F→D} \\\\\nL_{A→E} & L_{B→E} & L_{C→E} & L_{D→E} & L_{E→E} & L_{F→E} \\\\\nL_{A→F} & L_{B→F} & L_{C→F} & L_{D→F} & L_{E→F} & L_{F→F} \\\\\n\\end{bmatrix}\n\\]\nThe columns represent the probability of leaving a website for any other website, and sum to one.\nThe rows determine how likely you are to enter a website from any other, though these need not add to one.\nThe long time behaviour of this system is when \\(\\mathbf{r}^{(i+1)} = \\mathbf{r}^{(i)}\\), so we’ll drop the superscripts here, and that allows us to write, \\[ L \\,\\mathbf{r} = \\mathbf{r}\\]\nThis is an eigenvalue equation for the matrix \\(L\\), with eigenvalue 1 (this is guaranteed by the probabalistic structure of the matrix \\(L\\)).\n%pylab notebook\nimport numpy as np\nimport numpy.linalg as la\nfrom readonly.PageRankFunctions import *\nnp.set_printoptions(suppress=True)\n\n# column 1 represents the possibilites of going from A to each other site, and so on\nL = np.array([[0,   1/2, 1/3, 0, 0,   0 ],\n              [1/3, 0,   0,   0, 1/2, 0 ],\n              [1/3, 1/2, 0,   1, 0,   1/2 ],\n              [1/3, 0,   1/3, 0, 1/2, 1/2 ],\n              [0,   0,   0,   0, 0,   0 ],\n              [0,   0,   1/3, 0, 0,   0 ]])\nWe could use a linear algebra library, as below. But this gets unmanagable for large systems.\neVals, eVecs = la.eig(L) # Gets the eigenvalues and vectors\norder = np.absolute(eVals).argsort()[::-1] # Orders them by their eigenvalues\neVals = eVals[order]\neVecs = eVecs[:,order]\n\nr = eVecs[:, 0] # Sets r to be the principal eigenvector\n100 * np.real(r / np.sum(r)) # Make this eigenvector sum to one, then multiply by 100 Procrastinating Pats\n\n# output\narray([16., 5.333, 40, 25.333, 0, 13.333])\nthe PageRank of this micro-internet is: CatBabel, Dromeda, Avocado, FaceSpace, Bullseye, eTings\nSince we only care about the principal eigenvector (the one with the largest eigenvalue, which will be 1 in this case), we can use the power iteration method which will scale better, and is faster for large systems.\nLet’s now try to get the same result using the Power-Iteration method.\nFirst let’s set up our initial vector, \\(\\mathbf{r}^{(0)}\\), so that we have our 100 Procrastinating Pats equally distributed on each of our 6 websites.\nr = 100 * np.ones(6) / 6 # Sets up this vector (6 entries of 1/6 × 100 each)\n\n# output\narray([16.66666667, \n    16.66666667, \n    16.66666667,  \n    16.66666667, \n    16.66666667, \n    16.66666667\n])\nUpdate the vector to the next minute, with the matrix L\nr = 100 * np.ones(6) / 6 # Sets up this vector (6 entries of 1/6 × 100 each)\nfor i in np.arange(100) : # Repeat 100 times\n    r = L @ r\n\n# output\narray([16., 5.333, 40, 25.333, 0, 13.333])\nEven better, we can keep running until we get to the required tolerance.\nr = 100 * np.ones(6) / 6 # Sets up this vector (6 entries of 1/6 × 100 each)\nlastR = r\nr = L @ r\ni = 0\nwhile la.norm(lastR - r) > 0.01 :\n    lastR = r\n    r = L @ r\n    i += 1\nprint(str(i) + \" iterations to convergence.\")\n\n# output\n# 18 iterations to convergence.\narray([16., 5.333, 40, 25.333, 0, 13.333])\nDamping Parameter\nSay a new website is added to the micro-internet: Geoff’s Website.\n{:width=“400”}\nL2 = np.array([[0,   1/2, 1/3, 0, 0,   0, 0 ],\n               [1/3, 0,   0,   0, 1/2, 0, 0 ],\n               [1/3, 1/2, 0,   1, 0,   1/3, 0 ],\n               [1/3, 0,   1/3, 0, 1/2, 1/3, 0 ],\n               [0,   0,   0,   0, 0,   0, 0 ],\n               [0,   0,   1/3, 0, 0,   0, 0 ],\n               [0,   0,   0,   0, 0,   1/3, 1 ]])\n\nr = 100 * np.ones(7) / 7 # Sets up this vector (6 entries of 1/6 × 100 each)\nlastR = r\nr = L2 @ r\ni = 0\nwhile la.norm(lastR - r) > 0.01 :\n    lastR = r\n    r = L2 @ r\n    i += 1\nprint(str(i) + \" iterations to convergence.\")\n\n# output\n# 131 iterations to convergence.\narray([0.03046998, 0.01064323, 0.07126612, 0.04423198, 1, 0.02489342, 99.81849527])\nThis behaviour can be understood, because once a Pat get’s to Geoff’s Website, they can’t leave, as all links head back to Geoff.\nTo combat this, we can add a small probability that the Procrastinating Pats don’t follow any link on a webpage, but instead visit a website on the micro-internet at random.\nWe’ll say the probability of them following a link is \\(d\\) and the probability of choosing a random website is therefore \\(1-d\\). We can use a new matrix to work out where the Pat’s visit each minute. \\[ M = d \\, L + \\frac{1-d}{n} \\, J \\] where \\(J\\) is an \\(n\\times n\\) matrix where every element is one.\nd = 0.5 # Feel free to play with this parameter after running the code once.\nM = d * L2 + (1-d)/7 * np.ones([7, 7]) # np.ones() is the J matrix, with ones for each entry.\n\nr = 100 * np.ones(7) / 7 # Sets up this vector (6 entries of 1/6 × 100 each)\nlastR = r\nr = M @ r\ni = 0\nwhile la.norm(lastR - r) > 0.01 :\n    lastR = r\n    r = M @ r\n    i += 1\nprint(str(i) + \" iterations to convergence.\")\n\n# output\n# 8 iterations to convergence.\narray([ 13.68217054, 11.20902965, 22.41964343,  16.7593433, 7.14285714, 10.87976354, 17.90719239])\n\n\nimport numpy as np\nimport numpy.linalg as la\nfrom readonly.PageRankFunctions import *\nnp.set_printoptions(suppress=True)\n\ndef pageRank(linkMatrix, d) :\n    n = linkMatrix.shape[0]\n    M = d * linkMatrix + (1-d)/n * np.ones([n, n])\n    r = 100 * np.ones(n) / n\n    lastR = r\n    r = M @ r\n    i = 0\n    while la.norm(lastR - r) > 0.01 :\n        lastR = r\n        r = M @ r\n        i += 1\n    return r\n\n\n\nNumpy can calculate eigenvectors and eigenvalues\nM = np.array([[ 4, -5,6],\n       [7, -8, 6],\n       [3/2,  -1/2, -2]])\nvals, vecs = np.linalg.eig(M)\nvecs\n\n# output\n[[ 3. -2.  1.]\n [ 3. -2. -1.]\n [ 1.  1. -2.]]\nQ1 Select all the eigenvectors of the matrix above;\n\\(\\begin{bmatrix} 1/2 \\\\ -1/2 \\\\ -1\\end{bmatrix}\\), corresponds to the third column in the output\n\\(\\begin{bmatrix}-3 \\\\ -3 \\\\ -1\\end{bmatrix}\\), corresponds to the first column in the output\n\\(\\begin{bmatrix}-2/\\sqrt{9} \\\\ -2/\\sqrt{9} \\\\ 1/\\sqrt{9}\\end{bmatrix}\\), corresponds to the second column in the output\nQ2 In PageRank, we care about the eigenvector of the link matrix, \\(L\\), that has eigenvalue 1, and that we can find this using power iteration method as this will be the largest eigenvalue.\nPageRank can sometimes get into trouble if closed-loop structures appear. A simplified example might look like this,\n{:width=“200”}\nTherefore the \\(L\\) matrix will look like this;\n\\(L=\\left[\\begin{array}{llll} 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{array}\\right]\\)\nWhat might be going wrong?\n\nBecause of the loop, _Procrastinating Pats that are browsing will go around in a cycle rather than settling on a webpage. The system will never converge using the power iteration method.\nOther eigenvalues are not small compated to 1, and so do not decay away with each power iteration. The other eigenvectors in fact have the same size as 1 (they are \\(-1, i, -i\\))\n\nWhat we can do to overcome this, is to add damping\n\\(L^{\\prime}=\\left[\\begin{array}{cccc} 0.1 & 0.1 & 0.1 & 0.7 \\\\ 0.7 & 0.1 & 0.1 & 0.1 \\\\ 0.1 & 0.7 & 0.1 & 0.1 \\\\ 0.1 & 0.1 & 0.7 & 0.1 \\end{array}\\right]\\)\nThere is now a probability to move to any website. Pats are no longer constrained by the loop.\nThe other eigen values get smaller\nAnother issue that arises, is when parts of the system are decoupled.\nFor example;\n{:width=“200”}\nWith a link matrix;\n\\(L=\\left[\\begin{array}{llll} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0 \\end{array}\\right]\\)\nThis form is known as block diagonal, as it can be split into square blocks along the main diagonal, i.e.\n\\(L= \\begin{bmatrix}A & 0 \\\\ 0 & B\\end{bmatrix}\\)\nWith \\(A = B = \\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix}\\) in this case.\nIn this system, there are two loops \\((A \\rightleftarrows B)\\) and \\((C \\rightleftarrows D)\\).\nAs the system is disconnected there will not be a unique PageRank. There are two eigenvalues of 1.\nThe eigensystem is degenerate. Any linear combination of eigenvectors with the same eigenvalue is also an eigenvector.\nThe power iteration algorithm could settle on multiple values, depending on its starting conditions.\nDamping in this setup will produce only one eigenvalue of 1 and PageRank will settle to its’ eigenvector.\n\nGood note on the differences between a vecotr, matrix and a tensor here"
  },
  {
    "objectID": "posts/2021-02-20-linear-algebra.html#refs",
    "href": "posts/2021-02-20-linear-algebra.html#refs",
    "title": "An introduction to Linear Algebra",
    "section": "Refs",
    "text": "Refs\nImperial’s “Mathematics for Machine Learning: Linear Algebra” on Coursera\nJoel Grus’ “Data Science from Scratch” Github\n3Blue1Brown’s essence of linear algebra Youtube\nStandford’s CS229 pdf"
  },
  {
    "objectID": "posts/2022-03-14-pre-calculus.html",
    "href": "posts/2022-03-14-pre-calculus.html",
    "title": "Pre-Calculus",
    "section": "",
    "text": "Master the fundamentals of exponential, logarithmic, hyperbolic, and parametric equations."
  },
  {
    "objectID": "posts/2022-03-14-pre-calculus.html#logarithms",
    "href": "posts/2022-03-14-pre-calculus.html#logarithms",
    "title": "Pre-Calculus",
    "section": "Logarithms",
    "text": "Logarithms\nThe logarithm is the inverse of the exponential.\n{:width=“300px”}\nYou can read \\(\\log_b(x)\\) as “\\(b\\) raised to what power equals \\(x\\)?”\n{% include info.html text=“The log base 10 of a number (rounded down) is 1 less than the number of digits of the number” %}\nFor example;\n\\(\\log_{10}(1000) = 3\\)\n\\(\\log_{10}(49323) \\approx 4.69 \\rightarrow\\left \\lfloor{\\log_{10}(49323)}\\right \\rfloor = 4\\)\n\\(\\log_{10}(333333) \\approx 5.52 \\rightarrow\\left \\lfloor{\\log_{10}(333333)}\\right \\rfloor = 5\\)\n{% include info.html text=“In general, every positive number with 1 followed by only 00s will have an integer answer when taking the logarithm. Adding a 0 will ‘step’ to the next integer.” %}\n\\(\\log_{10}100=2 \\;\\; \\log_{10}1000=3 \\;\\; \\log_{10}10000=4\\)\n\nWith a logarithmic scale, moving forward a step multiplies the distance by a set amount. So For base 10; the scale, started a 0, i.e. $10^0 = 1. Each step is akin to multiplying the distance by 10. Moving right a linear amount \\(x\\) causes us to multiply the distance by \\(10^x\\).\n\\(10^2 * 10^3 = 10 ^ {(log_{10}(10^2) + log_{10}(10^3))}\\)\n\\(\\log_{10}(ab) = log_{10}(a) + log_{10}(b)\\)"
  },
  {
    "objectID": "posts/2022-03-14-pre-calculus.html#exponential-equations",
    "href": "posts/2022-03-14-pre-calculus.html#exponential-equations",
    "title": "Pre-Calculus",
    "section": "Exponential equations",
    "text": "Exponential equations\n\nThe rate of growth is proportional to the current amount at any given time.\n\n\\[ y = a + b^x \\]\n\nIt is impossible to undo the effect of doubling \\(b\\) with a change in \\(a\\)\n\nAlgebraically, if we start off with \\(y = a \\times b^xy=a×b^x\\) then doubling \\(b\\) gives \\(y = a \\times (2b)^x = a \\times 2^x \\times b^x\\).\nSo each point is being multiplied by an additional \\(2^x\\) term.\nThis can’t be undone by changing \\(a\\) because the value of \\(2^x\\) changes as \\(x\\) changes while the value of \\(a\\) will remain constant.\nSo while it is possible to change \\(a\\) so that the effect of doubling \\(b\\) is undone for a single point it will not work for all other points."
  },
  {
    "objectID": "posts/2022-03-14-pre-calculus.html#changing-the-base",
    "href": "posts/2022-03-14-pre-calculus.html#changing-the-base",
    "title": "Pre-Calculus",
    "section": "Changing the base",
    "text": "Changing the base\n\\[ y = b^x \\]\nFor all bases where \\(b > 0\\), all exponential functions pass through the point [0, 1]\n{:width=“300px”} {:width=“300px”}\nAs \\(b\\) grows, the proportion of the graph for \\(x > 0\\) becomes steeper, leading to a right angle shape.\nThe graph can be reflected over the y axis when \\(b\\) is below 1. \\(y = 0.5^x\\) reflects \\(y = 2^x\\)\n{% include info.html text=“An exponential function can’t be defined with a negative base” %}\n{:width=“300px”}\nChanging \\(b\\) is the only thing that will change the shape of the curve. The others shift or scale the graph."
  },
  {
    "objectID": "posts/2022-03-14-pre-calculus.html#exponential-arithmetic",
    "href": "posts/2022-03-14-pre-calculus.html#exponential-arithmetic",
    "title": "Pre-Calculus",
    "section": "Exponential arithmetic",
    "text": "Exponential arithmetic\n\\[ x^2 x^3 x^4 = x^9 \\]\nThis is an example of the product rule.\n{% include info.html text=“these rules are true if \\(a\\) is positive, and \\(m\\) and \\(n\\) are real numbers.” %}\n\nIf we have an expontential equation, \\(y = ab^{x +c}\\), only changing \\(b\\) will change the shape of the curve. Changing \\(x\\) or \\(c\\) changes where the curve falls/steepens.\nAn implication of the product rule for exponential expressions;\n\nmuliplication or division in one part of the expression can have the same effect as addition or subtraction in another part of the expression\ncompressing or expanding the graph vertically (multiplying or dividing the entire expression) has the same effect as shifting the graph horizontally (adding or subtracting from the exponent).\n\nIncreasing \\(c\\) stretches the graph vertically, increasing \\(a\\), by a larger factor, will have the same effect.\n\\[ y = 9 \\times 3^x \\]\nCan be rewritten using the product rule\n\\[ y = 3^2 \\times 3^2 = 3^{x +2} \\]\nExponential functions can be rewritten as logarithmic functions\n\\[ b^a = c \\equiv log_b(c) = a \\]"
  },
  {
    "objectID": "posts/2022-03-14-pre-calculus.html#graphing-logarithms",
    "href": "posts/2022-03-14-pre-calculus.html#graphing-logarithms",
    "title": "Pre-Calculus",
    "section": "Graphing Logarithms",
    "text": "Graphing Logarithms\nAll basic logarithmic function have the same general shape.\nAsking the value of \\(log_{10}(x)\\) is equivalent to asking “what value exponeont does 10 need to be raised to in order to get \\(x\\)?\nThink of the inverse function, \\(log_{10}(x) \\equiv y = 10^x\\)\n\nThe domain of \\(log_{10}(x)\\) will be equal to the range of \\(y = 10^x\\)\n\nDomain being the complete set of possible values of x.\nAll basic loagrithmic functions of the form \\(f(x) = log_b(x)\\) have an “anchor point” - a point that they all pass through, regardless of their base.\nRegardless of the base, \\(b\\), we always have \\(log_b(1) = 0\\) because \\(b^0 = 1\\) for any \\(b\\). That means all logarithmic functions will pass through the point where x = 1 and y = 0, or (1,0)\n{% include info.html text=“If \\(x\\) is less than 1, \\(x\\) we need to be raised to a negative exponent.\nFor example, with base 10, \\(10^{-3} = \\frac{1}{10^3}\\)\nAs \\(x\\) gets larger, the rate at which \\(\\log_{10}(x)\\) increases will become slower and slower” %}\n\nThe graph of \\(y = \\log_{10}(x)\\) is a reflection of the graph \\(y = 1-^x\\) over the line \\(y=x\\). This is a result of the inverse relationship between the two functions.\n\n\nIt is not possible to define \\(\\log_1(x)\\) because \\(y = 1^x\\) has the same output for any value of \\(x\\), \\(1^{60}\\), is the same as \\(1^{900}\\)."
  },
  {
    "objectID": "posts/2022-03-14-pre-calculus.html#understanding-logarithmic-arithmetic",
    "href": "posts/2022-03-14-pre-calculus.html#understanding-logarithmic-arithmetic",
    "title": "Pre-Calculus",
    "section": "Understanding logarithmic arithmetic",
    "text": "Understanding logarithmic arithmetic\n\\[ 10^5 - 10^2 \\approxeq 10^5\\]\n\\[ 10^5 - 10^2 = 99900 = 10^{\\log_{10}(99900)} \\approxeq 10^{4.999}  \\]\n\\[ \\log_{10}10^Q = Q \\]\nSay you have a logarthimic scale.\nAdding logarithms is equivalent to adding linear distances on the chart, and the values inside the logarithms (the distances in space) will get multiplied.\nIf we a distance of $(s) $ and then move an additional ${10}(t) $ we have moved ${10}st $\n\\[ \\log_{10}s + log_{10}t = \\log_{10}st \\]\nThis is how much we have moved on the chart, not through space, so you’re getting the point on the axis.\n\\[ \\log_{10}10^2  + \\log_{10}10^2 + \\log_{10}10^2 = \\log_{10}((10^2)(10^2)(10^2)) = \\log_{10}(10^6) \\]\nIf we move a distance of $_{10}(m) $ and repeat is \\(p\\) times, so \\(p \\times \\log_{10}(m)\\) we get \\(log_{10}(m^p)\\)\nIn the example, \\(p = 3\\), so we have \\((10^2)^3 = (10^6)\\), not \\((10^2 \\times 3)\\) which is \\((m \\times p)\\)\nTake the inverse\n\\[ \\log_{10}n - \\log_{10}d = \\log_{10}\\frac{n}{d}\\]\n{:width=“300”}\n\\[ \\log_b(s) + \\log_b(t) = \\log_b(st) \\] \\[ \\log_b(n) - \\log_b(d) = \\log_b(\\frac{n}{d}) \\] \\[ (p)\\log_b(m) = \\log_b(m^p) \\]\n\\[ \\log _{3}(x)+\\log _{3}(y)-\\log _{3}(z) = \\log _{3}\\left(\\frac{x y}{z}\\right)\\]\nSay we have;\n\\[y = \\log{b}(ax) + c\\]\n{:width=“300”}\n\nPractice\n\nExpress the following in a single log\n\n\\[ \\log _{2}(x)+\\log _{2}(x)+\\log _{2}(x)+\\log _{2}(x) \\]\n\\[\\log _{2}(x \\cdot x \\cdot x \\cdot x)=\\log _{2}\\left(x^{4}\\right) \\]\n\nSolve for \\(x\\) : \\[ \\log _{8}(x)-\\log _{8}(4)=\\log _{8}(36)-\\log _{8}(x) \\]\n\n\\[\n\\begin{aligned}\n\\log _{8}(x)-\\log _{8}(4) &=\\log _{8}(36)-\\log _{8}(x) \\\\\n\\log _{8}\\left(\\frac{x}{4}\\right) &=\\log _{8}\\left(\\frac{36}{x}\\right) \\\\\n\\left(\\frac{x}{4}\\right) &=\\left(\\frac{36}{x}\\right) \\\\\nx^{2} &=144 \\\\\nx &=12\n\\end{aligned}\n\\]\n\nNote\n\n$$_{b}(m) = y b^y = m $£\nWe also now that\n\\[\\log _{z}\\left(b^{y}\\right)=\\log _{z}(m) \\]\n\nIf you now isolate \\(y\\), what does it equal?\n\nApplying the rule \\((p) \\log _{b}(m)=\\log _{b}\\left(m^{p}\\right)\\) \\[\n\\begin{aligned}\n\\log _{z}\\left(b^{y}\\right) &=\\log _{z}(m) \\\\\n(y) \\log _{z}(b) &=\\log _{z}(m) \\\\\ny &=\\frac{\\log _{z}(m)}{\\log _{z}(b)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2022-03-14-pre-calculus.html#change-of-base",
    "href": "posts/2022-03-14-pre-calculus.html#change-of-base",
    "title": "Pre-Calculus",
    "section": "Change of base",
    "text": "Change of base\n\nChange of base is a very useful procedure to manipulate log functions into something more useful.\n\n\\[\n\\log _{a}(b)=\\frac{\\log _{c}(b)}{\\log _{c}(a)}\n\\]\n\\(\\text { If } \\log _{9}(243)=2.5, \\text { what is the value of } \\log _{3}(243) ?\\)\n\\[\n\\log _{9}(243)=\\frac{\\log _{3}(243)}{\\log _{3}(9)} = 2.5 \\\\\n\\frac{\\log _{3}(243)}{2} = 2.5 \\\\\n\\log _{3}(243)= 5 \\\\\n\\]\nSimplify;\n\\[ \\frac{\\log _{2}(a)}{\\log _{4}(a)} \\]\nInstincitvely you know to are going to have to raise 4 by a power \\(x\\) fewer times to reach \\(a\\) than 2. That means \\(\\log _{2}(a) > \\log _{4}(a)\\)\nWe can apply the change of base formula to both the numerator and denominator. The new base we choose doesn’t matter, so we will just use 10: \\[\n\\log _{2}(a)=\\frac{\\log _{10}(a)}{\\log _{10}(2)}, \\quad \\log _{4}(a)=\\frac{\\log _{10}(a)}{\\log _{10}(4)}\n\\] Substituting back into our starting expression, we now have \\[\n\\frac{\\frac{\\log _{10}(a)}{\\log _{10}(2)}}{\\frac{\\log _{10}(a)}{\\log _{10}(4)}}=\\frac{\\log _{10}(4)}{\\log _{10}(2)}\n\\] Now \\(4=2^{2}\\), so \\(\\log _{10}(4)=\\log _{10}\\left(2^{2}\\right)=2 \\log _{10}(2)\\), so we can rewrite the expression above as \\[\n\\frac{2 \\log _{10}(2)}{\\log _{10}(2)}=2\n\\]"
  },
  {
    "objectID": "posts/2022-03-14-pre-calculus.html#logarithmic-equations",
    "href": "posts/2022-03-14-pre-calculus.html#logarithmic-equations",
    "title": "Pre-Calculus",
    "section": "Logarithmic equations",
    "text": "Logarithmic equations\n{% include info.html text=“When solving equations with logarithms, a general strategy is to rewrite the equation in exponential form” %}\n\\[\\log _{a}(x)=y \\text { is equivalent to } a^{y}=x \\]\nSolve for \\(x\\):\n\n\\[\\log _{5}(x+1)=2 \\]\n\n\\[ 5^2 = x+ 1 = 25\\]\n\n\\[\\log _{10}(3x+1)=2 \\]\n\n\\[ 10^2 = 3x+ 1 = 100\\]\n\n\\[\\log _{4}(x) +log_{4}(x+6)=2 \\]\n\n\\[ \\log_{4}(x^2+6x)=2 \\]\n\\[ 4^2=x^2 + 6x\\]\n\\[ x^2 + 6x - 16 = 0\\]\nUsing this along, we could consider \\(x\\) to equal 2 or -8\n\\[ (x+8)(x-2)=0\\]\nHowever \\(\\log _{4}(x) \\text{ can't be defined for } x=-8\\)\n\n\n\n\\[\n\\left(\\log _{4}(x)\\right)^{2}+\\log _{4}\\left(x^{3}\\right)-4=0\n\\]\nRetwrite \\(log _{4}(x^{3})\\) as \\(3\\log_{4}(x)\\)\nSubstitute \\(\\log_{4}(x)\\) for \\(y\\)\n\\[ y^2 + 3y - 4 = 0 \\]\n\\[ (y + 4 )(y - 1) = 0\\]\n\\[ log_{4}(x) = -4 \\text{ or } 1 \\]\n\\[x=4^{-4}=\\frac{1}{256} \\quad \\text { or } \\quad x=4^{1}=4\\]\n\n\n\n\\[\\log _{2}(x)+\\log _{4}(9)=\\log _{2}(12)\\]\nChange base \\(\\log _{4}(9) = \\frac{log_2(9)} {log_2(4)}\\)\n\\[ log_2(x) + 1.5849625007211563 = 3.5849625007211565 \\]\n\\[ log_2(x) = 2 \\]\nNote, could have rewritten the rebasing as:\n\\[\n\\frac{log_2(9)} {log_2(4)} = \\frac{\\log _{2}(9)}{2}=\\left(\\frac{1}{2}\\right)\\left(\\log _{2}(9)\\right)=\\log _{2}\\left(9^{\\frac{1}{2}}\\right)=\\log _{2}(3)\n\\]"
  },
  {
    "objectID": "posts/2020-10-16-structure-and-interpretation-of-computer-programs.html",
    "href": "posts/2020-10-16-structure-and-interpretation-of-computer-programs.html",
    "title": "Introducing Structure and Interpretation of Computer Programs",
    "section": "",
    "text": "As I delved down the rabbit role of writing code, particularly looking to delve more into functional programming, a number of folks I respected and asked for advice mentioned the book Structure and Interpretation of Computer Programs. I was also recommended The Little Schemer as I was writing some recursive programs. What follows are quotes, musings and examples from those books and various other sources.\n\n\n\nIn order to be creative one must first gain control of the medium. One can not even begin to think about organizing a great photograph without having the skills to make it happen. In engineering, as in other creative arts, we must learn to do analysis to support our efforts in synthesis. One cannot build a beautiful and functional bridge without a knowledge of steel and dirt and considerable mathematical technique for using this knowledge to compute the properties of structures. Similarly, one cannot build a beautiful computer system without a deep understanding of how to “previsualise” the process generated by the procedures one writes. – Daniel P. Friedman\n\n\nEvery computer program is a mode, hatched in the mind, of a real or mental process. These processes, arising from human experience and though, are huge in number, intricate in detail, and at any time only partially understood. Thus even though our programs are carefully hand-crafted discrete collections of symbols, mosaics of interlocking functions, they continually evolve: we change them as our perception of the model deepens, enlarges, generalises until the model ultimately attains a metastable (stable state of a dynamical system) place within still another model with which we struggle.\nThe source of exhilaration associated with computer programming is the continual unfolding within the mind and on the computer of mechanisms expressed as programs and the explosion of perception they generate. If art interprets our dreams, the computer executes them in the guise of programs!\nEach breakthrough in hardware technology leads to more massive programming enterprises, new organisational principles, and an enrichment of abstract models. Every reader should aks themselves periodically “Toward what end, toward what end?” - but do not ask it too often lest you pass up the fun pf programming for the constipation of bitter-sweet philosophy.\n\nWe control complexity by building abstractions that hide details when appropriate.\nWe control complexity by establishing conventional interfaces that enable us to construct systems by combining standard, well-understood pieces in a “mix and match” way.\nWe control complexity by establishing new languages for describing a design, each of which emphasizes particular aspects of the design and de-emphasises others.\n\nUnderlying our approach to this subject is our conviction that “computer science” is not a science and that its significance has little to do with computers. The computer revolution is a revolution in the way we think and in the way we express what we think. The essence of this change is the emergence of what might best be called procedural epistemology the study of the structure of knowledge from an imperative point of view (describe the steps to take, like a recipe), as opposed to the more declarative (describe the desired state) point of view taken by classical mathematical subjects. Mathematics provides a framework for dealing precisely with notions of “what is.” Computation provides a framework for dealing precisely with notions of “how to.” – Alan J. Perlis\n\n\nA programmer should acquire good algorithms and idioms."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "on-data",
    "section": "",
    "text": "Canon\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nThese are some articles that have had a significant impact on my thinking, there are many more, but these are some.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBuilding an intuition for Calculus\n\n\n\n\n\n\n\nmaths for machine learning\n\n\n\n\nNotes from Imperial’s “Mathematics for Machine Learning course”\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAn introduction to Linear Algebra\n\n\n\n\n\n\n\nmaths for machine learning\n\n\n\n\nA gentle introduction to Linear Algebra\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPre-Calculus\n\n\n\n\n\n\n\nmaths for machine learning\n\n\n\n\nNotes from brilliant.org’s pre-calculus course\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Structure and Interpretation of Computer Programs\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nIntroductory notes & quotes from the foreward of SICP\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "On data",
    "section": "",
    "text": "Thinking out loud."
  }
]