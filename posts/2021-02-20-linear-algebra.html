<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="A gentle introduction to Linear Algebra">

<title>on-data - An introduction to Linear Algebra</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">on-data</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">On data</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">An introduction to Linear Algebra</h1>
                  <div>
        <div class="description">
          A gentle introduction to Linear Algebra
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">maths for machine learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#an-introduction-to-linear-algebra" id="toc-an-introduction-to-linear-algebra" class="nav-link active" data-scroll-target="#an-introduction-to-linear-algebra">An introduction to Linear Algebra</a>
  <ul class="collapse">
  <li><a href="#vectors" id="toc-vectors" class="nav-link" data-scroll-target="#vectors">Vectors</a>
  <ul class="collapse">
  <li><a href="#vectors-obey-two-rules" id="toc-vectors-obey-two-rules" class="nav-link" data-scroll-target="#vectors-obey-two-rules"><strong>Vectors obey two rules</strong></a></li>
  </ul></li>
  <li><a href="#simultaneous-equations" id="toc-simultaneous-equations" class="nav-link" data-scroll-target="#simultaneous-equations">Simultaneous Equations</a></li>
  <li><a href="#operations-with-vectors" id="toc-operations-with-vectors" class="nav-link" data-scroll-target="#operations-with-vectors">Operations with vectors</a></li>
  <li><a href="#size-of-a-vector" id="toc-size-of-a-vector" class="nav-link" data-scroll-target="#size-of-a-vector">Size of a vector</a></li>
  <li><a href="#dot-product-multiplying-vectors" id="toc-dot-product-multiplying-vectors" class="nav-link" data-scroll-target="#dot-product-multiplying-vectors">Dot product, multiplying vectors</a>
  <ul class="collapse">
  <li><a href="#cosine-dot-product" id="toc-cosine-dot-product" class="nav-link" data-scroll-target="#cosine-dot-product">Cosine &amp; dot product</a></li>
  <li><a href="#projection" id="toc-projection" class="nav-link" data-scroll-target="#projection">Projection</a></li>
  </ul></li>
  <li><a href="#basis-the-coordinate-system-in-which-our-vectors-exists" id="toc-basis-the-coordinate-system-in-which-our-vectors-exists" class="nav-link" data-scroll-target="#basis-the-coordinate-system-in-which-our-vectors-exists">Basis, the coordinate system in which our vectors exists</a>
  <ul class="collapse">
  <li><a href="#defining-basis-vector-space-and-linear-independence" id="toc-defining-basis-vector-space-and-linear-independence" class="nav-link" data-scroll-target="#defining-basis-vector-space-and-linear-independence">Defining basis, vector space and linear independence</a></li>
  <li><a href="#applications-of-changing-basis" id="toc-applications-of-changing-basis" class="nav-link" data-scroll-target="#applications-of-changing-basis">Applications of changing basis</a></li>
  </ul></li>
  <li><a href="#introduction-to-matrices" id="toc-introduction-to-matrices" class="nav-link" data-scroll-target="#introduction-to-matrices">Introduction to matrices</a>
  <ul class="collapse">
  <li><a href="#linear-transformations" id="toc-linear-transformations" class="nav-link" data-scroll-target="#linear-transformations">Linear transformations</a></li>
  <li><a href="#how-matrices-transform-space" id="toc-how-matrices-transform-space" class="nav-link" data-scroll-target="#how-matrices-transform-space">How matrices transform space</a></li>
  <li><a href="#combining-matrix-transformations" id="toc-combining-matrix-transformations" class="nav-link" data-scroll-target="#combining-matrix-transformations">Combining matrix transformations</a></li>
  <li><a href="#practice-matrix-transformations" id="toc-practice-matrix-transformations" class="nav-link" data-scroll-target="#practice-matrix-transformations">Practice matrix transformations</a></li>
  <li><a href="#matrix-inverses" id="toc-matrix-inverses" class="nav-link" data-scroll-target="#matrix-inverses">Matrix Inverses</a></li>
  <li><a href="#linear-algebra-to-solve-systems-of-equations" id="toc-linear-algebra-to-solve-systems-of-equations" class="nav-link" data-scroll-target="#linear-algebra-to-solve-systems-of-equations">Linear algebra to solve systems of equations</a></li>
  <li><a href="#solving-systems-of-equations" id="toc-solving-systems-of-equations" class="nav-link" data-scroll-target="#solving-systems-of-equations">Solving systems of equations</a></li>
  <li><a href="#practice-solving-linear-equations-using-the-inverse-matrix" id="toc-practice-solving-linear-equations-using-the-inverse-matrix" class="nav-link" data-scroll-target="#practice-solving-linear-equations-using-the-inverse-matrix">Practice solving linear equations using the inverse matrix</a></li>
  <li><a href="#determinants" id="toc-determinants" class="nav-link" data-scroll-target="#determinants">Determinants</a></li>
  <li><a href="#the-determinant-and-inverse-matrices" id="toc-the-determinant-and-inverse-matrices" class="nav-link" data-scroll-target="#the-determinant-and-inverse-matrices">The determinant and inverse matrices</a></li>
  <li><a href="#python-and-matrices" id="toc-python-and-matrices" class="nav-link" data-scroll-target="#python-and-matrices">Python and Matrices</a></li>
  </ul></li>
  <li><a href="#matrices-make-linear-mappings" id="toc-matrices-make-linear-mappings" class="nav-link" data-scroll-target="#matrices-make-linear-mappings">Matrices make linear mappings</a>
  <ul class="collapse">
  <li><a href="#einstein-summation-convention-and-the-symmetry-of-the-dot-product" id="toc-einstein-summation-convention-and-the-symmetry-of-the-dot-product" class="nav-link" data-scroll-target="#einstein-summation-convention-and-the-symmetry-of-the-dot-product">Einstein summation convention and the symmetry of the dot product</a></li>
  <li><a href="#non-square-matrix-multiplication" id="toc-non-square-matrix-multiplication" class="nav-link" data-scroll-target="#non-square-matrix-multiplication">Non-square matrix multiplication</a></li>
  <li><a href="#using-non-square-matrices-to-do-a-projection" id="toc-using-non-square-matrices-to-do-a-projection" class="nav-link" data-scroll-target="#using-non-square-matrices-to-do-a-projection">Using non-square matrices to do a projection</a></li>
  </ul></li>
  <li><a href="#matrices-transform-into-the-new-basis-vector-set" id="toc-matrices-transform-into-the-new-basis-vector-set" class="nav-link" data-scroll-target="#matrices-transform-into-the-new-basis-vector-set">Matrices transform into the new basis vector set</a>
  <ul class="collapse">
  <li><a href="#doing-a-transformation-in-a-changed-basis" id="toc-doing-a-transformation-in-a-changed-basis" class="nav-link" data-scroll-target="#doing-a-transformation-in-a-changed-basis">Doing a transformation in a changed basis</a></li>
  <li><a href="#orthogonal-matrices" id="toc-orthogonal-matrices" class="nav-link" data-scroll-target="#orthogonal-matrices">Orthogonal matrices</a></li>
  <li><a href="#the-gram-schmidt-process" id="toc-the-gram-schmidt-process" class="nav-link" data-scroll-target="#the-gram-schmidt-process">The Gram-Schmidt process</a></li>
  <li><a href="#reflecting-in-a-plane" id="toc-reflecting-in-a-plane" class="nav-link" data-scroll-target="#reflecting-in-a-plane">Reflecting in a plane</a></li>
  </ul></li>
  <li><a href="#eigenvalues-and-eigenvectors" id="toc-eigenvalues-and-eigenvectors" class="nav-link" data-scroll-target="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a>
  <ul class="collapse">
  <li><a href="#eigenvectors-a-formal-definition" id="toc-eigenvectors-a-formal-definition" class="nav-link" data-scroll-target="#eigenvectors-a-formal-definition">Eigenvectors, a formal definition</a></li>
  <li><a href="#using-eigenvectors-as-your-basis" id="toc-using-eigenvectors-as-your-basis" class="nav-link" data-scroll-target="#using-eigenvectors-as-your-basis">Using eigenvectors as your basis</a></li>
  <li><a href="#eigenbasis-example" id="toc-eigenbasis-example" class="nav-link" data-scroll-target="#eigenbasis-example">Eigenbasis example</a></li>
  </ul></li>
  <li><a href="#making-the-pagerank-algorithm" id="toc-making-the-pagerank-algorithm" class="nav-link" data-scroll-target="#making-the-pagerank-algorithm">Making the PageRank algorithm</a></li>
  <li><a href="#diving-into-the-pagerank-algorithm" id="toc-diving-into-the-pagerank-algorithm" class="nav-link" data-scroll-target="#diving-into-the-pagerank-algorithm">Diving into the PageRank algorithm</a>
  <ul class="collapse">
  <li><a href="#produce-a-function-that-can-calculate-the-pagerank-for-an-arbitrarily-large-probability-matrix" id="toc-produce-a-function-that-can-calculate-the-pagerank-for-an-arbitrarily-large-probability-matrix" class="nav-link" data-scroll-target="#produce-a-function-that-can-calculate-the-pagerank-for-an-arbitrarily-large-probability-matrix">Produce a function that can calculate the PageRank for an arbitrarily large probability matrix</a></li>
  <li><a href="#eigenvalues-and-eigenvectors-practice" id="toc-eigenvalues-and-eigenvectors-practice" class="nav-link" data-scroll-target="#eigenvalues-and-eigenvectors-practice">Eigenvalues and eigenvectors practice</a></li>
  </ul></li>
  <li><a href="#refs" id="toc-refs" class="nav-link" data-scroll-target="#refs">Refs</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="an-introduction-to-linear-algebra" class="level1">
<h1>An introduction to Linear Algebra</h1>
<p>Algebra is the generic term for the maths of equations in which numbers and operations are written as symbols.</p>
<blockquote class="blockquote">
<p>René Descarte’s <em>La Geometrie</em> first introduced standard algebraic notation. When the book was being printed, the printer started to run out of letters. When asked if it mattered if <em>x</em>, <em>y</em>, or <em>z</em> was used to represent the unknowns, Descarte said it did not. As a result, <em>x</em> became fixed in maths - and the wider culture - as the symbol for the unknown quantity. - <em>Alex Bellos, Numberland</em></p>
</blockquote>
<p>The beauty of applied mathematics is that is gives us a new layer of abstraction. In the case of linear algebra that is a language to represent data and manipulate space. In many of the examples, we’ll use a geometric lens, i.e.&nbsp;using data to describe a 2D space (an arrow). But mathematics allows us to take these learnings and generalise to <span class="math inline">\(n\)</span> number of dimensions and our data can describe not just space but objects.</p>
<p>We’ll look at building an intuition around the concepts and doing some basic work in linear algebra, before passing off the difficult stuff to computers. Though, we’ll be intentional about what we’re asking the computer to do.</p>
<p>Linear algebra has evolved to a more general and abstract form, representing <em>vector spaces</em>. Vectors are used to described points in some finite-dimensional space. An <em>n</em> dimensional space will be described by a vector with <em>n</em> coordinates. In a 2D world this will be an arrow, with <em>x</em>, and <em>y</em> coordinates, the vector describes will be an arrow pointing to B.</p>
<p>Linearity is the property of a mathematical relationship (function) that can be graphically represented as a straight line <em>- Wikipedia</em>. This statement will make sense when we start to do operations with vectors. But we’ll see how adding and <sub>scaling</sub> manipulating the size of vectors is done by moving in a straight line.</p>
<p>It’s important to note that linear algebra represents much of the trunk of knowledge required for modern data analysis and many machine learning techniques. Often the use of linear algebra will be implicit, you’ll forget you’re even using it. Think of it as a prerequisite, a string to your bow, which doesn’t get the same attention as it’s sexy cousins statistics and probability, but is just as essential.</p>
<p>Take the time to refresh your memory or grasp the concepts here.</p>
<section id="vectors" class="level2">
<h2 class="anchored" data-anchor-id="vectors">Vectors</h2>
<p>Vectors are the building blocks of linear algebra.</p>
<p>Think of a vector as an object that moves us about space (physical or data), it could just be a list of attributes of an object.</p>
<p>Depending on your world view, what exactly you deem to be a vector will vary. Vectors can be considered;</p>
<ul>
<li><p>Vectors are arrows pointing in space</p>
<p>What defines a vector is its length and the direction it’s pointing. You can move it anywhere in space, and it’s the same vector.</p>
<p>A vector on the flat plane are two-dimensional, those in broader space at three-dimensional.</p></li>
<li><p>Vectors are a list of numbers</p>
<p>Vectors are usually viewed by computers as an ordered list of numbers which they can perform “operations” on - such as multiplying by <em>scalars</em> (numbers) to form new vectors.</p>
<p>The numbers in vectors represent data about an object. The number of dimensions is determined by the length of the vector.</p></li>
</ul>
<p>At the start of this learning material, we’ll focus on applying a geometric lens to vectors. That is, think of them as an arrow describing dimensions of space, according to some co-ordinate system.</p>
<blockquote class="blockquote">
<p>Vectors can be thought of in a variety of different ways - some geometrically, some algebraically, some numerically. In this way, there are a lot of techniques one can use to deal with vectors.</p>
</blockquote>
<p>Concretely, vectors are often a useful way to represent data.</p>
<section id="vectors-obey-two-rules" class="level3">
<h3 class="anchored" data-anchor-id="vectors-obey-two-rules"><strong>Vectors obey two rules</strong></h3>
<ol type="1">
<li>Addition <span class="math inline">\((r + s == s + r)\)</span> -&gt; <em>vector addition is associative</em></li>
</ol>
<p>Conceptually you add vectors by moving the tail origin of one vector to the tip of the other. Then drawing a new vector from the tail of the first to the tip of where the second now sits.</p>
<p>You’re adding the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> components together.</p>
<p><span class="math inline">\(r + s = [r_x + s_x, r_y + s_y]\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/addingvectors.jpg" class="img-fluid">{:width=“300px”} <br></p>
<p>In the example above <span class="math inline">\(r = \begin{bmatrix} 1 \\ 2\end{bmatrix}\)</span>, <span class="math inline">\(s = \begin{bmatrix} 3 \\ -1\end{bmatrix}\)</span> we can arrive at <span class="math inline">\(t\)</span> geometrically, taking 1 step the right, 2 up, then 3 to the right, and one down.</p>
<p>We add the component parts (component wise), as add the <span class="math inline">\(x\)</span> coordinates and the <span class="math inline">\(y\)</span> coordinates to get our new vector.</p>
<ol type="1">
<li>Multiplication by a scalar (i.e.&nbsp;number, because numbers scale vectors, so we use the terms interchangeably) <br><em>negative numbers means reverse</em></li>
</ol>
<p><img src="{{ site.baseurl }}/images/linear_algebra/vector_multiplication.jpg" class="img-fluid">{:width=“250px”} <br></p>
<p>We multiple each component in the vector by that scalar.</p>
<p><span class="math inline">\(r = \begin{bmatrix} i \\ j\end{bmatrix}\)</span> <span class="math inline">\(3r = \begin{bmatrix} 3i \\ 3j\end{bmatrix}\)</span></p>
<p>Vectors give us a language of space and the ability to manipulate space. They allow us to represent lots of lists of data together.</p>
<p><strong>Ties to Machine Learning</strong></p>
<p>One of the tasks of machine learning is to fit a model to data in order to represent the underlying distribution.</p>
<p>A model allows us to predict the data in a distribution.</p>
<p>We can start with a parameter vector <span class="math inline">\(\mathbf{p}\)</span> and convert it to a vector of expected frequencies <span class="math inline">\(\mathbf{g}_\mathbf{p}\)</span></p>
<p>We need a way fit a model’s parameters to data and quantify how good that fit is.</p>
<p>One way of doing so is to calculate the “residuals”, which is the difference between the measured data and the modelled prediction for each histogram bin.</p>
<p>A better fit would have as much overlap as it can, reducing the residuals as much as possible.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/residuals.png" class="img-fluid">{:width=“400px”}</p>
<p>In the example above, we’d improve the model in orange by decreasing or increasing <span class="math inline">\(\mu\)</span> (the height) and keeping <span class="math inline">\(\sigma\)</span> (the width) roughly the same.</p>
<p>The performance of a model can be quantified in a single number. One measure we can use is the Sum of Squared Residuals, <span class="math inline">\(\mathrm{SSR}\)</span>. i.e.&nbsp;we square the difference for each value in actual and predicted, and add all those together;</p>
<p><span class="math display">\[ \mathrm{SSR}_\mathbf{p} = \lVert \mathbf{f} - \mathbf{g}_\mathbf{p} \rVert ^2 \]</span></p>
<p><br></p>
<blockquote class="blockquote">
<p>In the below; orange is the observed, purple the predicts and green the overlap.</p>
</blockquote>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>μ <span class="op">=</span> <span class="dv">160</span> <span class="op">;</span> σ <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> [μ, σ]</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>histogram(p)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="{{ site.baseurl }}/images/linear_algebra/testing_rss.png" class="img-fluid">{:width=“400px”}</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>μ <span class="op">=</span> <span class="dv">179</span> <span class="op">;</span> σ <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> [μ, σ]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>histogram(p)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="{{ site.baseurl }}/images/linear_algebra/testing_rss1.png" class="img-fluid">{:width=“400px”}</p>
<p>Since each parameter vector <span class="math inline">\(\mathbf{p}\)</span> represents a different bell curve, each with its own value for the sum of squared residuals, <span class="math inline">\(\mathrm{SSR}\)</span>, we can draw the surface of <span class="math inline">\(\mathrm{SSR}\)</span> values over the space spanned by <span class="math inline">\(\mathbf{p}\)</span>, such as <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in this example.</p>
<p>Every point on this surface represents the SSR of a choice of parameters</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/residuals_as_contours.png" class="img-fluid">{:width=“400px”}</p>
<p>We can take a ‘top-down’ view of the surface, and view it as a contour map, where each of the contours (in green here) represent a constant value for the <span class="math inline">\(\mathrm{SSR}\)</span>.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/residuals_as_contours1.png" class="img-fluid">{:width=“400px”}</p>
<p>Often we can’t see the whole parameter space, so instead of just picking the lowest point, we have to make educated guesses where better points will be.</p>
<p>We can define another vector, <span class="math inline">\(\Delta\mathbf{p}\)</span>, in the same space as <span class="math inline">\(\mathbf{p}\)</span> that tells us what change can be made to <span class="math inline">\(\mathbf{p}\)</span> to get a better fit.</p>
<p>For example, a model with parameters <span class="math inline">\(\mathbf{p}'\)</span> = <span class="math inline">\(\mathbf{p}\)</span> + <span class="math inline">\(\Delta\mathbf{p}\)</span> will produce a better fit to data, if we can find a suitable <span class="math inline">\(\Delta\mathbf{p}\)</span>.</p>
<ul>
<li><p>Moving at right angles to contour lines in the parameter space is the most effective way to move through the space. Moving along contonour lines has no effect. Moving perpendicular to them can significantly improve or reduce the quality of the fit.</p></li>
<li><p>Moving along the contour lines <strong>does not</strong> produce the same model</p></li>
</ul>
<p>The <span class="math inline">\(\Delta\mathbf{p}\)</span> <span class="math inline">\(\begin{bmatrix}-2 \\ 2\end{bmatrix}\)</span> will give the best improvement in the model below;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/moving_contours.png" class="img-fluid">{:width=“400px”}</p>
<p><span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> have to be different, decrease <span class="math inline">\(\mu\)</span> along the x-axis, increase <span class="math inline">\(\sigma\)</span> along the y-axis.</p>
</section>
</section>
<section id="simultaneous-equations" class="level2">
<h2 class="anchored" data-anchor-id="simultaneous-equations">Simultaneous Equations</h2>
<p>Solving simultaneous equations is the process of finding the values of the variables (here <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>) that satisfy the system of equations.</p>
<p>The first goal when solving simple simultaneous equations should be to isolate one of the variables.</p>
<p>For example, using elimination, taking the second equation away from the first to solve the following pair of equations:</p>
<p><span class="math inline">\(3x−2y=7\)</span></p>
<p><span class="math inline">\(2x - 2y = 2\)</span></p>
<p><span class="math inline">\(x = 5, y = 4\)</span></p>
<p>You can use elimination even when the coefficients, the numbers in front of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, aren’t the same.</p>
<p>For example, multiplying both sides of the first equation by 2, then solve using the same method as the last question.</p>
<p><span class="math inline">\(3x−2y=4\)</span> <span class="math inline">\(\equiv\)</span> <span class="math inline">\(6x - 4y = 8\)</span></p>
<p><span class="math inline">\(6x + 3y = 15\)</span></p>
<p><span class="math inline">\(x = 2, y = 1\)</span></p>
<p>A very similar technique can be used to find the inverse of a matrix.</p>
<p>There is also the substitution method, where we rearrange one of the equations to the form <span class="math inline">\(x = ay+b\)</span> or <span class="math inline">\(y = cx+d\)</span> and then substitute <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span> into the other equation.</p>
<p>Systems of simultaneous equations can have more than two unknown variables. <br>Below there is a system with three; <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span>.</p>
<p>First try to find one of the variables by elimination or substitution, which will lead to two equations and two unknown variables.</p>
<p>Continue the process to find all of the variables.</p>
<p><span class="math inline">\(3x − 2y + z =7\)</span></p>
<p><span class="math inline">\(x + y + z = 2\)</span></p>
<p><span class="math inline">\(3x − 2y - z =3\)</span></p>
<p><span class="math inline">\(x = 1, y = -1, z = 2\)</span></p>
</section>
<section id="operations-with-vectors" class="level2">
<h2 class="anchored" data-anchor-id="operations-with-vectors">Operations with vectors</h2>
<p>Think of vectors within a co-ordinate system as mentioned previously.</p>
<p>Think of the co-ordinates as scalars that describe but also allow us to manipulate vectors.</p>
<p>The basis vector for the <span class="math inline">\(x\)</span> axis is typically <span class="math inline">\(\hat{i}\)</span> and <span class="math inline">\(\hat{j}\)</span> for the <span class="math inline">\(y\)</span> axis.</p>
<p>Vector addition is <em>associative</em> we can do it component by component.</p>
<blockquote class="blockquote">
<p>Formally, what this means is that if we have three vectors, r, s and, t, it doesn’t matter whether we add r plus s and then add t, or whether we add r to s plus t, it doesn’t matter where we put the bracket.</p>
</blockquote>
<p>In the image below <span class="math inline">\(\hat{i}\)</span> and <span class="math inline">\(\hat{j}\)</span> are our <em>basis vectors</em>, things that define the space. Vector addition can be thought of as combining two scaled vectors of <span class="math inline">\(\hat{i}\)</span> and <span class="math inline">\(\hat{j}\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/vector_addition.jpg" class="img-fluid">{:width=“400px”}</p>
<p>Mulitplication on vectors; <span class="math inline">\(2r\)</span> is 2 * the components of <span class="math inline">\(r\)</span></p>
<p><span class="math display">\[r = \begin{bmatrix}3 \\ 2\end{bmatrix}\]</span></p>
<p><span class="math display">\[2r = \begin{bmatrix}2*3 \\ 2*2\end{bmatrix} = \begin{bmatrix}6 \\ 4\end{bmatrix}\]</span></p>
<p>Vector subtraction; minus <span class="math inline">\(r\)</span> is not a shorter vector, that would be 0.5, rather it is in the oppositie direction. (Addition of negative one multiple the vector, ie vector + <span class="math inline">\((s * -1)\)</span> -&gt; this is a very handy way to think of subtraction)</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/vector_operations.jpg" class="img-fluid">{:width=“400px”}</p>
<p><strong>Thinking of vectors as representing attributes of objects</strong></p>
<p>We don’t have to think of vectors in a geometric space.</p>
<p>Vecotrs can represent information about an object. In this example, each vector will hold data relating to a house. Each row or co-ordinate,represents a separate piece of information; square metres, the number of bedrooms, bathrooms, and the price of a house.</p>
<p>Our vector operations still apply to these house objects, we can do operations on multiple houses, adding and multiplying (and introducing the concept of a negative house if there’s such a thing)</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/vectors_as_attributes.jpg" class="img-fluid">{:width=“400px”}</p>
<p><strong>Doing operations with vectors</strong></p>
<p>We have the following vectors;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/doingvectoroperations.png" class="img-fluid">{:width=“400px”}</p>
<p><span class="math inline">\(a\)</span> = <span class="math inline">\(\begin{bmatrix}2 \\ 2\end{bmatrix}\)</span> <span class="math inline">\(-b\)</span> = <span class="math inline">\(\begin{bmatrix}-1 \\ 2\end{bmatrix}\)</span> <span class="math inline">\(2c\)</span> = <span class="math inline">\(\begin{bmatrix}2 \\ 2\end{bmatrix}\)</span> <span class="math inline">\(d\)</span> = <span class="math inline">\(\begin{bmatrix}-1 \\ 2\end{bmatrix}\)</span></p>
<p><strong>Calculate</strong></p>
<p><span class="math inline">\(b + e\)</span> = <span class="math inline">\(\begin{bmatrix}-1 \\ -1\end{bmatrix}\)</span></p>
<p><span class="math inline">\(d - b\)</span> = <span class="math inline">\(\begin{bmatrix}-2 \\ 4\end{bmatrix}\)</span> -&gt; <em>think of it as <span class="math inline">\(2d\)</span>, you’re flipping <span class="math inline">\(b\)</span> because it’s subtraction</em></p>
</section>
<section id="size-of-a-vector" class="level2">
<h2 class="anchored" data-anchor-id="size-of-a-vector">Size of a vector</h2>
<p>Addition and scaling by a number are two main vector operations and allow us define the mathematical properties that a vector has.</p>
<p>We can define a vector, say <span class="math inline">\(r\)</span>, without any reference to any coordinate system. We can treat it as a geometric object, with just two properties, it’s length (size) and its direction.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/vector_r.png" class="img-fluid">{:width=“200px”}</p>
<p>If we wanted to calculated the size, or the length of <span class="math inline">\(r\)</span>, we could use a coordinate system with two axis that are orthogonal to each other.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/i_j_axis.png" class="img-fluid">{:width=“200px”}</p>
<p><span class="math inline">\(r = ai + bj\)</span> -&gt; a number of i, and b number of j</p>
<p>We assume <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are of length one, denoted by <span class="math inline">\(\hat{i}\)</span> <span class="math inline">\(\hat{j}\)</span></p>
<p>In this case <span class="math inline">\(r\)</span> could often be written as a column vector, ignoring i and j;</p>
<p><span class="math inline">\(r = \begin{bmatrix}a \\ b\end{bmatrix}\)</span></p>
<p>Length of <span class="math inline">\(r\)</span>, also denoted as <span class="math inline">\(\lVert r \rVert\)</span> is giving by the hypotenuse. <em>Remember that thing? It’s the longest side of a right-angled triangle</em>.</p>
<p>The size of a vector with two components is calculated using Pythagoras’ theorem</p>
<p><span class="math inline">\(\lVert r \rVert = \sqrt{a^2 + b^2}\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/denoting_r_2d.png" class="img-fluid">{:width=“400px”}</p>
<p><strong>We define the size of a vector through the sums of the squares of its components</strong></p>
<p>In this case we’ve used two spatial directions (i and j) but <strong>it doesn’t matter if the different components of the vector are dimensions in space or even things that have different fiscal units</strong> like length, time and price.</p>
<p>In fact, this definition can be extended to <strong>any number of dimensions</strong>; the size of a vector is the square root of the sum of the squares of its components.</p>
<p>The size of vector <span class="math inline">\(s = \begin{bmatrix}1 \\ 3 \\ 4 \\2\end{bmatrix}\)</span> is <span class="math inline">\(\lVert s \rVert = \sqrt{30}\)</span></p>
<p>The size of a vector is equal to the square root of the dot product (we’ll get to this) of the vector with itself;</p>
<p><span class="math display">\[\lVert r \rVert = \sqrt{r.r}\]</span></p>
<p><strong>Practice</strong></p>
<p>Let <span class="math inline">\(a = \begin{bmatrix}3 \\ 0 \\ 4\end{bmatrix}\)</span> and <span class="math inline">\(b = \begin{bmatrix}0 \\ 5 \\ 12\end{bmatrix}\)</span></p>
<p>Which is larger? <span class="math inline">\(\lVert a + b \rVert\)</span> or <span class="math inline">\(\lVert a \rVert + \lVert b \rVert\)</span></p>
<p><span class="math inline">\(\lVert a + b \rVert = \sqrt{3^2 + 5^2 + 16^2} = 17.029\)</span></p>
<p><span class="math inline">\(\lVert a \rVert + \lVert b \rVert = 5 + 13 = 18\)</span></p>
<p>This is known as <em>triangle inequality</em></p>
<p><span class="math display">\[\lVert a + b \rVert \le \lVert a \rVert + \lVert b \rVert\]</span></p>
<p><a href="https://www.youtube.com/watch?v=KlKYvbigBqs">Short video from Khan Academy explaining the theorem</a></p>
<p>Any one side of a triangle has to be less than the sum of the other two sides.</p>
</section>
<section id="dot-product-multiplying-vectors" class="level2">
<h2 class="anchored" data-anchor-id="dot-product-multiplying-vectors">Dot product, multiplying vectors</h2>
<p>Say we have two vectors, <span class="math inline">\(r\)</span> and <span class="math inline">\(s\)</span>, <span class="math inline">\(r\)</span> has the components <span class="math inline">\(ri\)</span> and <span class="math inline">\(rj\)</span>, <span class="math inline">\(s\)</span> has the components <span class="math inline">\(si\)</span> and <span class="math inline">\(sj\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/r_and_s.png" class="img-fluid">{:width=“300px”}</p>
<p>We define <span class="math inline">\(r . s\)</span> (the dot product) as multiplying the <span class="math inline">\(i\)</span> components and the <span class="math inline">\(j\)</span> components and then summing them.</p>
<p><span class="math inline">\(r.s = ri * si + rj * sj\)</span></p>
<p>Another way of saying this is; the dot product of two vectors is the sum of their componentwise products;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/dotproduct2.png" class="img-fluid">{:width=“400px”}</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dot(v: Vector, w: Vector) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Computes v_1 * w_1 + ... + v_n * w_n"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(v) <span class="op">==</span> <span class="bu">len</span>(w), <span class="st">"vectors must be same length</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="er">    return sum(v_i * w_i for v_i, w_i in zip(v, w))</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="er">assert dot([1, 2, 3], [4, 5, 6]) == 32 # 1 * 4 + 2 * 5 + 3 * 6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>It’s the length of the vector you’d get if you projected v onto w.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/dotproduct1.png" class="img-fluid">{:width=“400px”}</p>
<p>The dot product is therefore a scalar number not a vector. However, we get the vector of the dotted line above by multiplying <span class="math inline">\(w\)</span> by the dot product.</p>
<p><span class="math inline">\(r = \begin{bmatrix}3 \\ 2\end{bmatrix}\)</span> <span class="math inline">\(s = \begin{bmatrix}-1 \\ 2\end{bmatrix}\)</span> dot product = 1</p>
<p>Two properties of the dot product;</p>
<ol type="1">
<li><p>The dot product is <strong>commutative</strong> ie <code>r.s == s.r</code></p></li>
<li><p>The dot product is <strong>distributive over addition</strong> ie <code>r.(s + t) == r.s + r.t</code> - think in terms of unpacking</p></li>
<li><p>The dot product is <strong>associative over scalar multiplication</strong>, similar to addition, here <span class="math inline">\(a\)</span> is a scalar number; <code>r.(as) == a(r.s)</code> we can just pull scalar numbers out</p></li>
</ol>
<p>There is an interesting relationship between the size and the dot product of a vector.</p>
<p>If we dot a vector with its self <span class="math inline">\(r.r\)</span> we get the sum of the squares of its components (the square of it’s size, it’s modulus);</p>
<p><span class="math display">\[r.r = r_1 r_1 + r_2 r_2\]</span> <span class="math display">\[  = r_1^2 + r_2^2\]</span> <span class="math display">\[  = (\sqrt{r_1^2 + r_2^2})^2\]</span> <span class="math display">\[  = \lVert r \rVert^2\]</span></p>
<p>So we can get the size of a vector by dotting it and taking the square root.</p>
<blockquote class="blockquote">
<p>The dot product for two <span class="math inline">\(n\)</span> component vectors <span class="math inline">\(a, b = a_1b_1 + a_2b_2 + ... + a_nb_n\)</span></p>
</blockquote>
<p>The dot product of <span class="math inline">\(r\)</span> and <span class="math inline">\(s\)</span> below;</p>
<p><span class="math inline">\(r = \begin{bmatrix}-5\\3\\2\\8\end{bmatrix} s = \begin{bmatrix}1\\2\\-1\\0\end{bmatrix}\)</span></p>
<p><span class="math inline">\(r.s = -5 + 6 + -2 + 0 = -1\)</span></p>
<section id="cosine-dot-product" class="level3">
<h3 class="anchored" data-anchor-id="cosine-dot-product">Cosine &amp; dot product</h3>
<p><strong>Cosine rule</strong></p>
<p>If we have a triangle with sides a, b and c</p>
<p><span class="math inline">\(c^2 = a^2 + b^2 - 2ab * cos\theta\)</span></p>
<p>Theta being the angle between a and b</p>
<p>The cosine rule, when combined with the dot product can tell us <strong>the degree to which the two vectors are pointing in the same direction</strong>.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/cosine_rule.png" class="img-fluid">{:width=“300px”}</p>
<p>First, translate out the cosine rule using vector notation;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/cosine_as_vector.png" class="img-fluid">{:width=“300px”}</p>
<p>We know that <span class="math inline">\(\lVert r-s \rVert^2\)</span> is equal to <span class="math inline">\((r-s).(r-s)\)</span></p>
<p><span class="math display">\[(r-s).(r-s) = r.r \;\; r.-s \; -s.r \; -s.-s\]</span> <span class="math display">\[  = \lVert r \rVert^2 \: -2s.r \;\; \lVert s \rVert^2 \]</span></p>
<p>Now we’ve multiplied out <span class="math inline">\(\lVert r-s \rVert^2\)</span></p>
<p>We can compare that to the right hand side.</p>
<p><span class="math display">\[\lVert r \rVert^2 \: -2s.r \;\; \lVert s \rVert^2 = \lVert r \rVert^2 + \lVert s \rVert^2 - 2\lVert r \rVert\lVert s \rVertcos\theta\]</span></p>
<p>Equivalent to;</p>
<p><span class="math display">\[ s.r = \lVert r \rVert\lVert s \rVertcos\theta \]</span></p>
<p><strong>The dot product takes the size of the vectors and multiples by the angle between them.</strong></p>
<p>This tells us the extent to which the two vectors go in the same direction.</p>
<blockquote class="blockquote">
<p>dot product of a vector is a scalar quantity describing only the magnitude of a particular vector</p>
</blockquote>
<p><img src="{{ site.baseurl }}/images/linear_algebra/dotproduct3.png" class="img-fluid">{:width=“400px”}</p>
<p>If the vectors are 90 degrees from one another - they’re orothognal - (ie <span class="math inline">\(\theta\)</span> = 90), - <span class="math inline">\(cos 90 = 0\)</span> - the dot product is 0</p>
<p>If the vectors are pointing in the same direction, - ie there is no angle - (<span class="math inline">\(\theta\)</span> = 0) - <span class="math inline">\(cos 0 = 1\)</span> - the dot product equals the multiplication of the two sizes of the vectors (mod r <span class="math inline">\(\lVert r \rVert\)</span> and mod s <span class="math inline">\(\lVert s \rVert\)</span>) - a positive dot product tells us their moving in the same direction</p>
<p>If the vectors are pointing in opposite directions, - (<span class="math inline">\(\theta\)</span> = 180), - <span class="math inline">\(cos 180 = -1\)</span> - the dot product equals the minus the multiplication of the two sizes of the vectors (mod r <span class="math inline">\(\lVert r \rVert\)</span> and mod s <span class="math inline">\(\lVert s \rVert\)</span>) - a negative dot product tells us their moving in opposite directions</p>
<p>From this, we’ve derived a property in the dot product; <span class="math display">\[ r.s = \lVert r \rVert\lVert s \rVert cos\theta\]</span></p>
<p>We can also use this formula to find the angle (<span class="math inline">\(\theta\)</span>) between the two vectors</p>
</section>
<section id="projection" class="level3">
<h3 class="anchored" data-anchor-id="projection">Projection</h3>
<p>Cast your mind back to sohcahtoa;</p>
<p>soh -&gt; Sine = Opposite / Hypotenuse</p>
<p>cah -&gt; Cosine = Adjacent / Hypotenuse</p>
<p>toa -&gt; Tangent = Opposite / Adjacent</p>
<p>Here, the hypotenuse is the size of <span class="math inline">\(s\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/projection.png" class="img-fluid">{:width=“200px”}</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/projection3.png" class="img-fluid">{:width=“200px”}</p>
<p>Remember that the scalar projection is the size of the green vector.</p>
<p>If the angle between <span class="math inline">\(s\)</span> and <span class="math inline">\(r\)</span> is greater than <span class="math inline">\(\pi/2\)</span>, the projection will also have a minus sign.</p>
<p>We can substitute this in with our dot product</p>
<p><span class="math display">\[ r.s = \lVert r \rVert\lVert s \rVert cos\theta\]</span></p>
<p><span class="math inline">\(\lVert s \rVert cos\theta\)</span> is the adjacent side (dotted line in images above)</p>
<p>Think of the projection as a light coming down from <span class="math inline">\(s\)</span> at a right angle and the shadow cast onto <span class="math inline">\(r\)</span>. If <span class="math inline">\(s\)</span> and <span class="math inline">\(r\)</span> are at 90 degrees, it would be 0.</p>
<p>The projection of <span class="math inline">\(s\)</span> onto <span class="math inline">\(r\)</span> is not the same as <span class="math inline">\(r\)</span> onto <span class="math inline">\(s\)</span> - the light will be pointing at different angles.</p>
<p>The dot product gives us the projection multiplied by the size of r <span class="math inline">\(\lVert r \rVert\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/projection2.png" class="img-fluid">{:width=“200px”}</p>
<p>We can get the adjacent size by diving the dot product by mod r</p>
<p><span class="math display">\[ \frac{r.s}{\lVert r \rVert} = \lVert s \rVert cos\theta\]</span></p>
<p>Remember, <span class="math inline">\(r.s\)</span> is a number and <span class="math inline">\(\lVert r \rVert\)</span> is a number so you get a number, known as the <code>scalar projection</code></p>
<p>The dot product is also known as the projection product. It takes the projection of one vector onto another.</p>
<p>We can do projection in any number of dimensions. Consider two vectors with three components</p>
<p><span class="math inline">\(r = \begin{bmatrix}3\\-4\\0\end{bmatrix} s = \begin{bmatrix}10\\5\\-6\end{bmatrix}\)</span></p>
<p>The scalar projection of <span class="math inline">\(s\)</span> on <span class="math inline">\(r\)</span>;</p>
<p><span class="math display">\[ proj_rs = \frac{s.r}{\lVert r \rVert}\]</span></p>
<p><span class="math inline">\(s.r = 10;\;\; r.r = 5\)</span></p>
<p><strong>The vector projection</strong> allows us to encode something about which direction <span class="math inline">\(r\)</span> was going, into the <code>scalar projection</code></p>
<p>Vector projection; <span class="math display">\[ r\; \frac{r.s}{\lVert r \rVert \lVert r \rVert} = r\; \frac{r.s}{r.r}\]</span></p>
<p>We’ve take the scalar projection (how much <span class="math inline">\(s\)</span> goes along <span class="math inline">\(r\)</span> or <span class="math inline">\(\frac{r.s}{\lVert r \rVert}\)</span> and multiplied it by <span class="math inline">\(r\)</span> divided by its length or <span class="math inline">\(r\; \frac{1}{\lVert r \rVert}\)</span> (this produces a vector going the direction of <span class="math inline">\(r\)</span> but normalised to have a length 1)</p>
<p>The vector projection is a number multiplied by a unit vector (that goes in the direction of <span class="math inline">\(r\)</span>)</p>
<p>Taking our example from above, given the the scalar projection is 2, the vector projection of <span class="math inline">\(s\)</span> onto <span class="math inline">\(r\)</span></p>
<p><span class="math inline">\(r = \begin{bmatrix}3\\-4\\0\end{bmatrix} s = \begin{bmatrix}10\\5\\-6\end{bmatrix}\)</span></p>
<p>The scalar projection of <span class="math inline">\(s\)</span> on <span class="math inline">\(r\)</span>;</p>
<p><span class="math display">\[ proj_rs = \frac{s.r}{\lVert r \rVert} = 2\]</span></p>
<p><span class="math inline">\(s.r = 10;\;\; r.r = 5\)</span></p>
<p>The vector projection is;</p>
<p><span class="math display">\[\frac{s.r}{r.r}r\]</span></p>
<p>So you can multiple <span class="math inline">\(r\)</span> by the scalar projection and then divide by <span class="math inline">\(r\)</span></p>
<p><span class="math inline">\(\begin{bmatrix}3\\-4\\0\end{bmatrix} *2 = \begin{bmatrix}6\\-8\\0\end{bmatrix}\)</span></p>
<p>normalised by length r = <span class="math inline">\(\begin{bmatrix}6/5\\-8/5\\0\end{bmatrix}\)</span></p>
</section>
</section>
<section id="basis-the-coordinate-system-in-which-our-vectors-exists" class="level2">
<h2 class="anchored" data-anchor-id="basis-the-coordinate-system-in-which-our-vectors-exists">Basis, the coordinate system in which our vectors exists</h2>
<p>The coordinate system is what we use to describe space. These are our basis vectors.</p>
<p>We describe our vectors in terms of our basis vectors.</p>
<p>Any time we describe vectors numerically it implies implicitly on what basis vectors we’re using.</p>
<p><span class="math inline">\(r\)</span> described in terms of <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> (hat -&gt; <span class="math inline">\(\hat{e}\)</span> represents they are of unit length)</p>
<p>In a 2D plane we can describe all points with different combinations of our two basis vectors (<em>as long as linearly independent</em> i.e.&nbsp;not pointing in the same direction).</p>
<p>The “span” of <span class="math inline">\(\hat{e_1}\)</span> and <span class="math inline">\(\hat{e_2}\)</span> is the set of all their linear combinations; <span class="math inline">\(a\hat{e_1} + b\hat{e_2}\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/r_intermsofe.png" class="img-fluid">{:width=“400px”}</p>
<p><span class="math inline">\(r\)</span> exists independently of the basis vectors. It takes us from an origin to another point, but we use the coordinate system to describe <span class="math inline">\(r\)</span>.</p>
<p>If we want to do a <strong>transformation of axes</strong> i.e.&nbsp;change the coordinate system we use to describe <span class="math inline">\(r\)</span>, <strong>if</strong> the vectors are at a 90 degree angle from one another we can use the <em>dot product</em>, otherwise we need to use a matrix.</p>
<p>Here, we know <span class="math inline">\(b\)</span> in terms of <span class="math inline">\(e\)</span> and the vectors are at a 90 degreee angle to one another (orthogonal), so we can work out <span class="math inline">\(r\)</span> in terms of <span class="math inline">\(b\)</span> using the dot product.</p>
<p>You check that two vectors are orthogonal by taking the dot product, multiply two vectors and divide by their length;</p>
<p><span class="math display">\[ cos\theta = \frac{b_1.b_2}{\lVert b_1 \rVert\lVert b_2 \rVert}\]</span></p>
<p>If the dot product is 0, <span class="math inline">\(cos\theta\)</span> is 0 and they’re at 90 degrees to one another.</p>
<p>Using the example below;</p>
<p><span class="math display">\[b_1.b_2 = (2*-2) + (1*4) = 0\]</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/axistransformation1.png" class="img-fluid">{:width=“400px”}</p>
<p>You can project <span class="math inline">\(r\)</span> down onto <span class="math inline">\(b_2\)</span> and calculate the vector projection. This will tell you “how much of that axis (<span class="math inline">\(b_2\)</span>) you need”. You then do the same for <span class="math inline">\(b_1\)</span></p>
<p>The sum of those two vector projections, give you <span class="math inline">\(r\)</span> in terms of <span class="math inline">\(b\)</span> or <span class="math inline">\(r_b\)</span>.</p>
<p>The vector projection for <span class="math inline">\(b_1\)</span>;</p>
<p><span class="math display">\[r_e = \begin{bmatrix}3\\4\end{bmatrix}\;\; b_1 = \begin{bmatrix}2\\1\end{bmatrix}\]</span></p>
<p><span class="math display">\[\frac{r_e.b_1}{\lVert b_1 \rVert^2} = \frac{(3*2)+(4*1)}{2^2 + 1^2} = \frac{10}{5} = 2\]</span></p>
<p>And the same for the other axis</p>
<p><span class="math display">\[b_2 = \begin{bmatrix}-2\\4\end{bmatrix}\]</span></p>
<p><span class="math display">\[\frac{r_e.b_2}{\lVert b_2 \rVert^2} = \frac{(3*-2)+(4*4)}{-2^2 + 4^2} = \frac{10}{20} = 1/2\]</span></p>
<p><span class="math display">\[r_b = \begin{bmatrix}2\\1/2\end{bmatrix}\]</span></p>
<p>We can verify by multiplying our <span class="math inline">\(b\)</span>s in terms of <span class="math inline">\(e\)</span> by the scalar projections (normalising) to product <span class="math inline">\(r_e\)</span></p>
<p><span class="math display">\[b_1 = \begin{bmatrix}2\\1\end{bmatrix} *2 = \begin{bmatrix}4\\2\end{bmatrix}\]</span></p>
<p><span class="math display">\[b_2 = \begin{bmatrix}-2\\4\end{bmatrix} *1/2 = \begin{bmatrix}-1\\2\end{bmatrix}\]</span></p>
<p><span class="math display">\[\begin{bmatrix}4\\2\end{bmatrix} + \begin{bmatrix}-1\\2\end{bmatrix} = \begin{bmatrix}3\\4\end{bmatrix} \]</span></p>
<p>We can re-describe <span class="math inline">\(r\)</span> using a new set of basis vectors.</p>
<hr>
<p><strong>Practice</strong></p>
<ol type="1">
<li></li>
</ol>
<p>Given the following vectors are written in the standard basis, represent v in terms of b;</p>
<p><span class="math display">\[v = \begin{bmatrix}5\\-1\end{bmatrix}\;\; b_1 = \begin{bmatrix}1\\1\end{bmatrix}\;\; b_2 = \begin{bmatrix}1\\-1\end{bmatrix}\]</span></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>vb1 <span class="op">=</span> ((<span class="dv">5</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> <span class="dv">1</span>)) <span class="op">/</span> (<span class="dv">1</span><span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span><span class="op">**</span><span class="dv">2</span>) <span class="co"># 2</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>vb2 <span class="op">=</span> ((<span class="dv">5</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> (<span class="dv">1</span><span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="bu">pow</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)) <span class="co"># 3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math display">\[v_b = \begin{bmatrix}2\\3\end{bmatrix}\]</span></p>
<ol start="2" type="1">
<li></li>
</ol>
<p><span class="math display">\[v = \begin{bmatrix}10\\-5\end{bmatrix}\;\; b_1 = \begin{bmatrix}3\\4\end{bmatrix}\;\; b_2 = \begin{bmatrix}4\\-3\end{bmatrix}\]</span></p>
<p><span class="math display">\[v_b = \begin{bmatrix}2/5\\11/5\end{bmatrix}\]</span></p>
<ol start="3" type="1">
<li></li>
</ol>
<p><span class="math display">\[v = \begin{bmatrix}2\\2\end{bmatrix}\;\; b_1 = \begin{bmatrix}-3\\1\end{bmatrix}\;\; b_2 = \begin{bmatrix}1\\3\end{bmatrix}\]</span></p>
<p><span class="math display">\[v_b = \begin{bmatrix}-2/5\\4/5\end{bmatrix}\]</span></p>
<ol start="4" type="1">
<li></li>
</ol>
<p><span class="math display">\[v = \begin{bmatrix}1\\1\\1\end{bmatrix}\;\; b_1 = \begin{bmatrix}2\\1\\0\end{bmatrix}\;\; b_2 = \begin{bmatrix}1\\-2\\-1\end{bmatrix}\;\; b_3 = \begin{bmatrix}-1\\2\\5\end{bmatrix}\]</span></p>
<p><span class="math display">\[v_b = \begin{bmatrix}3/5\\-1/3\\-2/15\end{bmatrix}\]</span></p>
<ol start="5" type="1">
<li></li>
</ol>
<p><span class="math display">\[v = \begin{bmatrix}1\\1\\2\\3\end{bmatrix}\;\; b_1 = \begin{bmatrix}1\\0\\0\\0\end{bmatrix}\;\; b_2 = \begin{bmatrix}0\\2\\-1\\0\end{bmatrix}\;\; b_3 = \begin{bmatrix}0\\1\\2\\0\end{bmatrix}\;\;
b_4 = \begin{bmatrix}0\\0\\0\\3\end{bmatrix}\]</span></p>
<p><span class="math display">\[v_b = \begin{bmatrix}1\\0\\1\\1\end{bmatrix}\]</span></p>
<hr>
<section id="defining-basis-vector-space-and-linear-independence" class="level3">
<h3 class="anchored" data-anchor-id="defining-basis-vector-space-and-linear-independence">Defining basis, vector space and linear independence</h3>
<p>A basis is a set of <span class="math inline">\(n\)</span> vectors that;</p>
<ul>
<li>Are not linear combinations of each other <br> and are therefore linearly independent</li>
<li>Span the space</li>
<li>Our space is therefore n-dimensional</li>
</ul>
<p><span class="math inline">\(b_3\)</span> is not a valid basis vector. Because I can take some combination of <span class="math inline">\(b_2\)</span> and <span class="math inline">\(b_1\)</span> to get <span class="math inline">\(b_3\)</span></p>
<p>We cannot write one of the vectors as a linear combination of the others</p>
<p>ie: <span class="math inline">\(b_3 = a_1 b_1 + a_2 b_2\)</span></p>
<p>“It lies in the same plane as <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span>”</p>
<p>Formula required to show linear independence;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/basis_linearindependence.png" class="img-fluid">{:width=“400px”}</p>
<p>Linearly <strong>dependent</strong>;</p>
<p><span class="math inline">\(a = \begin{bmatrix}1\\1\end{bmatrix}\;\; b = \begin{bmatrix}2\\2\end{bmatrix}\)</span></p>
<p>as;</p>
<p><span class="math inline">\(a = \frac{1}{2}b\)</span></p>
<p>Similarly, <span class="math inline">\(\mathbf{a} = q_1\mathbf{b} + q_2\mathbf{c}\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/basis_linearindependence2.png" class="img-fluid">{:width=“400px”}</p>
<p>Find <span class="math inline">\(q_1, q_2\)</span></p>
<p><span class="math inline">\(\begin{bmatrix}2\\2\end{bmatrix}\;\; = q_1 \begin{bmatrix}1\\-2\end{bmatrix} + q_2 \begin{bmatrix}-1\\0\end{bmatrix}\)</span></p>
<p><span class="math inline">\(q_1 = -1,\;\; q_2 = -3\)</span></p>
<p>Linearly <strong>independent</strong>; one is not a scalar multiple of the other</p>
<p><span class="math inline">\(a = \begin{bmatrix}1\\1\end{bmatrix}\;\; b = \begin{bmatrix}2\\1\end{bmatrix}\)</span></p>
<p>Basis vectors do not need to be; - Of unit length (length one) - Orthogonal - Though it’s much if they are both of these things</p>
<p>Linearly <strong>independent</strong></p>
<p><span class="math inline">\(a = \begin{bmatrix}1\\0\\0\end{bmatrix}\;\; b = \begin{bmatrix}1\\1\\0\end{bmatrix}\;\; c = \begin{bmatrix}1\\0\\1\end{bmatrix}\)</span></p>
<p>Easy to tell, as you need all three vectors to cover each element. Whereas in the following example, you do not;</p>
<p>Linearly <strong>dependent</strong></p>
<p><span class="math inline">\(a = \begin{bmatrix}1\\2\\0\end{bmatrix}\;\; b = \begin{bmatrix}-2\\1\\3\end{bmatrix}\;\; c = \begin{bmatrix}4\\3\\-3\end{bmatrix}\)</span></p>
<p><span class="math inline">\(c = 2a - b\)</span></p>
<p><strong>What happens when we map from one basis to another?</strong></p>
<p>The original grid projects down onto the new grid.</p>
<p>Though it will potentially have different values on that grid, the projection keeps the grid being evenly spaced.</p>
<p>Therefore, any mapping we do from one set of basis vectors, from one coordinate system to another, keeps the vector space being a regularly spaced grid. Ensuring, our original vector rules of vector addition and multiplication by a scalar still work.</p>
<p>It doesn’t warp or fold space which is what the linear bit in linear algebra means. Things might be stretched or rotated or inverted, but everything remains evenly spaced and linear combinations still work.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/axistransformation2.png" class="img-fluid">{:width=“400px”}</p>
</section>
<section id="applications-of-changing-basis" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-changing-basis">Applications of changing basis</h3>
<p>Changing the basis is what we’re doing when we do linear regression.</p>
<p>We’re working out the distance of points from a vector (distance least squared).</p>
<p>We use the dot-product to do the projection to map the data from the x-y space onto the space of the line.</p>
<p>There’s some theoretical disputes, whether the distance should be measured straight (y-axis) or orthognally (the angle).</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/applicationofchangbasis.png" class="img-fluid">{:width=“400px”}</p>
<p>In a neural network for face recognition, the goal of the learning process is going to be to somehow derive a set of basis vectors that extract the most information-rich features of the faces.</p>
<hr>
<p><strong>Practice</strong></p>
<p><strong>Q1</strong></p>
<p>A ship travels with the velcoity given by <span class="math inline">\(\begin{bmatrix}1\\2\end{bmatrix}\)</span>, with current flowing in the direction given by <span class="math inline">\(\begin{bmatrix}1\\1\end{bmatrix}\)</span> with respect to some co-ordinate axes.</p>
<p>What is the velocity of the ship in the direction of the current?</p>
<ul>
<li>vector projection of the velocity of the ship, onto the velocity of the current</li>
</ul>
<p><span class="math inline">\(\frac{ship . current}{current . current} * current\)</span></p>
<p><span class="math inline">\(\frac{\begin{bmatrix}1\\2\end{bmatrix} . \begin{bmatrix}1\\1\end{bmatrix}} {\begin{bmatrix}1\\1\end{bmatrix} . \begin{bmatrix}1\\1\end{bmatrix}} * \begin{bmatrix}1\\1\end{bmatrix}\)</span></p>
<p><span class="math inline">\(\begin{bmatrix}3/2\\3/2\end{bmatrix}\)</span></p>
<p><strong>Q2</strong></p>
<p>A ball travels with the velocity given by <span class="math inline">\(\begin{bmatrix}2\\1\end{bmatrix}\)</span>, with wind blowing in the direction given by <span class="math inline">\(\begin{bmatrix}3\\-4\end{bmatrix}\)</span> with respect to some co-ordinate axes.</p>
<p>What is the size of the velocity of the ball in the direction of the wind?</p>
<p>This is the scalar projection of the velocity of the ball onto the velocity of the wind.</p>
<p>If you were to draw a straight line from the co-ordinate of the ball onto the vector of the wind, what is the size of that vector in terms of the wind vector.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/projection3.png" class="img-fluid">{:width=“200px”}</p>
<p>Remember that the scalar projection is the size of the green vector.</p>
<p><span class="math inline">\(\frac{\begin{bmatrix}2\\1\end{bmatrix} . \begin{bmatrix}3\\-4\end{bmatrix}} {\sqrt{3^2 + -4^2}}\)</span></p>
<p><span class="math inline">\(\frac{2}{5}\)</span></p>
<p><strong>Q3</strong></p>
<p>Given vectors <span class="math inline">\(v = \begin{bmatrix}-4\\-3\\8\end{bmatrix}\; b_1 = \begin{bmatrix}1\\2\\3\end{bmatrix}\; b_2 = \begin{bmatrix}-2\\1\\0\end{bmatrix}\; b_3 = \begin{bmatrix}-3\\-6\\-5\end{bmatrix}\)</span></p>
<p>All written in the standard basis, what is <span class="math inline">\(v\)</span> in the basis defined by <span class="math inline">\(b_1, b_2, b_3\)</span>? -&gt; they are all pairwise orthogonal to one another.</p>
<p><em>Answer</em> What manipulation do you need to do in order to produce <span class="math inline">\(v\)</span>. Simply add them together, so one of each vector.</p>
<p>This is a change of basis in 3 dimensions.</p>
<p><span class="math inline">\(\begin{bmatrix}1\\1\\1\end{bmatrix}\)</span></p>
<p><strong>Q4</strong></p>
<p>Are the following vectors linearly independent?</p>
<p><span class="math inline">\(a = \begin{bmatrix}1\\2\\-1\end{bmatrix}\; b = \begin{bmatrix}3\\-4\\5\end{bmatrix}\; c = \begin{bmatrix}1\\-8\\7\end{bmatrix}\)</span></p>
<p>No, <span class="math inline">\(b = 2a + c\)</span></p>
<p><strong>Q5</strong></p>
<p>At 12:00 pm, a spaceship is at position <span class="math inline">\(\begin{bmatrix}3\\2\\4\end{bmatrix} km\)</span> away form the origin with respect to some 3 dimensional co-ordinate system. The ship is travelling with velocity <span class="math inline">\(\begin{bmatrix}3\\2\\4\end{bmatrix} km/h\)</span>. What is the location of the spaceship after 2 hours have passed?</p>
<p><em>Answer</em> Multiply the velocity by 2 and then add it to the current position to get the new position.</p>
<p><span class="math inline">\(\begin{bmatrix}1\\6\\-2\end{bmatrix}\)</span></p>
</section>
</section>
<section id="introduction-to-matrices" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-matrices">Introduction to matrices</h2>
<blockquote class="blockquote">
<p>Unfortunately, no one can be told what the Matrix is. You have to see if for yourself - <em>Morpheus</em></p>
</blockquote>
<p>Morpheus may have a point. Like the proverbial fish who has no idea what water is, we are swimming in matrices.</p>
<blockquote class="blockquote">
<p>Matrices are everywhere; anything that can be put in an Excel spreadsheet is a matrix, and language and pictures can be represented as matrices as well. <a href="https://github.com/fastai/numerical-linear-algebra/blob/master/README.md"><em>fast ai</em></a></p>
</blockquote>
<p>A matrix is a two-dimensional collection of numbers, a list of lists. If a matrix has <span class="math inline">\(n\)</span> rows and <span class="math inline">\(k\)</span> columns, we refer to it as an <span class="math inline">\(n \times k matrix\)</span>. We can think of each column in a matrix as a vector of length <span class="math inline">\(n\)</span>.</p>
<p>When working with matrices to represent data, we need to think of them slightly differently to tabular data structures. However, they can to some extent both follow the <a href="https://vita.had.co.nz/papers/tidy-data.pdf">tidy data</a> philosophy where each column represents an attribute and each row a record.</p>
<p>Matrices can also be used to represent binary relationships, where if <code>A[i][j] = 1</code> then nodes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are connected (see <em>hot encoding</em>).</p>
<p>Matrices can also be thought of as objects that rotate and stretch vectors. They also help us solve simultaneous equations.</p>
<p><span class="math display">\[2a + 3b = 8\]</span> <span class="math display">\[10a + 1b = 13\]</span></p>
<p>Can be rewritten as;</p>
<p><span class="math display">\[\begin{pmatrix} 2a + 3b \\ 10a + 1b \end{pmatrix} = \begin{pmatrix} 8 \\ 13 \end{pmatrix}\]</span></p>
<p>Or</p>
<p><span class="math display">\[\begin{pmatrix} 2 &amp; 3 \\ 10 &amp; 1 \end{pmatrix} \begin{pmatrix} a \\ b \end{pmatrix} = \begin{pmatrix} 8 \\ 13 \end{pmatrix}\]</span></p>
<p>The matrix <span class="math inline">\(\begin{pmatrix} 2 &amp; 3 \\ 10 &amp; 1 \end{pmatrix}\)</span> acts on the vector <span class="math inline">\(\begin{bmatrix}a\\b\end{bmatrix}\)</span></p>
<p>So we then ask, what vector transforms to produce <span class="math inline">\(\begin{pmatrix} 8 \\ 13 \end{pmatrix}\)</span></p>
<p>This is the heart of linear algebra.</p>
<blockquote class="blockquote">
<p>Now we can see what we mean now by the term linear algebra.</p>
<p>Linear algebra is linear, because it takes input values, our a and b, and multiplies them by constants. So everything is linear. And it’s algebra, that is it’s a notation describing mathematical objects and a system of manipulating those notations.</p>
<p>So linear algebra is a mathematical system for manipulating vectors in the spaces described by vectors.</p>
<p>So this is interesting. There seems to be some kind of deep connection between simultaneous equations, these things called matrices, the vectors. And it turns that the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices, which is the heart of linear algebra.</p>
</blockquote>
<p>We can multiply the matrix <span class="math inline">\(\begin{pmatrix} 2 &amp; 3 \\ 10 &amp; 1 \end{pmatrix}\)</span> by some basis vectors, for x we have <span class="math inline">\(e_1 = \begin{bmatrix}1\\0\end{bmatrix}\)</span> and for y we have <span class="math inline">\(e_2 = \begin{bmatrix}0\\1\end{bmatrix}\)</span></p>
<p>When multiplying vectors;</p>
<p><span class="math display">\[\begin{pmatrix} 2 &amp; 3 \\ 10 &amp; 1 \end{pmatrix} * \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 2*1 + 3*0 \\ 10*1 + 1*0 \end{pmatrix} = \begin{pmatrix} 2 \\ 10 \end{pmatrix}\]</span></p>
<p><span class="math display">\[\begin{pmatrix} 2 &amp; 3 \\ 10 &amp; 1 \end{pmatrix} * \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 2*0 + 3*1 \\ 10*0 + 1*1 \end{pmatrix} = \begin{pmatrix} 3 \\ 1 \end{pmatrix}\]</span></p>
<p>So the matrix <span class="math inline">\(\begin{pmatrix} 2 &amp; 3 \\ 10 &amp; 1 \end{pmatrix}\)</span> is transforming our basis vectors, it’s a function that operates on input vectors and gives us output vectors.</p>
<p>A set of simultaneous equations is asking what vector I need in order to get a transformed product at the position 8 13.</p>
<section id="linear-transformations" class="level3">
<h3 class="anchored" data-anchor-id="linear-transformations">Linear transformations</h3>
<p>Transformation is a another term for function <span class="math inline">\(f(x)\)</span>, it takes an input and produces an output.</p>
<p>In linear algebra a matrix transformation takes in a vector and produces another vectors. Transformation suggests movement, again thinking geometrically.</p>
<p>We can think of every corresponding input vector within a space, moving to its corresponding output vectors.</p>
<p>Think of vectors as points (i.e.&nbsp;where the arrow points) and think of that point moving to some other point.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/lineartransformations1.png" class="img-fluid">{:width=“400”}</p>
<p>We have the ability to move around all the points in space. Though the types of transformations are limited to <code>linear</code> transformations.</p>
<p>Visually, what this means is that all lines in our original coordinate system must remain lines, and our origin remains fixed.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/lineartransformations2.png" class="img-fluid">{:width=“400”}</p>
<p>It’s not just the vertical and horizontal lines that must remain lines, but all angles, so diagonals as well. Grid lines remain parallel and evenly spaces.</p>
<p>The implications of this are important.</p>
<p>A vector, say <span class="math inline">\(v = \begin{bmatrix} -1\\2 \end{bmatrix}\)</span></p>
<p>We know is actually, <span class="math inline">\(\begin{bmatrix} -1\hat{i}\\2 \hat{j}\end{bmatrix}\)</span></p>
<p>When do we the transformation, we simply need to understand what happens <span class="math inline">\(\hat{i}\)</span> and <span class="math inline">\(\hat{j}\)</span>.</p>
<p><span class="math inline">\(v\)</span> will still be the same linear combination of <span class="math inline">\(\hat{i}\)</span> and <span class="math inline">\(\hat{j}\)</span>.</p>
<p>transformed <span class="math inline">\(v\)</span> = -1(transformed <span class="math inline">\(\hat{i}\)</span>) + 2(transformed <span class="math inline">\(\hat{j}\)</span>)</p>
<p>In a 2 dimensional matrix, we need just four numbers to know where any vector will now land. The coordinates for <span class="math inline">\(\hat{i}\)</span> and <span class="math inline">\(\hat{j}\)</span>.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/lineartransformations3.png" class="img-fluid">{:width=“400”}</p>
<p>Any vector can then be translated from the old basis vectors to the new transformation.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/lineartransformations4.png" class="img-fluid">{:width=“400”}</p>
<p><code>in matrix multiplication, remember; rows times cols</code></p>
<p>The intuition here is to think of the two columns in the matrix as where your basis vectors end up, and your vector as a linear combination of your new basis vectors.</p>
<p>A matrix is therefore a transformation of space.</p>
</section>
<section id="how-matrices-transform-space" class="level3">
<h3 class="anchored" data-anchor-id="how-matrices-transform-space">How matrices transform space</h3>
<p>Space changes include;</p>
<ul>
<li>stretches,</li>
<li>inversions,</li>
<li>mirrors,</li>
<li>shears,</li>
<li>rotations</li>
</ul>
<blockquote class="blockquote">
<p>We can think of a matrix multiplication being the multiplication of the vector sum of the transformed basis vectors.</p>
</blockquote>
<p>The identity matrix <span class="math inline">\((I)\)</span>, the matrix that does nothing. It is composed of the basis vectors.</p>
<p><span class="math display">\[\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}\]</span></p>
<p>The “Identity Matrix” is the matrix equivalent of the number “1”. It is “square” (has same number of rows as columns)</p>
<p>Gives us an axis of 1 x, 1 y –&gt; <span class="math inline">\(\begin{bmatrix}x\\y\end{bmatrix}\)</span></p>
<p>The matrix; <span class="math display">\[\begin{pmatrix} 3 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\]</span></p>
<p>Would expand our basis vectors by 3x and 2y.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_transformation1.png" class="img-fluid">{:width=“200”}</p>
<p>A fraction would shrink space.</p>
<p>A negative reverses the space. “Changing the sense of the co-ordinate system”</p>
<p><span class="math display">\[\begin{pmatrix} -1 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\]</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_transformation2.png" class="img-fluid">{:width=“200”}</p>
<p>Two negatives will invert everything. Inversion.</p>
<p>You can have a matrix that is akin to having a mirror, where it shifts both axses</p>
<p><span class="math display">\[\begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\]</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_transformation3.png" class="img-fluid">{:width=“400”}</p>
<p>The following matrix acts as a vertical mirror plane <span class="math inline">\(\begin{pmatrix} -1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}\)</span></p>
<p>The following matrix acts as a horizontal mirror plane <span class="math inline">\(\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{pmatrix}\)</span></p>
<p>You can also have <code>shears</code></p>
<blockquote class="blockquote">
<p>In plane geometry, a shear mapping is a linear map that displaces each point in fixed direction, by an amount proportional to its signed distance from the line that is parallel to that direction and goes through the origin. This type of mapping is also called shear transformation, transvection, or just shearing.</p>
</blockquote>
<p>For instance, keeping <span class="math inline">\(\hat{e}_1\)</span> in place but transforming <span class="math inline">\(\hat{e}_2\)</span> to <span class="math inline">\(\prime{e}_2\)</span> (e-prime)</p>
<p><span class="math inline">\(\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span> to <span class="math inline">\(\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}\)</span></p>
<p>Creates a parallelogram;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_transformation4.png" class="img-fluid">{:width=“400”}</p>
<p>A 90 degree anticlockwise rotation will look like;</p>
<p><span class="math inline">\(\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span> to <span class="math inline">\(\begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_transformation5.png" class="img-fluid">{:width=“400”}</p>
<p>A general expression for a rotation in 2d can be written as;</p>
<p><span class="math inline">\(\begin{bmatrix} cos\theta &amp; sin\theta \\ -sin\theta &amp; cos\theta \end{bmatrix}\)</span></p>
<p>Rotations are less relevant in most data science applications of linear algebra. But image classification and facial recognition would use this transformations in order to get all the images in a certain state before analysing them - ie remove the camera angles and make the faces portrait.</p>
</section>
<section id="combining-matrix-transformations" class="level3">
<h3 class="anchored" data-anchor-id="combining-matrix-transformations">Combining matrix transformations</h3>
<p>If you apply one transformation (a rotation), and then another (a shear), you live get a linear transformation that is distinct from the rotation and shear.</p>
<p>Applying multiple transformations forms a composition. The composition is the product of multiple matrices.</p>
<p>The composition can be described by its own matrix.</p>
<p>Like function notation, we read from inside out <span class="math inline">\(f(g(x))\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/composition1.png" class="img-fluid">{:width=“400”}</p>
<p>You can see why we do <code>rows times cols</code>.</p>
<ul>
<li>The matrix first applied is where the new basis vectors land.</li>
<li><span class="math inline">\(\hat{i}\)</span> and <span class="math inline">\(\hat{j}\)</span> are represented as columns in the matrix on the right.</li>
<li>We then need to multiply each vector (column) by the new matrix.</li>
</ul>
<p>The image below gives us the first column of our composition.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/composition2.png" class="img-fluid">{:width=“400”}</p>
<p>The intuition is to think of applying one matrix transformation after another. This is why the order in which the transformations are applied matters. To rotate and then shear has a different effect than if you shear and then rotate.</p>
<p>Matrix multiplication is <em>associative</em> <span class="math inline">\(AB(C) \equal A(BC)\)</span>, because this doesn’t change the order in which the transformations are applied.</p>
<p>If we take the basis vectors <span class="math inline">\(\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(A_1\)</span> represents a 90 degree clockwise rotation <span class="math inline">\(\begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(A_2\)</span> represents a mirror along the vertical plane (<em>ie</em> flips the x axis, with no change to y) <span class="math inline">\(\begin{bmatrix} -1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span></p>
<p>We can calculate the result of applying the two transformation <span class="math inline">\(A_2(A_1 r)\)</span> by first applying <span class="math inline">\(A_1\)</span> to our basis vectors and then applying <span class="math inline">\(A_2\)</span> to the result.</p>
<p><span class="math display">\[A_2 A_1 = \begin{bmatrix} -1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix}\]</span></p>
<p>We can multiply all possible rows and columns, without having to draw it out.</p>
<ul>
<li>Top left value = the first row of <span class="math inline">\(A_2\)</span> and multiply it by the first column in <span class="math inline">\(A_1\)</span>,</li>
<li>Bottom left value = second row, first column</li>
<li>Top right = first row, second column</li>
<li>Bottom right = second row, second column</li>
</ul>
<p><span class="math display">\[ = \begin{bmatrix} (-1 * 0) + (0 * -1) &amp; (-1 * -1) + (0*0)\\ (-1 * 1) + (0 * 0) &amp; (0 * 1) + (1 * 0) \end{bmatrix}\]</span></p>
<p><span class="math display">\[ = \begin{bmatrix} 0 &amp; -1 \\ -1 &amp; 0 \end{bmatrix}\]</span></p>
<p>The order of transformations matter, doing <span class="math inline">\(A_2\)</span> and then <span class="math inline">\(A_1\)</span> gives us a different result.</p>
<p><span class="math display">\[A_1 A_2 = \begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix} \begin{bmatrix} -1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\]</span></p>
<p><span class="math display">\[A_1 A_2 = \begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}\]</span></p>
<p><strong>Matirx multiplication isn’t commutative</strong></p>
<p>We can do them in any order, meaning they are associative. <span class="math inline">\(A_3(A_2 A_1)\)</span> is the same as <span class="math inline">\((A_3 A_2) A_1\)</span> but we cannot interchange the order.</p>
</section>
<section id="practice-matrix-transformations" class="level3">
<h3 class="anchored" data-anchor-id="practice-matrix-transformations">Practice matrix transformations</h3>
<p>Matrices make transformations on vectors, potentially changing their magnitude and direction.</p>
<p>If we have two unit vectors (in orange) and another vector, <span class="math inline">\(r = \begin{bmatrix} 3 \\ 2 \end{bmatrix}\)</span> (in pink), before any transformations - these look like this:</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_transformation6.png" class="img-fluid">{:width=“200”}</p>
<p>The matrix, <span class="math inline">\(A = \begin{pmatrix} 1/2 &amp; -1 \\ 0 &amp; 3/4 \end{pmatrix}\)</span> will transform them;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_transformation7.png" class="img-fluid">{:width=“200”}</p>
<p><strong>Q1</strong></p>
<p>r prime, or <span class="math inline">\(A\)</span> applied to <span class="math inline">\(r\)</span> can be written as;</p>
<p><span class="math inline">\(Ar = \begin{pmatrix} 1/2 &amp; -1 \\ 0 &amp; 3/4 \end{pmatrix} \begin{bmatrix} 3 \\ 2 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(\prime{r} = \begin{bmatrix} -1/2 \\ 3/2 \end{bmatrix}\)</span></p>
<p><code>(1/2 * 3) + (-1 * 2) = -1/2</code></p>
<p><code>(0 * 3) + (3/4 * 2) = 3/2</code></p>
<p><strong>Q2</strong></p>
<p><span class="math inline">\(s = A \begin{bmatrix} -2 \\ 4 \end{bmatrix}\)</span></p>
<p><code>(1/2 * -2) + (-1 * 4) = -5</code></p>
<p><code>(0 * -2) + (3/4 * 4) = 3</code></p>
<p><strong>Q3</strong></p>
<p><span class="math inline">\(M = \begin{bmatrix} -1/2 &amp; 1/2 \\ 1/2 &amp; 1/2 \end{bmatrix}\)</span></p>
<p>Thoughts; - Everything gets smaller. - Top lefts inversion should flip the horizontal axis. - initially thought that because diagonals did not align it would be a parallelogram rather than a square, but not the case because it is a sheer and scale transformation - The axis have been rotated and flipped</p>
<p>Using unit vectors <span class="math inline">\(\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\)</span></p>
<p>top left -0.5 bottom left 0.5 top right 0.5 bottom right 0.5</p>
<p>Best corresponds to;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_transformation8.png" class="img-fluid">{:width=“200”}</p>
<p><strong>Q4</strong></p>
<p>Quick hack - anticlockwise rotation requires top right to be negative, all else positive.</p>
<p><strong>Q5</strong></p>
<p><span class="math inline">\(M = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 8 \end{bmatrix} \begin{bmatrix} 1 &amp; 0 \\ -1/2 &amp; 1 \end{bmatrix}\)</span></p>
<p><code>top left 1*1 + 0*-1/2 = 1</code></p>
<p><code>bottom left 0*1 + 8*-1/2 = -4</code></p>
<p><code>top right 1*0 + 0x1 = 0</code></p>
<p><code>bottom right 0x0 + 8x1 = 8</code></p>
</section>
<section id="matrix-inverses" class="level3">
<h3 class="anchored" data-anchor-id="matrix-inverses">Matrix Inverses</h3>
<p><strong>The apples and bananas problem</strong></p>
<p>I walked into a shop and bough 2 apples and 3 bananas for a price of £8. Another day I bought 10 apples and 1 banana for a price of £13.</p>
<p>This is a matrix (fruits <span class="math inline">\(A\)</span>) * a vector (prices <span class="math inline">\(r\)</span>) and produces an output vector (purchase <span class="math inline">\(s\)</span>).</p>
<p><span class="math inline">\(A\)</span> operates on <span class="math inline">\(r\)</span> and gives <span class="math inline">\(s\)</span></p>
<p><span class="math display">\[ \begin{pmatrix} 2 &amp; 3 \\ 10 &amp; 1 \end{pmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = \begin{bmatrix} 8 \\ 13 \end{bmatrix}\]</span></p>
<p><span class="math inline">\(A^{-1}\)</span>, the inverse of <span class="math inline">\(A\)</span> does exact opposite of what <span class="math inline">\(A\)</span> does.</p>
<p>By multiplying <span class="math inline">\(A\)</span> by <span class="math inline">\(A^{-1}\)</span>, we reverse whatever <span class="math inline">\(A\)</span> does and we get the identity matrix <span class="math inline">\(I\)</span>, the matrix that does nothing.</p>
<p><span class="math inline">\(Ar = s\)</span> becomes <span class="math inline">\(A^{-1} Ar = A^{-1} s\)</span></p>
<p><span class="math inline">\(A^{-1} A\)</span> is the identity matrix (<span class="math inline">\(A^{-1} A = I\)</span>), the matrix that does nothing, so you just have <span class="math inline">\(r\)</span></p>
<p><span class="math inline">\(r = A^{-1} s\)</span></p>
</section>
<section id="linear-algebra-to-solve-systems-of-equations" class="level3">
<h3 class="anchored" data-anchor-id="linear-algebra-to-solve-systems-of-equations">Linear algebra to solve systems of equations</h3>
<p>All of sudden we’ve moved away from planning with geometry and vector spaces and back to linear equations.</p>
<p>Systems of equations are a list of variables (things you don’t know) and a list of equations relating to them.</p>
<p>This is one of the core applications of linear algebra, it allows us to solve “systems of equations”.</p>
<p>The following system of equations;</p>
<p><span class="math display">\[4x_1 = 5x_2 = - 13\]</span> <span class="math display">\[-2x_1 + 3x_2 = 9\]</span></p>
<p>In matrix notation, we can write the system more compactly;</p>
<p><span class="math inline">\(Ax = b\)</span> with;</p>
<p><span class="math display">\[A = \begin{bmatrix} 4 &amp; -5 \\ -2 &amp; 3 \end{bmatrix}, b = \begin{bmatrix} -13 \\ 9 \end{bmatrix}\]</span></p>
<p>If the equations can be solved using linear algebra then (1) the variable in each equation is being scaled by some constant and (2) those variables are being added to each other in the equation. There are no exponents or multiplying variables together.</p>
<p>Arranging our systems of equations like this, sheds some geometric light on the problem;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/systemsofequations1.png" class="img-fluid">{:width=“400”}</p>
<p>We’re looking for a vector <span class="math inline">\(\bf{x}\)</span>, that after applying the transformation <span class="math inline">\(A\)</span>, lands on <span class="math inline">\(\bf{v}\)</span></p>
<p>If <span class="math inline">\(A\)</span> manipulated <span class="math inline">\(\bf{x}\)</span> such that it rotated 90 degrees counter-clockwise, then we need to find the matrix that reverses that transformation, i.e.&nbsp;a 90 degree clockwise rotation.</p>
<p>If <span class="math inline">\(A = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}, A^{-1} = \begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix}\)</span></p>
<p>If you first apply <span class="math inline">\(A\)</span>, then <span class="math inline">\(A^{-1}\)</span> you end up where you started.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/systemsofequations3.png" class="img-fluid">{:width=“200”}</p>
<p><span class="math inline">\(A^{-1}A\)</span> gets you back to where you started; multiplied out it gives you the identity matrix, the equivalent of multiplying by 1 in matrix multiplication.</p>
<p>Therefore;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/systemsofequations3.png" class="img-fluid">{:width=“400”}</p>
<p>If the determinant of <span class="math inline">\(A\)</span> is zero, it has manipulated space into a lower dimension, as a result we cannot use it’s inverse to understand what matrix was applied onto <span class="math inline">\(\bf{x}\)</span> to produce <span class="math inline">\(\bf{v}\)</span>.</p>
<p>The equation can be solved, but only with the vector <span class="math inline">\(\bf{v}\)</span> lives on the new dimension (in the case of 3D to 2D, the vector must live on that line).</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/systemsofequations4.png" class="img-fluid">{:width=“300”}</p>
<p>When the output of a transformation is a line, i.e.&nbsp;it’s one-dimensional, we say the transformation has a rank of 1. If all vectors land on a two-dimensional place, the transformation has a rank of 2, and so on.</p>
<p>The set of outputs for the transformation matrix is called the <em>column space</em>. The columns tell you where your vectors land, and the span of the columns gives you all possible outputs.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/systemsofequations5.png" class="img-fluid">{:width=“300”}</p>
<p>The rank is the number of dimensions in the column space. If a rank mantains it’s number of dimensions, then it is regarded as “<em>full rank</em>”.</p>
<p>When a matrix has full rank, only the point on the origin is the original origin. When a matrix loses dimensions, many points will now fall on the origin. Think of a line coming down to a single point. Lots of lines now sit at the origin. The set of vectors that land on the origin is called the “<em>null space</em>” or the “<em>kernel</em>” of your matrix. The space of all vectors that become null.</p>
</section>
<section id="solving-systems-of-equations" class="level3">
<h3 class="anchored" data-anchor-id="solving-systems-of-equations">Solving systems of equations</h3>
<p><strong>Elimination</strong></p>
<p>With a more complex example. We can simplify the problem by removing row one from the other two rows</p>
<p><span class="math display">\[ \begin{pmatrix} 1 &amp; 1 &amp; 3 \\ 1 &amp; 2 &amp; 4 \\ 1 &amp; 1 &amp; 2 \end{pmatrix} \begin{bmatrix} a \\ b \\ c \end{bmatrix} = \begin{bmatrix} 15 \\ 21 \\ 13 \end{bmatrix}\]</span></p>
<p>Removing row one gives us;</p>
<p><span class="math display">\[ \begin{pmatrix} 1 &amp; 1 &amp; 3 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; -1 \end{pmatrix} \begin{bmatrix} a \\ b \\ c \end{bmatrix} = \begin{bmatrix} 15 \\ 6 \\ -2 \end{bmatrix}\]</span></p>
<p>We’ve now solved <span class="math inline">\(c\)</span>, <span class="math inline">\(-c = -2\)</span>, <span class="math inline">\(c = 2\)</span></p>
<p>A matrix like this, where everything below the body diagonal is 0 is known as a triangular matrix.</p>
<p>The matrix has been reduced to what’s known as <code>Echelon form</code>. All the numbers below the leading diagonal is zero.</p>
<p>We can now use back substitution. Using the value of <span class="math inline">\(c\)</span> nad plugging back into the first two rows.</p>
<p>Removing <span class="math inline">\(c\)</span> from the rows we get;</p>
<p><span class="math display">\[ \begin{pmatrix} 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} \begin{bmatrix} a \\ b \\ c \end{bmatrix} = \begin{bmatrix} 9 \\ 4 \\ 2 \end{bmatrix}\]</span></p>
<p>Then getting to the final solution we remove <span class="math inline">\(b\)</span> from row 1;</p>
<p><span class="math display">\[ \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} \begin{bmatrix} a \\ b \\ c \end{bmatrix} = \begin{bmatrix} 5 \\ 4 \\ 2 \end{bmatrix}\]</span></p>
<p>However, we found <span class="math inline">\(r\)</span> for a specific output of <span class="math inline">\(s\)</span>, we didn’t compute the general. Calculating the inverse allows us to ensure we have the general case. We could find <span class="math inline">\(r\)</span> for any <span class="math inline">\(s\)</span>.</p>
<p>Elimination and back substitution are extremely computational efficient.</p>
<p><strong>Finding the inverse matrix</strong></p>
<p><span class="math inline">\(A B = I\)</span> where <span class="math inline">\(B\)</span> is the inverse of <span class="math inline">\(A\)</span>, <span class="math inline">\(B = A^{-1}\)</span>. You can apply the inverse to the right or the left, it’s commutative.</p>
<p><span class="math display">\[ A = \begin{pmatrix} 1 &amp; 1 &amp; 3 \\ 1 &amp; 2 &amp; 4 \\ 1 &amp; 1 &amp; 2 \end{pmatrix} B = \begin{pmatrix} b_{11} &amp; b_{12} &amp; b_{13} \\ b_{21} &amp; b_{22} &amp; b_{23} \\ b_{31} &amp; b_{32} &amp; b_{33} \end{pmatrix}\]</span></p>
<p>The identity matrix is <span class="math inline">\(I = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}\)</span></p>
<p>You could perform back substitution, by taking each column in the inverse matrix and use the output of the identity matrix.</p>
<p><span class="math display">\[ \begin{pmatrix} 1 &amp; 1 &amp; 3 \\ 1 &amp; 2 &amp; 4 \\ 1 &amp; 1 &amp; 2 \end{pmatrix} * \begin{pmatrix} b_{11}\\ b_{21}\\ b_{31} \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}\]</span></p>
<p>However, we can use elimination to do it more efficiently.</p>
<p>Subtracting the first row from rows two and three.</p>
<p><span class="math display">\[A* \begin{pmatrix} 1 &amp; 1 &amp; 3 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; -1 \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ -1 &amp; 1 &amp; 0 \\ -1 &amp; 0 &amp; 1 \end{pmatrix}\]</span></p>
<p>If we multiply the bottom row by -1 we can get the matrix in echelon form;</p>
<p><span class="math display">\[A* \begin{pmatrix} 1 &amp; 1 &amp; 3 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ -1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; -1 \end{pmatrix}\]</span></p>
<p>Then back substitute, make the third column 0 in both rows one and two.</p>
<p>Take the bottom row and subtract it from the second row and multiply by three and subtract from row one;</p>
<p><span class="math display">\[A* \begin{pmatrix} 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} = \begin{pmatrix} -2 &amp; 0 &amp; 3 \\ -2 &amp; 1 &amp; 1 \\ 1 &amp; 0 &amp; -1 \end{pmatrix}\]</span></p>
<p>Then back substitute b (the middle column) from the first row;</p>
<p><span class="math display">\[A* \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} = \begin{pmatrix} -2 &amp; -1 &amp; 2 \\ -2 &amp; 1 &amp; 1 \\ 1 &amp; 0 &amp; -1 \end{pmatrix}\]</span></p>
<p>This provides us with the inverse matrix.</p>
<p><span class="math display">\[ \begin{pmatrix} 1 &amp; 1 &amp; 3 \\ 1 &amp; 2 &amp; 4 \\ 1 &amp; 1 &amp; 2 \end{pmatrix} * \begin{pmatrix} -2 &amp; -1 &amp; 2 \\ -2 &amp; 1 &amp; 1 \\ 1 &amp; 0 &amp; -1 \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}\]</span></p>
<p>Computationally this approach is much easier, particularly when the dimensions increase. There are computationally faster methods to do a decomposition process.</p>
<p>Most software packages come with a solver function that we simply have to call on the matrix, and it will apply the most efficient process <code>inv(A)</code>.</p>
<p>The method above is a general method that we could write ourselves.</p>
</section>
<section id="practice-solving-linear-equations-using-the-inverse-matrix" class="level3">
<h3 class="anchored" data-anchor-id="practice-solving-linear-equations-using-the-inverse-matrix">Practice solving linear equations using the inverse matrix</h3>
<p><strong>Q1</strong></p>
<p>You go to the shops on Monday and buy 1 apple, 1 banana, and 1 carrot; the whole transaction totals €15. On Tuesday you buy 3 apples, 2 bananas, 1 carrot, all for €28. Then on Wednesday 2 apples, 1 banana, 2 carrots, for €23.</p>
<p><span class="math display">\[ A * \begin{bmatrix} a \\ b \\ c \end{bmatrix} =\begin{pmatrix} sMon \\ sTue \\ sWed \end{pmatrix}\]</span></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>],</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>]]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> [<span class="dv">15</span>, <span class="dv">28</span>, <span class="dv">23</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q2</strong></p>
<p>Given another system, <span class="math inline">\(Br = t\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/solvinglineq_inverse1.png" class="img-fluid">{:width=“450”}</p>
<p>We need to time the first row by three, then minus that from the second row. This will ensure the first value of the row is 0.</p>
<p>Then we need to multiply by minus 2 to ensure the second value of the row is 1.</p>
<p><span class="math inline">\(2{''} = [2{'} - (3*1{'})] * -2\)</span></p>
<p><strong>Q3</strong></p>
<p>From our previous question;</p>
<p><span class="math display">\[ \begin{pmatrix} 1 &amp; 3/2 &amp; 1/2 \\ 0 &amp; 1 &amp; 1 \\ 2 &amp; 8 &amp; 13 \end{pmatrix} * \begin{bmatrix} a\\ b\\ c\end{bmatrix} = \begin{pmatrix} 9/4 \\ -1/2 \\ 2 \end{pmatrix}\]</span></p>
<p>Fix row 3 to be a linear combination of the other two and provide a matrix in echelon form.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>row_3 <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">13</span>] <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiply row 1 by 2</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>, <span class="dv">3</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>] <span class="op">*</span> <span class="dv">2</span> <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="dv">9</span><span class="op">/</span><span class="dv">4</span> <span class="op">*</span> <span class="dv">2</span> <span class="op">=</span> <span class="fl">4.5</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Subtract from 3</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">13</span>] <span class="op">-</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>] <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">12</span>]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span> <span class="op">-</span> <span class="op">-</span><span class="fl">2.5</span> <span class="op">=</span> <span class="op">-</span><span class="fl">2.5</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiply row 2 by 5</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>] <span class="op">*</span> <span class="dv">5</span> <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">5</span>]</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> <span class="dv">5</span> <span class="op">=</span> <span class="op">-</span><span class="fl">2.5</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Subtract from 3'</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>[<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">12</span>] <span class="op">-</span> [<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">5</span>] <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">7</span>]</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="op">-</span><span class="fl">2.5</span> <span class="op">-</span> <span class="op">-</span><span class="fl">2.5</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Divide by 7</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">7</span>] <span class="op">/</span> <span class="dv">7</span> <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="dv">0</span><span class="op">/</span><span class="dv">7</span> <span class="op">=</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math display">\[ \begin{pmatrix} 1 &amp; 3/2 &amp; 1/2 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} * \begin{bmatrix} a\\ b\\ c\end{bmatrix} = \begin{pmatrix} 9/4 \\ -1/2 \\ 0 \end{pmatrix}\]</span></p>
<p>We can then compute a, b, c</p>
<p>b = -1/2 (ignore c as 0)</p>
<p>a = 9/4 - (-1/2 * 1.5) = 3 (multiply row 2 by 1.5 then remove it, ignore row 3 as 0)</p>
<p><span class="math inline">\(r = \begin{bmatrix} 3 \\ -1/2 \\ 0 \end{bmatrix}\)</span></p>
<p><strong>Q5/6</strong></p>
<p>Returning to Q1, convert the system to echelon form</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>],</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>]]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> [<span class="dv">15</span>, <span class="dv">28</span>, <span class="dv">23</span>]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Answer;</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Row 1 * 2</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>] , <span class="dv">30</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Sub from Row 3</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>[<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>] , <span class="op">-</span><span class="dv">7</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiply by -1</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>R3` <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>], <span class="dv">7</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Add R2 to R3`</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>[<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>], <span class="dv">35</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Row 1 * 3 and sub from R3``</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>[<span class="dv">0</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">2</span>], <span class="op">-</span><span class="dv">10</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiply by -0.5</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>R3` <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>], <span class="dv">5</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Row 1 * 3 and sub from R2</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>[<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>], <span class="op">-</span><span class="dv">17</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiply by -1</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>R2` <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>], <span class="dv">17</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> [[ <span class="dv">1</span> , <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>     [ <span class="dv">0</span>,  <span class="dv">1</span> , <span class="dv">2</span>],</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>     [ <span class="dv">0</span>,  <span class="dv">0</span> , <span class="dv">1</span>]]</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> [<span class="dv">15</span>, <span class="dv">17</span>, <span class="dv">5</span>]</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Price of individual elements;</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">5</span>]     </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q7</strong></p>
<p>Find the inverse of the matrix you used in Question 1</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Answer;</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>Ainv <span class="op">=</span> [[<span class="op">-</span><span class="fl">1.5</span>,  <span class="fl">0.5</span>,  <span class="fl">0.5</span>],</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>       [ <span class="fl">2.0</span>,  <span class="fl">0.0</span>, <span class="op">-</span><span class="fl">1.0</span>],</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>       [ <span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.5</span>,  <span class="fl">0.5</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q8</strong></p>
<p>In practice, for larger systems, one never solves a linear system by hand as there are software packages that can do this for you - such as numpy in Python.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>],</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>],</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>]]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>Ainv <span class="op">=</span> np.linalg.inv(A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In general, one shouldn’t calculate the inverse of a matrix unless absolutely necessary. It is more computationally efficient to solve the linear algebra system if that is all you need.</p>
<p>Numpy can also do this for you.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> [[<span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">2</span>],</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">1</span>],</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">13</span>]]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> [<span class="dv">9</span>, <span class="dv">7</span>, <span class="dv">2</span>]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.linalg.solve(A, s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="determinants" class="level3">
<h3 class="anchored" data-anchor-id="determinants">Determinants</h3>
<blockquote class="blockquote">
<p>The determinant of a linear transformation measures how much areas/volumes change during the transformation.</p>
</blockquote>
<p>The matrix scales space. Creating <span class="math inline">\(e'_1\)</span> and <span class="math inline">\(e'_2\)</span> from our original basis vectors.</p>
<p>The space has been expanded a factor of <span class="math inline">\(a\)</span> horizontally and a factor of <span class="math inline">\(d\)</span> vertically. The total space has been expanded by a factor <span class="math inline">\(ad\)</span></p>
<p>Everything in the space has grown by a factor <span class="math inline">\(ad\)</span></p>
<p>This is the determinant of the transformation matrix. <em>the determinant is how much we grow/shrink space</em>. More precisely, it is the factor by which a given area increases or decreases.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/determinants1.png" class="img-fluid">{:width=“400”}</p>
<p>A more concrete example,</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/determinants5.png" class="img-fluid">{:width=“400”}</p>
<p>However, these transformations are not always equal.</p>
<p>Adding <span class="math inline">\(b\)</span> into the matrix;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/determinants2.png" class="img-fluid">{:width=“400”}</p>
<p>The area, the determinant, is still the same <span class="math inline">\(ad\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/determinants6.png" class="img-fluid">{:width=“400”}</p>
<p>To get the area of a general matrix, such as;</p>
<p><span class="math display">\[ A = \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}\]</span></p>
<p>The maths gets a little complex; but is <span class="math inline">\(ad - bc\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/determinants3.png" class="img-fluid">{:width=“450”}</p>
<p>This is finding the determinant of A</p>
<p><span class="math inline">\(\lVert a \rVert = ad - bc\)</span></p>
<p>If a 2D matirx transforms the space such that all points fit onto a single line or even a single point, then the determinant will be 0.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/determinants7.png" class="img-fluid">{:width=“450”}</p>
<p>If a transformation has a determinant of 0, it is transforming the space into a smaller dimension. For a 2-dimenisonal space this is putting everything onto a single line, but the concept applies to <span class="math inline">\(n\)</span> dimensions.</p>
<p>When we “lose a dimensions”, the columns must be linearly dependent.</p>
<p><strong>Orientation</strong></p>
<p>You can scale an area by a negative amount. As we’ve seen with vector operations, this does not mean shrinking the area but rather reversing it’s orientation. “flipping space”. They “invert the orientation of space.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/determinants8.png" class="img-fluid">{:width=“450”}</p>
<p>In a 3D space, the determinant tells us how much volume gets scaled.</p>
</section>
<section id="the-determinant-and-inverse-matrices" class="level3">
<h3 class="anchored" data-anchor-id="the-determinant-and-inverse-matrices">The determinant and inverse matrices</h3>
<p>To get the inverse of a 2x2 matrix we flip the terms on the leading diagonal and take the minus of the off-diagonal terms</p>
<p><span class="math display">\[\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}\begin{pmatrix} d &amp; -b \\ -c &amp; a \end{pmatrix}\]</span></p>
<p>Multiply these out to get the determinant;</p>
<p><span class="math display">\[\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}\begin{pmatrix} d &amp; -b \\ -c &amp; a \end{pmatrix} = \begin{pmatrix} ad-bc &amp; 0 \\ 0 &amp; ad-bc \end{pmatrix}\]</span></p>
<p>Then multiplying by <span class="math inline">\(\frac{1}{ad-bc}\)</span> gives us the identity matrix</p>
<p><span class="math display">\[\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}\]</span></p>
<blockquote class="blockquote">
<p>The determinant here is what we need to divide the inverse matrix by in order for it to probably be an inverse.</p>
</blockquote>
<p>Knowing how to find the determinant in a general sense doesn’t add much to the learning process. Unlike the row echelon process we followed previously. Follow <code>QR decomposition</code> if interested.</p>
<p><strong>Linear independence</strong></p>
<p>Transformations can also remove linear independence.</p>
<p><span class="math inline">\(A = \begin{pmatrix} 1 &amp; 2 \\ 1 &amp; 2 \end{pmatrix}\)</span> will transform our identity matrix to a single line</p>
<p><span class="math inline">\(e'_1\)</span> and <span class="math inline">\(e'_2\)</span> become multiples of one another.</p>
<p>The matrix is transforming every point in space along a line. Therefore the determinant is going to be 0.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/determinants4.png" class="img-fluid">{:width=“450”}</p>
<p>If we’re working in 3D space, a 3x3 matrix and one of the new basis vectors is just a linear multiple of the other two (ie not linearly independent), the new space is a plane.</p>
<blockquote class="blockquote">
<p>a plane is a flat, two-dimensional surface that extends infinitely far. A plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension)</p>
</blockquote>
<p>Again, as turning a 2D shape into a single dimensional line, the determinant, would be zero.</p>
<p>For example,</p>
<p>In our matirx - row 3 = row1 + row2 - column 3 = 2*column1 + column2</p>
<p><span class="math display">\[\begin{pmatrix} 1 &amp; 1 &amp; 3 \\ 1 &amp; 2 &amp; 4 \\ 2 &amp; 3 &amp; 7 \end{pmatrix} \begin{bmatrix} a\\b\\c\end{bmatrix} = \begin{pmatrix} 12\\17\\29\end{pmatrix}\]</span></p>
<p>This matrix doesn’t describe three independent basis vectors. It doesn’t describe any 3D space but collapses into a 2D space.</p>
<p>As a result, we cannot reduce this to row echelon form.</p>
<p>If you take away the first row from the second, and then the first and second row from the third row, you end up with;</p>
<p><span class="math display">\[\begin{pmatrix} 1 &amp; 1 &amp; 3 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} \begin{bmatrix} a\\b\\c\end{bmatrix} = \begin{pmatrix} 12\\5\\0\end{pmatrix}\]</span></p>
<p><span class="math inline">\(0c = 0\)</span> isn’t very helpful</p>
<p>There are essentially an infinite number of solutions for <span class="math inline">\(c\)</span>, any value of <span class="math inline">\(c\)</span> would work.</p>
<p>As a result, we cannot solve this.</p>
<p>If you think of this from a simultaneous equation point of view, in trying to solve “apples, bananas, and carrots”. It’s equivalent to the third time going to the copy and ordering a copy of the first two orders. You gathered no new information.</p>
<blockquote class="blockquote">
<p>where the basis vectors describing the matrix are not linearly independent, then the determinant is zero, and that means I can’t solve the system of simultaneous equations. Which means I can’t invert the matrix because I can’t take one over the determinant either. That means I’m stuck, this matrix has no inverse</p>
</blockquote>
<p>Doing a transformation that collapses the number of dimensions in space comes at a cost.</p>
<p>The inverse of a matrix, allows us to un-do our transformation. You cannot un-do, moving from a 3D space to a 2D plane.</p>
</section>
<section id="python-and-matrices" class="level3">
<h3 class="anchored" data-anchor-id="python-and-matrices">Python and Matrices</h3>
<p>Write a function that will test if a 4×4 matrix is singular, i.e.&nbsp;to determine if an inverse exists, before calculating it.</p>
<p>You shall use the method of converting a matrix to echelon form, and testing if this fails by leaving zeros that can’t be removed on the leading diagonal.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> isSingular(A) :</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    B <span class="op">=</span> np.array(A, dtype<span class="op">=</span>np.float_) </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Make B as a copy of A. We're going to alter it's values"""</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        fixRowZero(B)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        fixRowOne(B)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        fixRowTwo(B)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        fixRowThree(B)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> MatrixIsSingular:</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">"defines our error flag"</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MatrixIsSingular(<span class="pp">Exception</span>): <span class="cf">pass</span>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co">For Row Zero, all we require is the first element is equal to 1.</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">We'll divide the row by the value of A[0, 0].</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">This will get us in trouble though if A[0, 0] equals 0</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">    , so first we'll test for that,</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">If this is true, we'll add one of the lower rows to the first one before the division.</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">We'll repeat the test going down each lower row until we can do the division.</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">There is no need to edit this function.</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fixRowZero(A) :</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> A[<span class="dv">0</span>,<span class="dv">0</span>] <span class="op">==</span> <span class="dv">0</span> :</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        A[<span class="dv">0</span>] <span class="op">=</span> A[<span class="dv">0</span>] <span class="op">+</span> A[<span class="dv">1</span>]</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> A[<span class="dv">0</span>,<span class="dv">0</span>] <span class="op">==</span> <span class="dv">0</span> :</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        A[<span class="dv">0</span>] <span class="op">=</span> A[<span class="dv">0</span>] <span class="op">+</span> A[<span class="dv">2</span>]</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> A[<span class="dv">0</span>,<span class="dv">0</span>] <span class="op">==</span> <span class="dv">0</span> :</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        A[<span class="dv">0</span>] <span class="op">=</span> A[<span class="dv">0</span>] <span class="op">+</span> A[<span class="dv">3</span>]</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> A[<span class="dv">0</span>,<span class="dv">0</span>] <span class="op">==</span> <span class="dv">0</span> :</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> MatrixIsSingular()</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    A[<span class="dv">0</span>] <span class="op">=</span> A[<span class="dv">0</span>] <span class="op">/</span> A[<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""First we'll set the sub-diagonal elements to zero, i.e. A[1,0].</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co">Next we want the diagonal element to be equal to one.</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">We'll divide the row by the value of A[1, 1].</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">Again, we need to test if this is zero.</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">If so, we'll add a lower row and repeat setting the sub-diagonal elements to zero.</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">There is no need to edit this function."""</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fixRowOne(A) :</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    A[<span class="dv">1</span>] <span class="op">=</span> A[<span class="dv">1</span>] <span class="op">-</span> A[<span class="dv">1</span>,<span class="dv">0</span>] <span class="op">*</span> A[<span class="dv">0</span>]</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> A[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">==</span> <span class="dv">0</span> :</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        A[<span class="dv">1</span>] <span class="op">=</span> A[<span class="dv">1</span>] <span class="op">+</span> A[<span class="dv">2</span>]</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        A[<span class="dv">1</span>] <span class="op">=</span> A[<span class="dv">1</span>] <span class="op">-</span> A[<span class="dv">1</span>,<span class="dv">0</span>] <span class="op">*</span> A[<span class="dv">0</span>]</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> A[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">==</span> <span class="dv">0</span> :</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        A[<span class="dv">1</span>] <span class="op">=</span> A[<span class="dv">1</span>] <span class="op">+</span> A[<span class="dv">3</span>]</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        A[<span class="dv">1</span>] <span class="op">=</span> A[<span class="dv">1</span>] <span class="op">-</span> A[<span class="dv">1</span>,<span class="dv">0</span>] <span class="op">*</span> A[<span class="dv">0</span>]</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> A[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">==</span> <span class="dv">0</span> :</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> MatrixIsSingular()</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    A[<span class="dv">1</span>] <span class="op">=</span> A[<span class="dv">1</span>] <span class="op">/</span> A[<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fixRowTwo(A) :</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Insert code below to set the sub-diagonal elements of row two to zero (there are two of them)."""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    A[<span class="dv">2</span>] <span class="op">=</span> A[<span class="dv">2</span>] <span class="op">-</span> A[<span class="dv">2</span>,<span class="dv">0</span>] <span class="op">*</span> A[<span class="dv">0</span>]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    A[<span class="dv">2</span>] <span class="op">=</span> A[<span class="dv">2</span>] <span class="op">-</span> A[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">*</span> A[<span class="dv">1</span>]</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Next we'll test that the diagonal element is not zero."</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> A[<span class="dv">2</span>,<span class="dv">2</span>] <span class="op">==</span> <span class="dv">0</span> :</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Insert code below that adds a lower row to row 2."</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        A[<span class="dv">2</span>] <span class="op">=</span> A[<span class="dv">2</span>] <span class="op">+</span> A[<span class="dv">3</span>]</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Now repeat your code which sets the sub-diagonal elements to zero."</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        A[<span class="dv">2</span>] <span class="op">=</span> A[<span class="dv">2</span>] <span class="op">-</span> A[<span class="dv">2</span>,<span class="dv">0</span>] <span class="op">*</span> A[<span class="dv">0</span>]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        A[<span class="dv">2</span>] <span class="op">=</span> A[<span class="dv">2</span>] <span class="op">-</span> A[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">*</span> A[<span class="dv">1</span>]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> A[<span class="dv">2</span>,<span class="dv">2</span>] <span class="op">==</span> <span class="dv">0</span> :</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> MatrixIsSingular()</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Finally set the diagonal element to one by dividing the whole row by that element."</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    A[<span class="dv">2</span>] <span class="op">=</span> A[<span class="dv">2</span>] <span class="op">/</span> A[<span class="dv">2</span>,<span class="dv">2</span>]</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fixRowThree(A) :</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Insert code below to set the sub-diagonal elements of row three to zero."""</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    A[<span class="dv">3</span>] <span class="op">=</span> A[<span class="dv">3</span>] <span class="op">-</span> A[<span class="dv">3</span>,<span class="dv">0</span>] <span class="op">*</span> A[<span class="dv">0</span>]</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    A[<span class="dv">3</span>] <span class="op">=</span> A[<span class="dv">3</span>] <span class="op">-</span> A[<span class="dv">3</span>,<span class="dv">1</span>] <span class="op">*</span> A[<span class="dv">1</span>]</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    A[<span class="dv">3</span>] <span class="op">=</span> A[<span class="dv">3</span>] <span class="op">-</span> A[<span class="dv">3</span>,<span class="dv">2</span>] <span class="op">*</span> A[<span class="dv">2</span>]</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Complete the if statement to test if the diagonal element is zero."</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> A[<span class="dv">3</span>,<span class="dv">3</span>] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> MatrixIsSingular()</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Transform the row to set the diagonal element to one."</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    A[<span class="dv">3</span>] <span class="op">=</span> A[<span class="dv">3</span>] <span class="op">/</span> A[<span class="dv">3</span>,<span class="dv">3</span>]</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="matrices-make-linear-mappings" class="level2">
<h2 class="anchored" data-anchor-id="matrices-make-linear-mappings">Matrices make linear mappings</h2>
<section id="einstein-summation-convention-and-the-symmetry-of-the-dot-product" class="level3">
<h3 class="anchored" data-anchor-id="einstein-summation-convention-and-the-symmetry-of-the-dot-product">Einstein summation convention and the symmetry of the dot product</h3>
<p>The Einstein summation convention is a way to write matrix transformations. It writes down what the actual operations are on the elements of the matrix.</p>
<p>When we started, we said that multiplying a matrix by a vector or with another matrix is a process of taking every element in each row in turn, multiplied with corresponding element in each column in the other matrix, and adding them all up and putting them in place.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/einsteinconvention1.png" class="img-fluid">{:width=“400”}</p>
<p>Einstein convention,says, you have a sum over some elements <span class="math inline">\(j\)</span>, for all the possible combinations of <span class="math inline">\(i\)</span> and <span class="math inline">\(k\)</span>. As this is a repeated index, don’t bother with the sum;</p>
<p><span class="math display">\[ ab_{ik} = \sum_{j} a_{ij} b_{jk} = a_{ij} b_{jk}\]</span></p>
<p>We’d only have to run <code>for loops</code> over <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span>. Then use an accumulator on the <span class="math inline">\(j\)</span>’s to find the elements of the product matrix <span class="math inline">\(AB\)</span>.</p>
<p>We can multiply matrices that are not square (ie same numbers of rows in A as the same number of columns in B)</p>
<p>As long as you have the same number of <span class="math inline">\(j\)</span>s you can multiply them together.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/einsteinconvention2.png" class="img-fluid">{:width=“400”}</p>
<blockquote class="blockquote">
<p>Now, all sorts of matrix properties that you might want, inverses and so on, determinants, all start to get messy and mucky, and you sometimes can’t even compute them when you’re doing this sort of thing.</p>
</blockquote>
<p><strong>Revisiting the dot product</strong></p>
<p>If we have two column vectors (a single column with n elements);</p>
<p><span class="math inline">\(U = \begin{bmatrix} u_i \end{bmatrix}\)</span> and <span class="math inline">\(V = \begin{bmatrix} v_i \end{bmatrix}\)</span></p>
<p>The summation convention is <span class="math inline">\(u_i v_i\)</span>, we repeat over all the <span class="math inline">\(i\)</span>s and add.</p>
<p>That’s the same as doing a matrix transformation, where <span class="math inline">\(u\)</span> is a row and <span class="math inline">\(v\)</span> a column.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/einsteinconvention3.png" class="img-fluid">{:width=“400”}</p>
<p>Projection is symmetric. Projection is the dot product.</p>
<p>If you project <span class="math inline">\(\hat{u}\)</span> down onto <span class="math inline">\(\hat{e_1}\)</span> and vice versa, the two triangles split by the pink line are identical.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/einsteinconvention4.png" class="img-fluid">{:width=“400”}</p>
<blockquote class="blockquote">
<p>there is this connection between this numerical thing, matrix multiplication, and this geometric thing, projection</p>
</blockquote>
<p>That’s why we talk about a matrix multiplication with a vector as being the projection of that vector onto the vectors composing the matrix, the columns of the matrix</p>
</section>
<section id="non-square-matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="non-square-matrix-multiplication">Non-square matrix multiplication</h3>
<p>In traditional notion we might write <span class="math inline">\(\sum^3 A_{ij} v_j = A_{i1} v_1 + A_{i2} v_2 + A_{i3} v_3\)</span></p>
<p>With Einstein summation convention we can simply write <span class="math inline">\(A_{ij}v{j}\)</span> and know that we sum over <span class="math inline">\(j\)</span> because it appears twice.</p>
<p>We can multiply any matrices together as long as the terms which we sum over have the same number of elements - need to clarify</p>
<p>We can multiply an <span class="math inline">\(m \times n\)</span> matrix with an <span class="math inline">\(n \times k\)</span> matrix, and the resultant matrix will be an <span class="math inline">\(m \times k\)</span> matrix.</p>
<p><span class="math display">\[A=\begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 0 &amp; 1\end{bmatrix} B=\begin{bmatrix} 1 &amp; 1 &amp; 0\\0 &amp; 1 &amp; 1\\1 &amp; 0 &amp; 1\end{bmatrix}\]</span></p>
<p><span class="math display">\[C_{mn} = A_{mj} B{jn}\]</span></p>
<p><span class="math display">\[C_{21} = A_{2j} B_{j1} = 5\]</span></p>
<p>Equivalent to <span class="math inline">\(C_{21} = A_{2j} B_{j1} = A_{21} B_{11} + A_{22} B_{21} + A_{23} B_{31}\)</span></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">4</span><span class="op">*</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">0</span><span class="op">*</span><span class="dv">0</span>) <span class="op">+</span> (<span class="dv">1</span><span class="op">*</span><span class="dv">1</span>) <span class="op">=</span> <span class="dv">5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>elements in second in A with elements in first column in B</p>
<p><span class="math inline">\(C_{11}\)</span> for example is;</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="op">*</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">2</span><span class="op">*</span><span class="dv">0</span>) <span class="op">+</span> (<span class="dv">3</span><span class="op">*</span><span class="dv">1</span>) <span class="op">=</span> <span class="dv">4</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math display">\[C=\begin{bmatrix} 4 &amp; 3 &amp; 5 \\ 5 &amp; 4 &amp; 1\end{bmatrix}\]</span></p>
<p><strong>Calculate the product</strong></p>
<p><code>Question</code></p>
<p><span class="math display">\[\begin{bmatrix} 2 &amp; 4 &amp; 5 &amp; 6\end{bmatrix} \begin{bmatrix} 1 \\ 3 \\ 2 \\ 1\end{bmatrix}\]</span></p>
<p><code>Answer 30</code></p>
<p><code>Question</code></p>
<p><span class="math display">\[\begin{bmatrix} 1 \\ 3 \\ 2 \\ 1\end{bmatrix} \begin{bmatrix} 2 &amp; 4 &amp; 5 &amp; 6\end{bmatrix}\]</span></p>
<p><code>Answer, produces a 4x4</code></p>
<p><code>Question</code></p>
<p><span class="math display">\[\begin{bmatrix} 2 &amp; 4 &amp; 5 &amp; 6 \\6 &amp; 12 &amp; 15 &amp; 18\\4 &amp; 8 &amp; 10 &amp; 12\\2 &amp; 4 &amp; 5 &amp; 6\end{bmatrix}\]</span></p>
<pre><code>- the row is repeated each time
- multiplied by the relevant element in the first vector</code></pre>
<p><code>Question</code></p>
<p><span class="math display">\[\begin{bmatrix} 2 &amp; -1 \\ 0 &amp; 3 \\ 1 &amp; 0\end{bmatrix} \begin{bmatrix} 0 &amp; 1 &amp; 4 &amp; -1 \\ -2 &amp; 0 &amp; 0 &amp; 2\end{bmatrix}\]</span></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Answer</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Column1;</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>(<span class="dv">2</span><span class="op">*</span><span class="dv">0</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span><span class="op">*-</span><span class="dv">2</span>) <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span><span class="op">*</span><span class="dv">0</span>) <span class="op">+</span> (<span class="dv">3</span><span class="op">*-</span><span class="dv">2</span>) <span class="op">=</span> <span class="op">-</span><span class="dv">6</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="op">*</span><span class="dv">0</span>) <span class="op">+</span> (<span class="dv">0</span><span class="op">*-</span><span class="dv">2</span>) <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Column2;</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>(<span class="dv">2</span><span class="op">*</span><span class="dv">1</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span><span class="op">*</span><span class="dv">0</span>) <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span><span class="op">*</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">3</span><span class="op">*</span><span class="dv">0</span>) <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="op">*</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">0</span><span class="op">*</span><span class="dv">0</span>) <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Column3;</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>(<span class="dv">2</span><span class="op">*</span><span class="dv">4</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span><span class="op">*</span><span class="dv">0</span>) <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span><span class="op">*</span><span class="dv">4</span>) <span class="op">+</span> (<span class="dv">3</span><span class="op">*</span><span class="dv">0</span>) <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="op">*</span><span class="dv">4</span>) <span class="op">+</span> (<span class="dv">0</span><span class="op">*</span><span class="dv">0</span>) <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Column3;</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>(<span class="dv">2</span><span class="op">*-</span><span class="dv">1</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span><span class="op">*</span><span class="dv">2</span>) <span class="op">=</span> <span class="op">-</span><span class="dv">4</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span><span class="op">*-</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">3</span><span class="op">*</span><span class="dv">2</span>) <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="op">*-</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">0</span><span class="op">*</span><span class="dv">2</span>) <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math display">\[\begin{bmatrix} 2 &amp; 2 &amp; 8 &amp; -4 \\ -6 &amp; 0 &amp; 0 &amp; 6 \\ 0 &amp; 1 &amp; 4 &amp; -1\end{bmatrix}\]</span></p>
<p><code>Question</code></p>
<p><span class="math inline">\(D = ABC\)</span> where - A is a 5<em>3 matrix - B is a 3</em>7 matrix - C is a 7*4 matrix</p>
<p>What are the dimensions of <span class="math inline">\(D\)</span>?</p>
<p>The size of <span class="math inline">\(AB\)</span> is 5*7</p>
<p>The size of <span class="math inline">\((AB)C\)</span> == <span class="math inline">\(A(BC)\)</span> therefore the matrices can be multiplied together.</p>
<p><span class="math inline">\(D\)</span> is a 5*4 matrix</p>
<p><code>Question</code></p>
<p>Calculate the product;</p>
<p><span class="math display">\[\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{bmatrix} \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6\end{bmatrix}\]</span></p>
<p>The identity matrix doesn’t change the second matrix, so remains <span class="math inline">\(\begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6\end{bmatrix}\)</span></p>
<p>Let <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(\mathbf{v}\)</span> be vectors with <span class="math inline">\(n\)</span> elements. Write the dot product;</p>
<p>–TODO</p>
</section>
<section id="using-non-square-matrices-to-do-a-projection" class="level3">
<h3 class="anchored" data-anchor-id="using-non-square-matrices-to-do-a-projection">Using non-square matrices to do a projection</h3>
<p>Shadows are an example of a transformation that reduces the number of dimensions. For example, 3D objects in the world cast shadows on surfaces that are 2D.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/nonsquareprojections1.png" class="img-fluid">{:width=“400”}</p>
<p>The sun is sufficiently far away that effectively all of its rays come in parallel to each other. We can describe their direction with the unit vector <span class="math inline">\(\hat{\mathbf{s}}\)</span></p>
<p>We can describe the 3D coordinates of points on objects in our space with the vector <span class="math inline">\(\mathbf{r}\)</span>. Objects will cast a shadow on the ground at the point <span class="math inline">\(\mathbf{r}'\)</span> along the path that light would have taken if it hadn’t been blocked at <span class="math inline">\(\mathbf{r}\)</span>, that is, <span class="math inline">\(\mathbf{r}' = \mathbf{r} + \lambda \hat{\mathbf{s}}\)</span></p>
<p>The ground is at <span class="math inline">\(\mathbf{r}'_3 = 0\)</span>; by using <span class="math inline">\(\mathbf{r}'.\hat{\mathbf{e}}_3 = 0\)</span>, we can derive the expression, <span class="math inline">\(\mathbf{r}.\hat{\mathbf{e}}_3 + \lambda s_3 = 0\)</span>, (where <span class="math inline">\(s_3 = \hat{\mathbf{s}}.\hat{\mathbf{e}}_3\)</span>).</p>
<p>Rearrange this expression for <span class="math inline">\(\lambda\)</span> and substitute it back into the expression for <span class="math inline">\(\mathbf{r}'\)</span>, in order to get <span class="math inline">\(\mathbf{r}'\)</span> in terms of <span class="math inline">\(\mathbf{r}\)</span>.</p>
<p><span class="math display">\[ \mathbf{r}' = r - \hat{s}(r.\hat{e_3})/s_3\]</span></p>
<p>Remember that; The scalar projection of <span class="math inline">\(s\)</span> on <span class="math inline">\(r\)</span>;</p>
<p><span class="math display">\[ proj_rs = \frac{s.r}{\lVert r \rVert}\]</span></p>
<p><span class="math inline">\(\mathbf{r}'\)</span> can be written as a linear transformation of <span class="math inline">\(r\)</span>. This means we should be able to write <span class="math inline">\(\mathbf{r}' = Ar\)</span> for some matrix <span class="math inline">\(A\)</span></p>
<p>Usiing Einstein summation convention, we can rewrite our previous answer as; <span class="math display">\[r'_i = r_i - s_i[\hat{e_3}]_jr_j/s_3\]</span> or <span class="math display">\[r'_i = r_i - s_ir_3/s_3\]</span> or <span class="math display">\[r'_i = (I_{ij} - s_i[\hat{e_3}]_j/s_3)r_j\]</span> or <span class="math display">\[r'_i = (I_{ij} - s_iI_{3j}/s_3)r_j\]</span></p>
<p>We can now give an expression for <span class="math inline">\(A\)</span> in its component form by evaluating the components A_{ij} for each row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span>.</p>
<p><span class="math inline">\(A\)</span> takes a 3D vector <span class="math inline">\(r\)</span> and transforms it into a 2D vector <span class="math inline">\(r'\)</span>. Therefore the matrix will be a 2x3. - remember; the columns of a matrix are the vectors in the new space that the unit vectors of the old space transform to</p>
<p><span class="math display">\[ A = \begin{bmatrix} 1 &amp; 0 &amp; -s_1/s_3\\ 0 &amp; 1 &amp; -s_2/s_3\end{bmatrix}\]</span></p>
<p>If you were to evaluate it’s third row it would be all zeros as <span class="math inline">\(r'\)</span> has no value in the third dimension; <span class="math inline">\(A3 = [0, 0, 0]\)</span></p>
<p>Assume the Sun’s rays come in the direction; <span class="math display">\[ \hat{s} = \begin{bmatrix} 4/13 \\ -3/13 \\ -12/13 \end{bmatrix}\]</span></p>
<p>Construct the matrix A, apply it to a point, on an object in our space to find the coordinate of that point’s shadow;</p>
<p><span class="math inline">\(r = \begin{bmatrix} 6 \\ 2 \\ 3 \end{bmatrix}\)</span>, <span class="math inline">\(A = \begin{bmatrix} 1 &amp; 0 &amp; -4/13 / -12/13\\ 0 &amp; 1 &amp; 3/13 / -12/13\end{bmatrix}\)</span></p>
<p><span class="math inline">\(r' = A_{ij}r{j}\)</span></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># r'</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span> <span class="op">*</span> <span class="dv">6</span>) <span class="op">+</span> (<span class="dv">0</span> <span class="op">*</span> <span class="dv">2</span>) <span class="op">+</span> (<span class="fl">0.333</span> <span class="op">*</span> <span class="dv">3</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span> <span class="op">*</span> <span class="dv">6</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">*</span> <span class="dv">2</span>) <span class="op">+</span> (<span class="op">-</span><span class="fl">0.25</span> <span class="op">*</span> <span class="dv">3</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>[<span class="dv">7</span>, <span class="fl">1.25</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Another use of non-square matrices is applying a matrix to a list of vectors.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/nonsquare1.png" class="img-fluid">{:width=“600”}</p>
<p>Observe that it’s the same result as treating the columns as separate vectors and calculating them individually.</p>
<p>Using <span class="math inline">\(\hat{s} = \begin{bmatrix} 4/13 \\ -3/13 \\ -12/13 \end{bmatrix}\)</span>, apply A to the matrix;</p>
<p><span class="math inline">\(R = \begin{bmatrix} 5&amp;-1&amp;-3&amp;-7 \\ 4&amp;-4&amp;1&amp;-2 \\ 9&amp;3&amp;0&amp;12\end{bmatrix}\)</span></p>
<p><span class="math inline">\(A = \begin{bmatrix} 1 &amp; 0 &amp; 0.333\\ 0 &amp; 1 &amp; 0.25\end{bmatrix}\)</span></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># R`</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>Rp <span class="op">=</span> [[a,  b,  c,  d],</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>      [e,  f,  g,  h]]</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> (<span class="dv">1</span> <span class="op">*</span> <span class="dv">5</span>) <span class="op">+</span> (<span class="dv">0</span> <span class="op">*</span> <span class="dv">4</span>) <span class="op">+</span> ((<span class="op">-</span><span class="dv">4</span><span class="op">/</span><span class="dv">13</span>)<span class="op">/</span>(<span class="op">-</span><span class="dv">12</span><span class="op">/</span><span class="dv">13</span>) <span class="op">*</span> <span class="dv">9</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> (<span class="dv">0</span> <span class="op">*</span> <span class="dv">5</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">*</span> <span class="dv">4</span>) <span class="op">+</span> (<span class="op">-</span><span class="fl">0.25</span> <span class="op">*</span> <span class="dv">9</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> (<span class="dv">1</span> <span class="op">*</span> <span class="op">-</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">0</span> <span class="op">*</span> <span class="op">-</span><span class="dv">4</span>) <span class="op">+</span> ((<span class="op">-</span><span class="dv">4</span><span class="op">/</span><span class="dv">13</span>)<span class="op">/</span>(<span class="op">-</span><span class="dv">12</span><span class="op">/</span><span class="dv">13</span>) <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> (<span class="dv">0</span> <span class="op">*</span> <span class="op">-</span><span class="dv">1</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">*</span> <span class="op">-</span><span class="dv">4</span>) <span class="op">+</span> (<span class="op">-</span><span class="fl">0.25</span> <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> (<span class="dv">1</span> <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>) <span class="op">+</span> (<span class="dv">0</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> ((<span class="op">-</span><span class="dv">4</span><span class="op">/</span><span class="dv">13</span>)<span class="op">/</span>(<span class="op">-</span><span class="dv">12</span><span class="op">/</span><span class="dv">13</span>) <span class="op">*</span> <span class="dv">0</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> (<span class="dv">0</span> <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="op">-</span><span class="fl">0.25</span> <span class="op">*</span> <span class="dv">0</span>)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> (<span class="dv">1</span> <span class="op">*</span> <span class="dv">7</span>) <span class="op">+</span> (<span class="dv">0</span> <span class="op">*</span> <span class="op">-</span><span class="dv">2</span>) <span class="op">+</span> ((<span class="op">-</span><span class="dv">4</span><span class="op">/</span><span class="dv">13</span>)<span class="op">/</span>(<span class="op">-</span><span class="dv">12</span><span class="op">/</span><span class="dv">13</span>) <span class="op">*</span> <span class="dv">12</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> (<span class="dv">0</span> <span class="op">*</span> <span class="dv">7</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">*</span> <span class="op">-</span><span class="dv">2</span>) <span class="op">+</span> (<span class="op">-</span><span class="fl">0.25</span> <span class="op">*</span> <span class="dv">12</span>)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>Rp <span class="op">=</span> [[<span class="fl">8.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="op">-</span><span class="fl">3.0</span>, <span class="fl">11.0</span>],</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>      [<span class="fl">1.75</span>, <span class="op">-</span><span class="fl">3.25</span>, <span class="fl">1.0</span>, <span class="op">-</span><span class="fl">5.0</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="matrices-transform-into-the-new-basis-vector-set" class="level2">
<h2 class="anchored" data-anchor-id="matrices-transform-into-the-new-basis-vector-set">Matrices transform into the new basis vector set</h2>
<p>The columns of a transformation matrix, are the axes of the new basis vectors of the mapping in my coordinate system.</p>
<p>The lines in yellow describe the world of Panda Bear. To him, these vectors are [1,0] and [0,1] but in my frame, they are [3,1] and [1,1]</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_basis1.png" class="img-fluid">{:width=“500”}</p>
<p>Bear’s transformation matrix is therefore; <span class="math inline">\(\begin{bmatrix} 3&amp;1 \\ 1&amp;1 \end{bmatrix}\)</span></p>
<p>Now if we take a vector in Bear’s world, we can understand it in my coordinate system.</p>
<p>The transformation matrix is Bear’s basis vectors in my coordinate system.</p>
<p>The vector <code>1/2[3, 1]</code> or <code>[3/2, 1/2]</code> in Bear’s world becomes;</p>
<p><code>[(3 * 3/2) + (1 * 1/2), (1 * 3/2) + (1 * 1/2)]</code> or <code>[5, 2]</code></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_basis2.png" class="img-fluid">{:width=“500”}</p>
<p>However, we need to figure out how to go the other way. Translating my world to Bear’s world.</p>
<p>To get my basis vectors in Bear’s coordinates, we need to take the inverse of the transformation matrix; - flip on the leading diagonal, and put a minus on the off diagonal terms - Recall that when a matrix is transformed into its diagonal form, the entries along the diagonal are the eigenvalues of the matrix - this can save lots of calculation!</p>
<ul>
<li><p><span class="math inline">\(\begin{bmatrix} 1&amp;-1 \\ -1&amp;3 \end{bmatrix}\)</span></p></li>
<li><p>then divide by the determinant (three minus one over one, 3-1/1 = 2), so multiply by a half</p></li>
<li><p><span class="math inline">\(\frac{1}{2} \begin{bmatrix} 1&amp;-1 \\ -1&amp;3 \end{bmatrix}\)</span></p></li>
<li><p>remember - <em>the determinant is how much we grow/shrink space</em></p></li>
</ul>
<p><strong>Example</strong></p>
<p>Here Bear’s world is going to be an orthonormal basis vector set (they form a v)</p>
<p><em>you can to a dot product to verfiy they are at 90 degrees to one another (orthogonal)</em></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_basis3.png" class="img-fluid">{:width=“400”}</p>
<p>Bear’s transformation matrix, ie the matrix that converts a vector in Bear’s world to my coordinate system;</p>
<p><span class="math inline">\(B = \frac{1}{\sqrt2} \begin{bmatrix} 1&amp;-1 \\ 1&amp;1 \end{bmatrix}\)</span></p>
<p>The inverse, or <span class="math inline">\(B^{-1}\)</span></p>
<p><span class="math inline">\(B = \frac{1}{\sqrt2} \begin{bmatrix} 1&amp;1 \\ -1&amp;1 \end{bmatrix}\)</span></p>
<p>Because Bear’s vectors are orthogonal, we can do this using projections, rather than having to calculate the transformation matrices.</p>
<p>Take my version of the vector and dot it with Bear’s axis, then we get the answer of the vector in Bear’s world;</p>
<p>first component, the vector * the first axis; <span class="math display">\[\frac{1}{\sqrt2} \begin{bmatrix} 1 \\ 3 \end{bmatrix} . \frac{1}{\sqrt2} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{2} 4 = 2\]</span></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">/</span>sqrt{<span class="dv">2</span>} <span class="op">*</span> <span class="dv">1</span><span class="op">/</span>sqrt{<span class="dv">2</span>} <span class="op">=</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>(<span class="dv">3</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">=</span> <span class="dv">4</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>second component, the vector * the second axis; <span class="math display">\[\frac{1}{\sqrt2} \begin{bmatrix} 1 \\ 3 \end{bmatrix} . \frac{1}{\sqrt2} \begin{bmatrix} -1 \\ 1 \end{bmatrix} = 1\]</span></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">/</span>sqrt{<span class="dv">2</span>} <span class="op">*</span> <span class="dv">1</span><span class="op">/</span>sqrt{<span class="dv">2</span>} <span class="op">=</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>(<span class="dv">3</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">*</span> <span class="op">-</span><span class="dv">1</span>) <span class="op">=</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In this case the lengths are one. When using projections normally we would need to normalise by their lengths (see scalar projections).</p>
<p>If Bear’s vectors are orthogonal to one another, we don’t have to use the matrix transformations, we can use the dot product.</p>
<section id="doing-a-transformation-in-a-changed-basis" class="level3">
<h3 class="anchored" data-anchor-id="doing-a-transformation-in-a-changed-basis">Doing a transformation in a changed basis</h3>
<p>Doing a 45 degree rotation in my world is done using the following matrix;</p>
<p><span class="math display">\[\frac{1}{\sqrt2}  \begin{pmatrix} 1 &amp; -1 \\ 1 &amp; 1 \end{pmatrix}\]</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_transformation_newbasis1.png" class="img-fluid">{:width=“400”}</p>
<p>However, we don’t know what a 45 degree transformation looks like in Bear’s world.</p>
<p>First, we need to transform the vector into our world using the matrix <span class="math inline">\(B\)</span>, which is the Bear’s basis vector in our world;</p>
<p><span class="math inline">\(B = \begin{bmatrix} 3&amp;1 \\ 1&amp;1 \end{bmatrix}\)</span></p>
<p>Then apply the transformation,</p>
<p>Then reverse the result using <span class="math inline">\(B^{-1}\)</span></p>
<p><span class="math inline">\(B^{-1} = \frac{1}{2} \begin{bmatrix} 1&amp;-1 \\ -1&amp;3 \end{bmatrix}\)</span></p>
<p>Or written as; <span class="math inline">\(B^{-1} R B = R_B\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/matrix_transformation_newbasis2.png" class="img-fluid">{:width=“400”}</p>
<blockquote class="blockquote">
<p>If we want to transform to normal form normal coordinate systems, then the translation matrices also change. We have to be mindful of that</p>
<p>We’ve got the transformation matrix R, wrapped around by B, B is the minus 1, that does the translation from my world to the world of the new basis system.</p>
</blockquote>
<p><strong>Practice</strong></p>
<p>Finish off the calculation;</p>
<p><span class="math display">\[\frac{1}{2} \begin{bmatrix} 3&amp;-1 \\ -1&amp;1\end{bmatrix} * \frac{1}{\sqrt2} \begin{bmatrix} 2&amp;0 \\ 4&amp;2\end{bmatrix} = \frac{1}{\sqrt2} \begin{bmatrix} 1&amp;-1 \\ 1&amp;1\end{bmatrix}\]</span></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># working</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># first matrix row * second matrix column</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>x1y1 <span class="op">=</span> (<span class="fl">1.5</span> <span class="op">*</span> <span class="dv">2</span>) <span class="op">+</span> (<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> <span class="dv">4</span>) <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>x1y2 <span class="op">=</span> (<span class="fl">1.5</span> <span class="op">*</span> <span class="dv">0</span>) <span class="op">+</span> (<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> <span class="dv">2</span>) <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>x2y1 <span class="op">=</span> (<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> <span class="dv">2</span>) <span class="op">+</span> (<span class="fl">0.5</span> <span class="op">*</span> <span class="dv">4</span>) <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>x2y2 <span class="op">=</span> (<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> <span class="dv">0</span>) <span class="op">+</span> (<span class="fl">0.5</span> <span class="op">*</span> <span class="dv">2</span>) <span class="op">=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="orthogonal-matrices" class="level3">
<h3 class="anchored" data-anchor-id="orthogonal-matrices">Orthogonal matrices</h3>
<p>We can transpose a matrix, where we interchange all the elements of the rows and columns of the matrix; <span class="math inline">\(A^T_{ij} = A_{ij}\)</span></p>
<p><span class="math display">\[\begin{bmatrix} 1&amp;2 \\ 3&amp;4\end{bmatrix}^T = \begin{bmatrix} 1&amp;3 \\ 2&amp;4\end{bmatrix}\]</span></p>
<p>We interchange the elements that are off the diagonal. So the one and the four stay where they are.</p>
<p>Because if is, I find <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are the same; elements at the coorindates 1,1 would stay the same, the same for 2,2. But the element 1,2 interchange to the element 2,1.</p>
<p>Imagine we have a square matrix <span class="math inline">\(A\)</span>, with dimensions n x n.&nbsp;This defines a transformation (ie we apply to vectors to transform them), the columns in this matrix are the basis vectors in a new space.</p>
<p>In this matrix;</p>
<ul>
<li>the vectors are orthogonal to each other and of unit length
<ul>
<li><span class="math inline">\(a_i . a_j = 0\)</span> if <span class="math inline">\(i \neq j\)</span></li>
<li><span class="math inline">\(a_i . a_j = 1\)</span> if <span class="math inline">\(i = j\)</span></li>
</ul></li>
</ul>
<p>If we multiply this matrix by it’s transpose <span class="math inline">\(A^T\)</span>, the columns <span class="math inline">\(a_1\)</span> to <span class="math inline">\(a_n\)</span> become rows.</p>
<p>The result of the multiplication is an identity matrix;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/orthogonal_matrices1.png" class="img-fluid">{:width=“400”}</p>
<p><span class="math inline">\(A^T\)</span> is a valid inverse of <span class="math inline">\(A\)</span></p>
<blockquote class="blockquote">
<p>A set of unit length basis vectors that are all perpendicular to each other are called an <strong>orthonormal basis set</strong></p>
</blockquote>
<p>They must meet the criteria; - <span class="math inline">\(a_i . a_j = 0\)</span> if <span class="math inline">\(i \neq j\)</span> - <span class="math inline">\(a_i . a_j = 1\)</span> if <span class="math inline">\(i = j\)</span></p>
<blockquote class="blockquote">
<p>The matrix composed of them is called an orthogonal matrix</p>
</blockquote>
<p>Because all the basis vectors in an orthogonal matrix are of unit length, it must scale space by a factor of one.</p>
<p>The determinant of an orthogonal matrix must be either plus or minus one (depending on if you do <span class="math inline">\(A^T * A or A * A^T\)</span>).</p>
<p>We want our transformation matrix to be an orthogonal matrix, we want the basis vectors to be orthonormal.</p>
<ul>
<li>the inverse is easy to compute</li>
<li>the transformation is reversible as it doesn’t collapse the space</li>
<li>the projection is the dot product</li>
<li>if arranged in the right order the determinant is one</li>
</ul>
</section>
<section id="the-gram-schmidt-process" class="level3">
<h3 class="anchored" data-anchor-id="the-gram-schmidt-process">The Gram-Schmidt process</h3>
<p>If we assume we already have some linearly independent vectors that span the space we’re interested in, we can construct an orthonormal basis vector set.</p>
<p>We can check linear independence by calculating the determinant. If there are linearly independent the determinant will be 0.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/grahmschmidt1.png" class="img-fluid">{:width=“400”}</p>
<p>However, these vectors are not orthogonal to one another and are not of unit length.</p>
<p>The Gram-Schmidt process allows us to transform this vector set into an orthonormal set.</p>
<p>Take the first vector <span class="math inline">\(v_1\)</span> and normalise it to be on unit length;</p>
<p><span class="math inline">\(e_1 = \frac{v_1}{\lVert v_1 \rVert}\)</span></p>
<p><span class="math inline">\(v_2\)</span> can now be thought of as (1) a component of that’s in the direction of <span class="math inline">\(e_1\)</span> plus (2) a component that is perpendicular to <span class="math inline">\(e_1\)</span>, which we can find by taking the projection of <span class="math inline">\(v_2\)</span> onto <span class="math inline">\(e_1\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/grahmschmidt2.png" class="img-fluid">{:width=“250”}</p>
<p>The vector projection is <span class="math inline">\(\frac{(v_2 . e_1)}{\lVert e_1 \rVert}\)</span> or simply (<span class="math inline">\(v_2 . e_1\)</span>) as <span class="math inline">\(e_1\)</span> has already been normalised, it has size 1. To get this as a vector, not just a number we multiply by <span class="math inline">\(e_1\)</span></p>
<p><span class="math inline">\(v_2 = (v_2 . e_1) e_1\)</span> + <span class="math inline">\(u_2\)</span></p>
<p>Rewritten as;</p>
<p><span class="math inline">\(u_2 = v_2 - (v_2 . e_1) e_1\)</span></p>
<p>Then normalised to unit length</p>
<p><span class="math inline">\(e_2 = \frac{u_2}{\lVert u_2 \rVert}\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/grahmschmidt3.png" class="img-fluid">{:width=“400”}</p>
<p><span class="math inline">\(v_3\)</span> is not linear combination of <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>, therefore it is not in the plane defined by <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> and following that, will not be defined by <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span>.</p>
<p>As a result, we need to project <span class="math inline">\(v_3\)</span> down onto the plane of <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> and the projection will be some vector in the plane composed of <span class="math inline">\(e_1\)</span>s and <span class="math inline">\(e_2\)</span>s</p>
<p>The perpendicular vector <span class="math inline">\(u_3\)</span> is what is left when you remove the elements of <span class="math inline">\(v_3\)</span> made up of <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span></p>
<p><span class="math inline">\(u_3 = v_3 - (v_3 . e_1)e_1 - (v_3 . e_2)e_2\)</span></p>
<p>Then normalise;</p>
<p><span class="math inline">\(e_3 = \frac{u_3}{\lVert u_3 \rVert}\)</span></p>
<p><span class="math inline">\(e_1, e_2, e_3\)</span> now form an orthonormal basis set</p>
<p>We can write a function to perform the Gram-Schmidt procedure. Taking in a list of vectors and forming an orthonormal basis set.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remember we access elements in matrix as;</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co"># individual elements</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>A[n, m]</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># rows</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>A[n]</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co"># columns</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>A[:, m]</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the dot product using @</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>u <span class="op">@</span> v</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Say we have 4 basis vectors;</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.linalg <span class="im">as</span> la</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>verySmallNumber <span class="op">=</span> <span class="fl">1e-14</span> <span class="co"># That's 1×10⁻¹⁴ = 0.00000000000001</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Our first function will perform the Gram-Schmidt procedure for 4 basis vectors.</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll take this list of vectors as the columns of a matrix, A.</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll then go through the vectors one at a time and set them to be orthogonal</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co"># to all the vectors that came before it. Before normalising.</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gsBasis4(A) :</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    B <span class="op">=</span> np.array(A, dtype<span class="op">=</span>np.float_) <span class="co"># Make B as a copy of A, since we're going to alter it's values.</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The zeroth column is easy, since it has no other vectors to make it normal to.</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># All that needs to be done is to normalise it. I.e. divide by its modulus, or norm.</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    B[:, <span class="dv">0</span>] <span class="op">=</span> B[:, <span class="dv">0</span>] <span class="op">/</span> la.norm(B[:, <span class="dv">0</span>])</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For the first column, we need to subtract any overlap with our new zeroth vector.</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    B[:, <span class="dv">1</span>] <span class="op">=</span> B[:, <span class="dv">1</span>] <span class="op">-</span> B[:, <span class="dv">1</span>] <span class="op">@</span> B[:, <span class="dv">0</span>] <span class="op">*</span> B[:, <span class="dv">0</span>]</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If there's anything left after that subtraction, then B[:, 1] is linearly independant of B[:, 0]</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If this is the case, we can normalise it. Otherwise we'll set that vector to zero.</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> la.norm(B[:, <span class="dv">1</span>]) <span class="op">&gt;</span> verySmallNumber :</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        B[:, <span class="dv">1</span>] <span class="op">=</span> B[:, <span class="dv">1</span>] <span class="op">/</span> la.norm(B[:, <span class="dv">1</span>])</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> :</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        B[:, <span class="dv">1</span>] <span class="op">=</span> np.zeros_like(B[:, <span class="dv">1</span>])</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now we need to repeat the process for column 2</span></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Subtract the overlap with the zeroth vector,</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>    B[:, <span class="dv">2</span>] <span class="op">=</span> B[:, <span class="dv">2</span>] <span class="op">-</span> B[:, <span class="dv">2</span>] <span class="op">@</span> B[:, <span class="dv">0</span>] <span class="op">*</span> B[:, <span class="dv">0</span>]</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Subtract the overlap with the first.</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>    B[:, <span class="dv">2</span>] <span class="op">=</span> B[:, <span class="dv">2</span>] <span class="op">-</span> B[:, <span class="dv">2</span>] <span class="op">@</span> B[:, <span class="dv">1</span>] <span class="op">*</span> B[:, <span class="dv">1</span>]</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Again we'll need to normalise our new vector.</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> la.norm(B[:, <span class="dv">2</span>]) <span class="op">&gt;</span> verySmallNumber :</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>        B[:, <span class="dv">2</span>] <span class="op">=</span> B[:, <span class="dv">2</span>] <span class="op">/</span> la.norm(B[:, <span class="dv">2</span>])</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> :</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>        B[:, <span class="dv">2</span>] <span class="op">=</span> np.zeros_like(B[:, <span class="dv">2</span>])    </span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Finally, column three:</span></span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Subtract the overlap with the first three vectors.</span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>    B[:, <span class="dv">3</span>] <span class="op">=</span> B[:, <span class="dv">3</span>] <span class="op">-</span> B[:, <span class="dv">3</span>] <span class="op">@</span> B[:, <span class="dv">0</span>] <span class="op">*</span> B[:, <span class="dv">0</span>]</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    B[:, <span class="dv">3</span>] <span class="op">=</span> B[:, <span class="dv">3</span>] <span class="op">-</span> B[:, <span class="dv">3</span>] <span class="op">@</span> B[:, <span class="dv">1</span>] <span class="op">*</span> B[:, <span class="dv">1</span>]</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    B[:, <span class="dv">3</span>] <span class="op">=</span> B[:, <span class="dv">3</span>] <span class="op">-</span> B[:, <span class="dv">3</span>] <span class="op">@</span> B[:, <span class="dv">2</span>] <span class="op">*</span> B[:, <span class="dv">2</span>]    </span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now normalise if possible</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> la.norm(B[:, <span class="dv">3</span>]) <span class="op">&gt;</span> verySmallNumber :</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>        B[:, <span class="dv">3</span>] <span class="op">=</span> B[:, <span class="dv">3</span>] <span class="op">/</span> la.norm(B[:, <span class="dv">3</span>])</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> :</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>        B[:, <span class="dv">3</span>] <span class="op">=</span> np.zeros_like(B[:, <span class="dv">3</span>])     </span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Finally, we return the result:</span></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> B</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>However, we can generalise the procedure;</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gsBasis(A) :</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    B <span class="op">=</span> np.array(A, dtype<span class="op">=</span>np.float_) <span class="co"># Make B as a copy of A, since we're going to alter it's values.</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop over all vectors, starting with zero, label them with i</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B.shape[<span class="dv">1</span>]) :</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Inside that loop, loop over all previous vectors, j, to subtract.</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i) :</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>            B[:, i] <span class="op">=</span> B[:, i] <span class="op">-</span> B[:, i] <span class="op">@</span> B[:, j] <span class="op">*</span> B[:, j]</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># do the normalisation test for B[:, i]</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> la.norm(B[:, i]) <span class="op">&gt;</span> verySmallNumber :</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>            B[:, i] <span class="op">=</span> B[:, i] <span class="op">/</span> la.norm(B[:, i])</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span> :</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>            B[:, i] <span class="op">=</span> np.zeros_like(B[:, i])</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Finally, we return the result:</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> B</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="co"># This function uses the Gram-schmidt process to calculate the dimension</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a><span class="co"># spanned by a list of vectors.</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Since each vector is normalised to one, or is zero,</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a><span class="co"># the sum of all the norms will be the dimension.</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dimensions(A) :</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(la.norm(gsBasis(A), axis<span class="op">=</span><span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="reflecting-in-a-plane" class="level3">
<h3 class="anchored" data-anchor-id="reflecting-in-a-plane">Reflecting in a plane</h3>
<p>Say we want to know what a vector looks like when reflected in some plane.</p>
<p>We have three vectors, the first two are within the plane of the mirror, the third is outside the plane.</p>
<p><span class="math display">\[v_1 = \begin{bmatrix} 1\\1\\1\end{bmatrix} v_2 = \begin{bmatrix} 2\\0\\1\end{bmatrix} v_3 = \begin{bmatrix} 3\\1\\-1\end{bmatrix}\]</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/reflecting.png" class="img-fluid">{:width=“200”}</p>
<p>Using the Grahm-Schimdt procedure to get some orthonormal basis vectors that desrribe the plane and its normal <span class="math inline">\(v_3\)</span></p>
<p><span class="math inline">\(e_1\)</span> is the normalised version of <span class="math inline">\(v_1\)</span>. <span class="math inline">\(v_1\)</span> is a length 3 (<span class="math inline">\(\sqrt{1^2 + 1^2 + 1^2}\)</span>)</p>
<p><span class="math display">\[ e_1 = \frac{v_1}{\lVert v_1 \rVert} = \frac{1}{\sqrt3} \begin{bmatrix} 1\\1\\1\end{bmatrix}\]</span></p>
<p><span class="math inline">\(u_2\)</span> is <span class="math inline">\(v_2\)</span> minus some number of <span class="math inline">\(e_1\)</span>s -&gt; more precisely; the projection of <span class="math inline">\(v_2\)</span> onto <span class="math inline">\(e_1\)</span> (<span class="math inline">\(v_2 . e_1\)</span>) multiplied by <span class="math inline">\(e_1\)</span></p>
<p><span class="math display">\[u_2 = v_2 - (v_2 . e_1)e_1 = \begin{bmatrix} 2\\0\\1\end{bmatrix} - (\begin{bmatrix} 2\\0\\1\end{bmatrix} . \frac{1}{\sqrt3} \begin{bmatrix} 1\\1\\1\end{bmatrix}) \frac{1}{\sqrt3} \begin{bmatrix} 1\\1\\1\end{bmatrix}\]</span></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>the root threes come outside, so become <span class="dv">1</span><span class="op">/</span><span class="dv">3</span> </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>] dotted <span class="cf">with</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>] <span class="kw">is</span> <span class="dv">3</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>so they cancel out to <span class="dv">1</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>so [<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>] <span class="op">-</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math display">\[ = \begin{bmatrix} 1\\-1\\0\end{bmatrix}\]</span></p>
<p><span class="math inline">\(e_2\)</span> is equal to the normalised version of <span class="math inline">\(u_2\)</span></p>
<p><span class="math display">\[e_2 = \frac{u_2}{\lVert u_2 \rVert} = \frac{1}{\sqrt2} \begin{bmatrix} 1\\-1\\0\end{bmatrix}\]</span></p>
<p>Then we need <span class="math inline">\(u_3\)</span></p>
<p><span class="math display">\[u_3 = v_3 - (v_3 . e_1)e_1 - (v_3 . e_2)e_2 = \begin{bmatrix} 1\\1\\-2\end{bmatrix}\]</span></p>
<p><span class="math display">\[e_3 = \frac{1}{\sqrt6} \begin{bmatrix} 1\\1\\-2\end{bmatrix}\]</span></p>
<p>Our new transformation is <span class="math inline">\(E\)</span>, described by our new basis vectors</p>
<p><span class="math display">\[E = \begin{pmatrix} \begin{bmatrix}e_1\end{bmatrix} \begin{bmatrix}e_2\end{bmatrix} \begin{bmatrix}e_3\end{bmatrix} \end{pmatrix}\]</span></p>
<p>This contains the plane (<span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> and then the normal to the plane <span class="math inline">\(e_3\)</span>. <em>It’s the bit of v3 that we can’t make by projecting on to v1 and v2, then of unit length</em>)</p>
<p>Say we have some vector <span class="math inline">\(r\)</span> that we want to reflect down through the pane, and get <span class="math inline">\(r'\)</span> on the other side;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/reflecting2.png" class="img-fluid">{:width=“400”}</p>
<p>We can think of <span class="math inline">\(r\)</span> as composed of some vector within the pane (composed of <span class="math inline">\(e_1\)</span>s and <span class="math inline">\(e_2\)</span>s) - this the dotted line perpendicular to <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> and some vector that is normal (ie made up of <span class="math inline">\(e_3\)</span>s) - this is the dotted line up to <span class="math inline">\(r\)</span></p>
<p>When we reflect through the pane, the bit made up of <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> will be unchanged and the bit composed of <span class="math inline">\(e_3\)</span>s will be inverted.</p>
<p><span class="math display">\[T_E = \begin{pmatrix} \begin{bmatrix}e_1\end{bmatrix} \begin{bmatrix}e_2\end{bmatrix} \begin{bmatrix}e_3'\end{bmatrix} \end{pmatrix}\]</span></p>
<p>Getting from <span class="math inline">\(r\)</span> to <span class="math inline">\(r'\)</span> is hard. As we saw with Bear.</p>
<ul>
<li>First we need to transform <span class="math inline">\(r\)</span> into the basis plane using the inverse of our orthogonal basis matrix,</li>
<li>Then transform it, do the reflection in the basis of the plane</li>
<li>Then read that back into my basis using the orthogonal basis matrix.</li>
</ul>
<p><em>Note</em> in this example we’re changing from our basis vectors to the plane’s and then translating back into our basis. So the <span class="math inline">\(E\)</span> and the <span class="math inline">\(E^{-1}\)</span> are flipped compared to when we were working with Bear.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/reflecting3.png" class="img-fluid">{:width=“250”}</p>
<p><span class="math inline">\(E T_E E^{-1}r = r'\)</span></p>
<p>Written out the whole thing looks a little ungodly, but that’s more down to the volume of arthimetic required than the complexity</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/reflecting4.png" class="img-fluid">{:width=“400”}</p>
<p>Generalised, this is the process of reflecting a point in space in a mirror (we can transform something through the looking glass).</p>
<p>In the <em>practical</em> world of machine learning, this will be the technique used when transforming images of forces for the purpose of doing facial recognition. You transform a face from being side on to profile, and then using some form of neural network to do the recognition.</p>
<p><strong>Practice reflections</strong></p>
<p>Perform a transformation that is easy in a particular basis, but complicated in our starting basis.</p>
<p>Namely we shall help Panda Bear determine what his reflection will look like in a mirror that he has placed at an angle.</p>
<p>The mirror lies along the first axis. But, as is the way with bears, his coordinate system is not orthonormal: so what he thinks is the direction perpendicular to the mirror isn’t actually the direction the mirror reflects in</p>
<p>Write a Python function that will produce a transformation matrix for reflecting vectors in an arbitrarily angled mirror.</p>
<p><span class="math inline">\(T = E T_E E^{-1}\)</span></p>
<p><em>note</em> &gt; the @ operator is used to combine vectors and/or matrices in the expected linear algebra way, i.e.&nbsp;it will be either the vector dot product, matrix multiplication, or matrix operation on a vector, depending on it’s input.</p>
<blockquote class="blockquote">
<p>This is in contrast to the \(*\) operator, which performs element-wise multiplication, or multiplication by a scalar.</p>
</blockquote>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> norm, inv</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> transpose</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> readonly.bearNecessities <span class="im">import</span> <span class="op">*</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_reflection_matrix(bearBasis):</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co">"""Return the transformation T"""</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co"># built using an orthonormal basis set (E), created from Bear's Basis</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="co"># and a transformation matrix (TE) in the mirror ccoordinates"""</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the gsBasis function on bearBasis to get the mirror's orthonormal basis</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    E <span class="op">=</span> gsBasis(bearBasis) <span class="co"># bearBasis is a 2×2 matrix</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Write a matrix in component form that performs the mirror's reflection in the mirror's basis</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the mirror operates by negating the last component of a vector - one axis doesn't change</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    TE <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>                   [<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine the matrices E and TE to produce your transformation matrix.</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> E <span class="op">@</span> TE <span class="op">@</span> inv(E) </span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="eigenvalues-and-eigenvectors" class="level2">
<h2 class="anchored" data-anchor-id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h2>
<p>We’ll start by using geometric expressions (shapes in 2d) to conceptually understand “<em>eigen-ness</em>”</p>
<p>Eigen is perhaps most usefully translated from German to mean charactersitic. When we talk about an eigenproblem, we’re talking about finding the charactersitic properities of something.</p>
<p>We’ve seen that we can express linear transformations using matrices. These transformation operations include scalings, rotations and shears.</p>
<blockquote class="blockquote">
<p>A transformation in which all points along a given line L remain fixed while other points are shifted parallel to L by a distance proportional to their perpendicular distance from L. Shearing a plane figure does not change its area. The shear can also be generalized to three dimensions, in which planes are translated instead of lines. - Wolfram</p>
</blockquote>
<p><img src="{{ site.baseurl }}/images/linear_algebra/shear1.gif" class="img-fluid">{:width=“250”}</p>
<p>Typically, we have thought about how these transformations change a single vector. What about if the matrix was applied to all vectors in the space?</p>
<p>We can think of this by having a square, centred in the middle of our basis vectors and seeing how the shape is transformed.</p>
<p>Applying a scaling of 2 in the vertical direction, the square becomes a rectangle. Applying a horizontal sheer gives us a parrallelegram.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/eigen1.png" class="img-fluid">{:width=“400”}</p>
<p>The square helps us to understand what is happening to many vectors. However, some vectors remain on the same line they started on, while others do not.</p>
<p>Take our initial square, with three vectors drawn on;</p>
<p>If we scale vertically, the diagonal vector will <strong>not</strong> be pointing in the same direction. Any other vector’s direction would have changed (apart from the horizontal and vertical, their angle and size will have changed)</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/eigen2.png" class="img-fluid">{:width=“250”}</p>
<p>The horizontal and vertical vectors are charactersitic of this particular trnasformation. They are the only ones that do not change. They are referred to as eigenvectors.</p>
<p>Becuase the horiztonal’s length was unchanged, we say it has a “corresponding eigenvalue of one”, whereas the vertical eigenvector doubled in length, so it has a “corresponding eigenvalue of two”.</p>
<p>Eigenvectors are those laying on the same span as before the transformation. Then we measure how much their length has changed.</p>
<p>Take a pure shear (where there is no rotation or scaling so the area is unchanged), here we would have one eigenvector, along the horiztonal;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/eigen3.png" class="img-fluid">{:width=“250”}</p>
<p>In rotation, there are no eigenvectors.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/eigen4.png" class="img-fluid">{:width=“250”}</p>
<p><strong>Practice</strong></p>
<p>In all examples, we’ll start with the following vectors and apply a transformation <span class="math inline">\(T\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/eigen5.png" class="img-fluid">{:width=“250”}</p>
<p><span class="math inline">\(T_1= \begin{bmatrix} 2&amp;0\\0&amp;2 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(T_1\)</span> scales each vectors by 2, so all three can be considered eigenvectors.</p>
<p><span class="math inline">\(T_2= \begin{bmatrix} 3&amp;0\\0&amp;2 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(T_2\)</span> scales the x axis by 3 and the y axis by 2, so our purple vector will no long be on the same plane.</p>
<p><span class="math inline">\(T_3= \begin{bmatrix} 1&amp;2\\0&amp;1 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(T_3\)</span> is a sheer, the x axis is unchanged, but the angle and size of the other two vectors is changed along the x-axis</p>
<p><span class="math inline">\(T_4= \begin{bmatrix} 0&amp;-1\\1&amp;0 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(T_4\)</span> is an anti-clockwise rotation, so there will be no vectors that remain pointing along the same pane.</p>
<p><span class="math inline">\(T_4= \begin{bmatrix} 0&amp;-1\\1&amp;0 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(T_4\)</span> is an anti-clockwise rotation, so there will be no eigenvectors</p>
<p><span class="math inline">\(T_5= \begin{bmatrix} -1&amp;0\\0&amp;-1 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(T_5\)</span> is a reflection, all vectors will be pointing along the same pane, just in the opposite direction</p>
<p><span class="math inline">\(T_6= \begin{bmatrix} 2&amp;1\\0&amp;2 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(T_6\)</span> scales all vectors by 2, but alters the angle at which the orange and purple vectors are pointing</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/eigen6.png" class="img-fluid">{:width=“650”}</p>
<p>To summaise, eigenvectors are those that lay along the same path after applying a linear transformation to a space. Eigenvalues are the amount that each of those vectors has been stretched in the process (or negative if flipped).</p>
<p>In the practice examples, it appeared that rotation would leave us without any eigenvectors, however, 180 degree rotation, is equivalent to a reflection, where the vectors are pointing in the opposite direction. All vectors will be eigenvectors will an eigven value of -1.</p>
<p>A transformation that is some combination of horizontal shearing and vertical scaling does have two eigenvectors. The first which is most obvious is the horiztonal vector. The second is between the organge and the pink vector. Though the concept is straight forward, there are not always easy to spot.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/eigen7.png" class="img-fluid">{:width=“400”}</p>
<p>This problem is amplified in three or more dimensions, where we can’t simply use geometric representations to spot eigenvectors.</p>
<p>In 3D scaling and shearing work much the same way, but rotation works differently. The eigenvector represents the axis of rotation.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/eigen8.png" class="img-fluid">{:width=“400”}</p>
<section id="eigenvectors-a-formal-definition" class="level3">
<h3 class="anchored" data-anchor-id="eigenvectors-a-formal-definition">Eigenvectors, a formal definition</h3>
<p>Alegrabically eigenvectors (<span class="math inline">\(x\)</span>) can be represented as;</p>
<p><span class="math inline">\(Ax = \lambda x\)</span></p>
<p>On the lefthand side <span class="math inline">\(A\)</span> represents a transformation matrix being applied to the vector <span class="math inline">\(x\)</span>. On the righthand side we are stretching the vector by some scalar factor lambda.</p>
<p><span class="math inline">\(A\)</span> must be a square transformation (<span class="math inline">\(n \times n\)</span>) and <span class="math inline">\(x\)</span> must be an <span class="math inline">\(n\)</span> dimensional vector. Otherwise it’s shape would change, it wouldn’t just scale.</p>
<p><span class="math inline">\((A - \lambda I) x = 0\)</span> <em>I represents an identity matrix, the same size as <span class="math inline">\(A\)</span></em></p>
<blockquote class="blockquote">
<p>We didn’t need this in the first expression we wrote, as multiplying vectors by scalars is defined. However, subtracting scalars from matrices is not defined</p>
</blockquote>
<p>Either <span class="math inline">\(x\)</span> is 0 or the contents of the brackets. However, we’re not interested we <span class="math inline">\(x = 0\)</span>, that means the vector has no length or direction, it’s a trivial solution.</p>
<p>We can test if a matrix operation will result in a 0 output by calculating its determinant.</p>
<p><span class="math inline">\(det(A - \lambda I) = 0\)</span></p>
<p>In the case of a 2x2;</p>
<p><span class="math display">\[det\begin{pmatrix} \begin{pmatrix} a&amp;b\\c&amp;d \end{pmatrix} - \begin{pmatrix} \lambda&amp;0 \\ 0&amp;\lambda \end{pmatrix} \end{pmatrix} = 0\]</span></p>
<p>Evaluating this determinant, we get what is referred to as the <em>characteristic polynomial</em></p>
<p><span class="math display">\[\lambda^2 - (a+d)\lambda + ad - bc = 0\]</span></p>
<p>Our eigenvalues are simply the solutions of this equation, and we can then plug these eigenvalues back into the original expression to calculate our eigenvectors.</p>
<p>This gets complex in high dimensions, but that’s why we have computers (they truly are a bicycle for the mind).</p>
<p>Let’s take the example of a vertical scaling of 2</p>
<p><span class="math inline">\(A = \begin{pmatrix} 1&amp;0\\0&amp;2 \end{pmatrix}\)</span></p>
<p>Take the determinant of A minus lambda I (<span class="math inline">\(A - \lambda I\)</span>) and set it to zero</p>
<p><span class="math display">\[det\begin{pmatrix} 1-\lambda&amp;0\\0&amp;2-\lambda \end{pmatrix} = 0 = (1-\lambda)(2-\lambda)\]</span></p>
<p>Our equation has solutions at lambda = 1 and lambda = 2, so we can substitute back in;</p>
<p><span class="math display">\[@\lambda=1: \begin{pmatrix} 1-1&amp;0\\0&amp;2-1\end{pmatrix} \begin{bmatrix} x_1\\x_2 \end{bmatrix} = \begin{pmatrix} 0&amp;0\\0&amp;1\end{pmatrix} \begin{bmatrix} x_1\\x_2 \end{bmatrix} = \begin{bmatrix} 0\\x_2\end{bmatrix} = 0\]</span></p>
<p><span class="math display">\[@\lambda=2: \begin{pmatrix} 1-2&amp;0\\0&amp;2-2\end{pmatrix} \begin{bmatrix} x_1\\x_2 \end{bmatrix} = \begin{pmatrix} -1&amp;0\\0&amp;0\end{pmatrix} \begin{bmatrix} x_1\\x_2 \end{bmatrix} = \begin{bmatrix} -x_1\\0\end{bmatrix} = 0\]</span></p>
<p>At lambda equals one, the <span class="math inline">\(x_2\)</span> term must = 0. Our x axis <span class="math inline">\(x_1\)</span> can equal anything, as long as there’s 0 in the vertical direction.</p>
<p>We express this as; <span class="math inline">\(@\lambda=1: x = \begin{bmatrix} t\\0 \end{bmatrix}\)</span></p>
<p>At lambda equals two, we can express our eigen vector as not moving in the horizontal direction, any scaling along the vertical axis.</p>
<p>We express this as; <span class="math inline">\(@\lambda=2: x = \begin{bmatrix} 0\\t \end{bmatrix}\)</span></p>
<p>Let’s take the example of a 90 degree rotation, where we expect no eigenvectors.</p>
<p><span class="math inline">\(A = \begin{pmatrix} 0&amp;-1\\1&amp;0 \end{pmatrix}\)</span></p>
<p>Our characteristic polynomial is;</p>
<p><span class="math display">\[\lambda^2 - (a+d)\lambda + ad - bc = 0\]</span></p>
<p>In this case <span class="math inline">\(\lambda^2 + 1\)</span> as <span class="math inline">\(a+d = 0\)</span>, as is <span class="math inline">\(a\times d\)</span>, and <span class="math inline">\(b\times c = -1\)</span>, so minus -1 gives us <span class="math inline">\(+1\)</span>.</p>
<p><span class="math inline">\(\lambda^2 + 1 = 0\)</span></p>
<p>Doesn’t have any real numbered solutions at all. Hence, no real eigenvectors.</p>
<blockquote class="blockquote">
<p>We saw that our approach required finding the roots of a polynomial of order n, i.e., the dimension of your matrix. Which means that the problem will very quickly stop being possible by analytical methods alone. So when a computer finds the eigensolutions of a 100 dimensional problem it’s forced to employ iterative numerical methods.</p>
</blockquote>
<p><strong>Practice</strong></p>
<p>Practice calculating and solving the characteristic polynomial to find the eigenvalues of simple matrices.</p>
<p><strong>Q1</strong></p>
<p><span class="math inline">\(A = \begin{pmatrix} 1&amp;0\\0&amp;2 \end{pmatrix}\)</span>, what is the characteristic polynomial, and the solutions to the characteristic polynomial?</p>
<p><span class="math inline">\(\lambda^2 - 3\lambda + 2 = 0\)</span></p>
<p><span class="math inline">\(\lambda_1 = 1, \lambda_2 = 2\)</span></p>
<p>Select the eigenvectors;</p>
<p><span class="math inline">\(\begin{bmatrix} 0\\2 \end{bmatrix}\)</span> <span class="math inline">\(\begin{bmatrix} 0\\3 \end{bmatrix}\)</span> <span class="math inline">\(\begin{bmatrix} 1\\0 \end{bmatrix}\)</span></p>
<blockquote class="blockquote">
<p>Recall that if a vector is an eigenvector of a matrix, then so is any (non-zero) multiple of that vector.</p>
</blockquote>
<blockquote class="blockquote">
<p>One way to check that a vector is an eigenvector is to simply apply the matrix transformation and see if this is the same as multiplying by a scalar. Another way is to calculate the eigenvector by hand.</p>
</blockquote>
<p><strong>Q2</strong></p>
<p><span class="math inline">\(A = \begin{pmatrix} 3&amp;4\\0&amp;5 \end{pmatrix}\)</span>, what is the characteristic polynomial, and the solutions to the characteristic polynomial?</p>
<p><span class="math inline">\(\lambda^2 - 8\lambda + 15 = 0\)</span></p>
<p><span class="math inline">\(\lambda_1 = 3, \lambda_2 = 5\)</span></p>
<p>Select the eigenvectors;</p>
<p><span class="math inline">\(\begin{bmatrix} 2\\1 \end{bmatrix}\)</span> <span class="math inline">\(\begin{bmatrix} 3\\0 \end{bmatrix}\)</span> <span class="math inline">\(\begin{bmatrix} -1\\-\frac{1}{2} \end{bmatrix}\)</span> <span class="math inline">\(\begin{bmatrix} 0\\0 \end{bmatrix}\)</span></p>
<p>For example</p>
<p><span class="math inline">\(A \times \begin{bmatrix} 2\\1 \end{bmatrix} = \begin{bmatrix} 10\\5 \end{bmatrix}\)</span> equivalent to scaling by 5</p>
<p><span class="math inline">\(A \times \begin{bmatrix} -1\\-\frac{1}{2} \end{bmatrix} = \begin{bmatrix} -5\\-2.5 \end{bmatrix}\)</span> equivalent to scaling by 5</p>
<p><strong>Q3</strong></p>
<p><span class="math inline">\(A = \begin{pmatrix} 1&amp;0\\-1&amp;4 \end{pmatrix}\)</span>, what is the characteristic polynomial, and the solutions to the characteristic polynomial?</p>
<p><span class="math inline">\(\lambda^2 - 5\lambda + 4 = 0\)</span></p>
<p><span class="math inline">\(\lambda_1 = 1, \lambda_2 = 4\)</span></p>
<p>Select the eigenvectors;</p>
<p><span class="math inline">\(\begin{bmatrix} 3\\1 \end{bmatrix}\)</span> <span class="math inline">\(\begin{bmatrix} 0\\1 \end{bmatrix}\)</span></p>
<p><strong>Q4</strong></p>
<p><span class="math inline">\(A = \begin{pmatrix} -3&amp;8\\2&amp;3 \end{pmatrix}\)</span>, what is the characteristic polynomial, and the solutions to the characteristic polynomial?</p>
<p><span class="math inline">\(\lambda^2 -25 = 0\)</span></p>
<p><span class="math inline">\(\lambda_1 = -5, \lambda_2 = 5\)</span></p>
<p>Select the eigenvectors;</p>
<p><span class="math inline">\(\begin{bmatrix} -1\\-1 \end{bmatrix}\)</span> <span class="math inline">\(\begin{bmatrix} 1\\1 \end{bmatrix}\)</span> <span class="math inline">\(\begin{bmatrix} 4\\-1 \end{bmatrix}\)</span></p>
<p>For example</p>
<p><span class="math inline">\(A \times \begin{bmatrix} 4\\-1 \end{bmatrix} = \begin{bmatrix} -20\\5 \end{bmatrix}\)</span> equivalent to scaling by 5 and inversion</p>
<p>In this case <span class="math inline">\(\begin{bmatrix} 0\\2 \end{bmatrix}\)</span> is not;</p>
<p><span class="math inline">\(A \times \begin{bmatrix} 0\\2 \end{bmatrix} = \begin{bmatrix} 10\\6 \end{bmatrix}\)</span> not very eigen</p>
<p><strong>Q5</strong></p>
<p><span class="math inline">\(A = \begin{pmatrix} 5&amp;4\\-4&amp;-3 \end{pmatrix}\)</span>, what is the characteristic polynomial, and the solutions to the characteristic polynomial?</p>
<p><span class="math inline">\(\lambda^2 - 2\lambda + 1 = 0\)</span></p>
<p><span class="math inline">\(\lambda_1 = \lambda_2 = 1\)</span></p>
<p>This matrix has one repeated eigenvalue - which means it may have one or two distinct eigenvectors (which are not scalar multiples of each other).</p>
<p><strong>Q6</strong></p>
<p><span class="math inline">\(A = \begin{pmatrix} -2&amp;-3\\1&amp;1 \end{pmatrix}\)</span>, what is the characteristic polynomial, and the solutions to the characteristic polynomial?</p>
<p><span class="math inline">\(\lambda^2 + \lambda + 1 = 0\)</span></p>
<p>No real solutions</p>
<p>This matrix has no real eigenvalues, so any eigenvalues are complex in nature. This is beyond the scope of this course, so we won’t delve too deeply on this.</p>
</section>
<section id="using-eigenvectors-as-your-basis" class="level3">
<h3 class="anchored" data-anchor-id="using-eigenvectors-as-your-basis">Using eigenvectors as your basis</h3>
<p><em>Diagonalisation</em> is when we perform efficient matrix operations using eigenvectors as our basis.</p>
<p>There will be times when we want to apply the same matrix transformation multiple times. For instance, in a time series, once an event occurs.</p>
<p>We start at <span class="math inline">\(v_0\)</span>, then apply <span class="math inline">\(T\)</span> to get to <span class="math inline">\(v_1\)</span>, then apply <span class="math inline">\(T\)</span> again, to get to <span class="math inline">\(v_2\)</span></p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/eigenbasis1.png" class="img-fluid">{:width=“400”}</p>
<p><span class="math inline">\(v_2\)</span> is equivalent to applying <span class="math inline">\(T\)</span> to <span class="math inline">\(v_1\)</span> or applying <span class="math inline">\(T\)</span> to <span class="math inline">\(v_0\)</span> twice</p>
<p><span class="math display">\[v_2 = Tv_1 = T(Tv_0) = T^2v_0\]</span></p>
<p>We can generalise this to <span class="math inline">\(n\)</span> times, <span class="math inline">\(v_n = T^nv_0\)</span></p>
<p>Matrix multiplication can be comuputationally expensive, particularly as the dimensions increase.</p>
<p>If all the terms in a matrix are zero, except for those along the leadig diagonal, we refer to it as a diagonal matrix. When raising matrices to powers, i.e.&nbsp;as in the example above, applying the same transformation continuously, diagonal matrices make things a lot easier.</p>
<p><span class="math inline">\(A = \begin{bmatrix} 2&amp;0\\0&amp;2 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(A^3 = \begin{bmatrix} 8&amp;0\\0&amp;8 \end{bmatrix}\)</span></p>
<p><span class="math inline">\(T^n = \begin{pmatrix} a^n&amp;0&amp;0\\0&amp;b^n&amp;0\\0&amp;0&amp;c^n \end{pmatrix}\)</span></p>
<p>However, if <span class="math inline">\(T\)</span> is not a diagonal matrix, we transform it to an eigenbasis, which will be diagonal.</p>
<p>As we saw in the section on changing basis, each column of our transform matrix simply represents the new location of the transformed unit vectors.</p>
<p>To build our eigen-basis conversion matrix (<span class="math inline">\(C\)</span>), we plug in each of our eigenvectors as columns (<span class="math inline">\(ev\)</span>).</p>
<p><span class="math inline">\(C = \begin{pmatrix} ev_1\\ev_2\\ev_3 \end{pmatrix}\)</span></p>
<p>Our diagonal matrix <span class="math inline">\(D\)</span>, which is what we’ll scale up, contains the corresponding eigenvalues of the matrix <span class="math inline">\(T\)</span></p>
<p><span class="math inline">\(T^n = \begin{pmatrix} \lambda_1&amp;0&amp;0\\0&amp;\lambda_2&amp;0\\0&amp;0&amp;\lambda_3 \end{pmatrix}\)</span></p>
<p>We convert our matrix <span class="math inline">\(T\)</span> to our eigenbasis, applying our diagonalised matrix and then convert back again.</p>
<p><span class="math inline">\(T = CDC^{-1}\)</span></p>
<p><span class="math inline">\(T^2 = CDC^{-1}CDC^{-1}\)</span></p>
<p>Multiplying a matrix by it’s inversion directly, as we do in the middle of <span class="math inline">\(T^2\)</span> is the same as doing nothing at all, so we can remove this expression;</p>
<p><span class="math inline">\(T^2 = CDDC^{-1} = CD^2C^{-1}\)</span></p>
<p>This becomes generalisable;</p>
<p><span class="math inline">\(T^n = CDDC^{-1} = CD^nC^{-1}\)</span></p>
<p>We now have a method which lets us apply a transformation matrix as many times as we’d like without paying a large computational cost.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/eigenbasis2.png" class="img-fluid">{:width=“400”}</p>
</section>
<section id="eigenbasis-example" class="level3">
<h3 class="anchored" data-anchor-id="eigenbasis-example">Eigenbasis example</h3>
<p>Using the transformation matrix <span class="math inline">\(T = \begin{pmatrix} 1&amp;1\\0&amp;2\end{pmatrix}\)</span></p>
<p>As the first column is just <code>[1, 0]</code>, this means that our <span class="math inline">\(\hat{i}\)</span> vector will be unchanged. However, the second column tells us that <span class="math inline">\(\hat{j}\)</span>, the second vector will be moving to the point <code>[1, 2]</code>.</p>
<blockquote class="blockquote">
<p>This particular transform could be decomposed into a vertical scaling by a factor of 2, and then a horizontal shear by a half step.</p>
</blockquote>
<p>The diagonal vector of <code>[1, 1]</code> will become <code>[2, 2]</code> working below;</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># T rows * columns</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="dv">2</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">=</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Applying T</strong></p>
<p>Say we apply <span class="math inline">\(T\)</span> to <code>[-1, 1]</code>, here we are applying <span class="math inline">\(T\)</span> for the first time</p>
<p><span class="math inline">\(\begin{pmatrix} 1&amp;1\\0&amp;2\end{pmatrix} \begin{pmatrix} -1\\1\end{pmatrix} = \begin{pmatrix} -1+1\\0+2\end{pmatrix} = \begin{pmatrix} 0\\2\end{pmatrix}\)</span></p>
<p>Apply <span class="math inline">\(T\)</span> again, here we are applying <span class="math inline">\(T\)</span> for the second time</p>
<p><span class="math inline">\(\begin{pmatrix} 1&amp;1\\0&amp;2\end{pmatrix} \begin{pmatrix} 0\\2\end{pmatrix} = \begin{pmatrix} 0+2\\0+4\end{pmatrix} = \begin{pmatrix} 2\\4\end{pmatrix}\)</span></p>
<p>However, we could have started with finding <span class="math inline">\(T^2\)</span>;</p>
<p><span class="math inline">\(\begin{pmatrix} 1&amp;1\\0&amp;2\end{pmatrix} \begin{pmatrix} 1&amp;1\\0&amp;2\end{pmatrix} = \begin{pmatrix} 1&amp;3\\0&amp;4\end{pmatrix}\)</span></p>
<p>And applying that to our vector <code>[-1, 1]</code></p>
<p><span class="math inline">\(\begin{pmatrix} 1&amp;3\\0&amp;4\end{pmatrix} \begin{pmatrix} -1\\1\end{pmatrix} = \begin{pmatrix} -1+3\\0+4\end{pmatrix} = \begin{pmatrix} 2\\4\end{pmatrix}\)</span></p>
<p>We can do the whole process using our eigenbasis approach;</p>
<p>The conversion matrix is made up of the eigen vectors;</p>
<p>eigenvectors and eigenvalues;</p>
<p><span class="math inline">\(ev_1 = [1, 0], \lambda = 1\)</span>, x axis vector (<span class="math inline">\(\hat{i}\)</span>) does not alter</p>
<p><span class="math inline">\(ev_2 = [1, 1], \lambda = 2\)</span>, diagonal vector scales by 2</p>
<p><span class="math inline">\(C = \begin{pmatrix} 1&amp;1\\0&amp;1\end{pmatrix}\)</span></p>
<p>The inverse of <span class="math inline">\(C\)</span> can be calculated mentally, we shift to the left, rather than the right, though best to compute computationally;</p>
<p><span class="math inline">\(C^{-1} = \begin{pmatrix} 1&amp;-1\\0&amp;1\end{pmatrix}\)</span></p>
<p><span class="math inline">\(T^2 = CD^2C^{-1}\)</span></p>
<p><em>remember</em> <span class="math inline">\(D\)</span> is the diagonal matrix of <span class="math inline">\(T\)</span></p>
<p><span class="math display">\[T^2 = \begin{pmatrix} 1&amp;1\\0&amp;1\end{pmatrix} \begin{pmatrix} 1&amp;0\\0&amp;2\end{pmatrix}^2 \begin{pmatrix} 1&amp;-1\\0&amp;1\end{pmatrix}\]</span></p>
<p><em>start from the in, and work our way out</em></p>
<p><span class="math display">\[T^2 = \begin{pmatrix} 1&amp;1\\0&amp;1\end{pmatrix} \begin{pmatrix} 1&amp;-1\\0&amp;4\end{pmatrix}\]</span></p>
<p><em>one more step</em></p>
<p><span class="math display">\[T^2 = \begin{pmatrix} 1&amp;3\\0&amp;4\end{pmatrix}\]</span></p>
<p><em>then apply to our vector</em> <code>[-1, 1]</code></p>
<p><span class="math display">\[\begin{pmatrix} 1&amp;3\\0&amp;4\end{pmatrix} \begin{pmatrix} -1\\1\end{pmatrix} = \begin{pmatrix} 2\\4\end{pmatrix}\]</span></p>
<p><strong>Practice</strong></p>
<p><strong>Q1</strong></p>
<p>Give the matrix <span class="math inline">\(T = \begin{pmatrix} 6&amp;-1\\2&amp;3\end{pmatrix}\)</span> and a change basis matrix of <span class="math inline">\(C = \begin{pmatrix} 1&amp;1\\1&amp;2\end{pmatrix}\)</span></p>
<p>Calculate <span class="math inline">\(D = C^{-1}TC\)</span></p>
<p><span class="math inline">\(D = \begin{pmatrix} 1&amp;1\\1&amp;2\end{pmatrix}\)</span></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># start w T * C</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> (<span class="dv">6</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> (<span class="dv">6</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> <span class="dv">2</span>) <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> (<span class="dv">2</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="dv">3</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> (<span class="dv">2</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="dv">3</span> <span class="op">*</span> <span class="dv">2</span>) <span class="op">=</span>  <span class="dv">8</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>[[<span class="dv">5</span>, <span class="dv">4</span>], [<span class="dv">5</span>, <span class="dv">8</span>]]</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co"># verify</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>np.array([[<span class="dv">6</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">3</span>]]) <span class="op">@</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">2</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math inline">\(C^{-1}\)</span> flip the off diagonal and make the leading diagonal negative; <span class="math inline">\(\begin{pmatrix} 2&amp;-1\\-1&amp;1\end{pmatrix}\)</span></p>
<blockquote class="blockquote">
<p>Recall that when a matrix is transformed into its diagonal form, the entries along the diagonal are the eigenvalues of the matrix - this can save lots of calculation!</p>
</blockquote>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> inv</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>C<span class="op">^-</span><span class="dv">1</span> <span class="op">=</span> inv([[<span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>c_inv <span class="op">@</span> np.array([[<span class="dv">5</span>, <span class="dv">4</span>], [<span class="dv">5</span>, <span class="dv">8</span>]])</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>array([[<span class="fl">5.</span>, <span class="fl">0.</span>],</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>       [<span class="fl">0.</span>, <span class="fl">4.</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q2</strong></p>
<p>Give the matrix <span class="math inline">\(T = \begin{pmatrix} 2&amp;7\\0&amp;-1\end{pmatrix}\)</span> and a change basis matrix of <span class="math inline">\(C = \begin{pmatrix} 7&amp;1\\-3&amp;0\end{pmatrix}\)</span></p>
<p>Calculate <span class="math inline">\(D = C^{-1}TC\)</span></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># start w T * C</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> (<span class="dv">2</span> <span class="op">*</span> <span class="dv">7</span>) <span class="op">+</span> (<span class="dv">7</span> <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>) <span class="op">=</span> <span class="op">-</span><span class="dv">7</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> (<span class="dv">2</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="dv">7</span> <span class="op">*</span> <span class="dv">0</span>) <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> (<span class="dv">0</span> <span class="op">*</span> <span class="dv">7</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> <span class="op">-</span><span class="dv">3</span>) <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> (<span class="dv">0</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> <span class="dv">0</span>) <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>[[<span class="op">-</span><span class="dv">7</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">0</span>]]</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="co"># verify</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>np.array([[<span class="dv">2</span>, <span class="dv">7</span>], [<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]]) <span class="op">@</span> np.array([[<span class="dv">7</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math inline">\(C^{-1}\)</span> flip the off diagonal and make the leading diagnoal negative; <span class="math inline">\(\begin{pmatrix} 0&amp;1\\-3&amp;7\end{pmatrix}\)</span> does not seem to work here</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> inv</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>c_inv <span class="op">=</span> inv(np.array([[<span class="dv">7</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">0</span>]])) <span class="op">=</span> array([[<span class="dv">0</span>, <span class="op">-</span><span class="fl">0.333</span>], [<span class="dv">1</span>,  <span class="fl">2.333</span>]])</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>c_inv <span class="op">@</span> np.array([[<span class="op">-</span><span class="dv">7</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">0</span>]])</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>array([[<span class="op">-</span><span class="fl">1.</span>, <span class="fl">0.</span>],</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>       [<span class="fl">4.4409</span>, <span class="dv">2</span>]]) <span class="co"># something wrong bottom left</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># should be</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>array([[<span class="op">-</span><span class="fl">1.</span>, <span class="fl">0.</span>],</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>       [<span class="dv">0</span>, <span class="dv">2</span>]])    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q3</strong></p>
<p>Give the matrix <span class="math inline">\(T = \begin{pmatrix} 1&amp;0\\2&amp;-1\end{pmatrix}\)</span> and a change basis matrix of <span class="math inline">\(C = \begin{pmatrix} 1&amp;0\\1&amp;1\end{pmatrix}\)</span></p>
<p>Calculate <span class="math inline">\(D = C^{-1}TC\)</span></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># start w T * C</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> (<span class="dv">1</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="dv">0</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> (<span class="dv">1</span> <span class="op">*</span> <span class="dv">0</span>) <span class="op">+</span> (<span class="dv">0</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> (<span class="dv">2</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> (<span class="dv">2</span> <span class="op">*</span> <span class="dv">0</span>) <span class="op">+</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> <span class="dv">1</span>) <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>[[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co"># verify</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>np.array([[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>]]) <span class="op">@</span> np.array([[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math inline">\(C^{-1}\)</span> flip the off diagonal and make the leading diagonal negative; <span class="math inline">\(\begin{pmatrix} 1&amp;0\\-1&amp;1\end{pmatrix}\)</span></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> inv</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>c_inv <span class="op">=</span> inv(np.array([[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]]))</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co"># array([[1, 0], [-1, 1]])</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>c_inv <span class="op">@</span> np.array([[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>array([[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q4</strong></p>
<p>Give the matrix <span class="math inline">\(T = \begin{pmatrix} a&amp;0\\0&amp;a\end{pmatrix}\)</span> and a change basis matrix of <span class="math inline">\(C = \begin{pmatrix} 1&amp;2\\0&amp;1\end{pmatrix}\)</span> and <span class="math inline">\(C^{-1} = \begin{pmatrix} 1&amp;-2\\0&amp;1\end{pmatrix}\)</span></p>
<p>Calculate <span class="math inline">\(D = C^{-1}TC\)</span></p>
<p><span class="math inline">\(T = \begin{pmatrix} a&amp;0\\0&amp;a\end{pmatrix}\)</span></p>
<p><em>TODO return and do properly with eigenvectors</em></p>
<p><strong>Q5</strong></p>
<p>Give the matrix <span class="math inline">\(T = \begin{pmatrix} 6&amp;-1\\2&amp;3\end{pmatrix} = \begin{pmatrix} 1&amp;1\\1&amp;2\end{pmatrix} \begin{pmatrix} 5&amp;0\\0&amp;4\end{pmatrix} \begin{pmatrix} 2&amp;-1\\-1&amp;1\end{pmatrix}\)</span></p>
<p>Calculate the matrix <span class="math inline">\(T^3\)</span></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>np.array([[<span class="dv">6</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">3</span>]]) <span class="op">@</span> </span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    np.array([[<span class="dv">6</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">3</span>]]) <span class="op">@</span> </span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    np.array([[<span class="dv">6</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">3</span>]])</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>[[<span class="dv">186</span>, <span class="op">-</span><span class="dv">61</span>], [<span class="dv">122</span>, <span class="dv">3</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><em>TODO return and do properly with eigenvectors</em></p>
<p><strong>Q6</strong></p>
<p>Give the matrix <span class="math inline">\(T = \begin{pmatrix} 2&amp;7\\0&amp;-1\end{pmatrix} = \begin{pmatrix} 7&amp;1\\-3&amp;0\end{pmatrix} \begin{pmatrix} -1&amp;0\\0&amp;2\end{pmatrix} \begin{pmatrix} 0&amp;-\frac{1}{3}\\1&amp;\frac{7}{3}\end{pmatrix}\)</span></p>
<p>Calculate the matrix <span class="math inline">\(T^3\)</span></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>np.array([[<span class="dv">2</span>, <span class="dv">7</span>], [<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]]) <span class="op">@</span> </span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    np.array([[<span class="dv">2</span>, <span class="dv">7</span>], [<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]]) <span class="op">@</span> </span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    np.array([[<span class="dv">2</span>, <span class="dv">7</span>], [<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>[[<span class="dv">8</span>, <span class="dv">21</span>], [<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><em>TODO return and do properly with eigenvectors</em></p>
<p><strong>Q7</strong></p>
<p>Give the matrix <span class="math inline">\(T = \begin{pmatrix} 1&amp;0\\2&amp;-1\end{pmatrix} = \begin{pmatrix} 1&amp;0\\1&amp;1\end{pmatrix} \begin{pmatrix} 1&amp;0\\0&amp;-1\end{pmatrix} \begin{pmatrix} 1&amp;0\\-1&amp;1\end{pmatrix}\)</span></p>
<p>Calculate the matrix <span class="math inline">\(T^5\)</span></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>np.array([[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>]]) <span class="op">@</span> </span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>    np.array([[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>]]) <span class="op">@</span> </span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    np.array([[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>]]) <span class="op">@</span> </span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    np.array([[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>]]) <span class="op">@</span> </span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>    np.array([[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>[[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><em>TODO return and do properly with eigenvectors</em></p>
</section>
</section>
<section id="making-the-pagerank-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="making-the-pagerank-algorithm">Making the PageRank algorithm</h2>
<p>The central assumption underpinning page rank is that the importance of a website is related to its links to and from other websites.</p>
<p>We’re trying to build an expression that tells us, based on this network structure, which of these webpages is most relevant to the person who made the search.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/pagerank1.png" class="img-fluid">{:width=“200”}</p>
<p>By mapping all the possible links, we can build a model to estimate the amount of time we would expect Procrastinating Pat to spend on each webpage.</p>
<p>We can describe the links present on page A as a vector, where each row is either a one or a zero based on whether there is a link to the corresponding page. And then normalise the vector by the total number of the links, such that they can be used to describe a probability for that page.</p>
<p><span class="math inline">\(A = \begin{bmatrix}0,&amp;1,&amp;1,&amp;1\end{bmatrix}\)</span> <span class="math inline">\(A\)</span> has a link to each of <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span>, and <span class="math inline">\(D\)</span> but not itsself.</p>
<p>We’ll normalise by a third, as there are three links.</p>
<p>Don’t forget, <span class="math inline">\(L_A\)</span> is a column in our final <span class="math inline">\(L\)</span> matrix</p>
<p><span class="math inline">\(L_A = \begin{bmatrix}0,&amp;\frac{1}{3},&amp;\frac{1}{3},&amp;\frac{1}{3}\end{bmatrix}\)</span></p>
<p><span class="math inline">\(L_B = \begin{bmatrix}\frac{1}{2},&amp;0,&amp;0,&amp;\frac{1}{2}\end{bmatrix}\)</span></p>
<p><span class="math inline">\(L_C = \begin{bmatrix}0,&amp;0,&amp;0,&amp;1\end{bmatrix}\)</span></p>
<p><span class="math inline">\(L = \begin{pmatrix}0&amp;\frac{1}{2}&amp;0&amp;0 \\ \frac{1}{3}&amp;0&amp;0&amp;\frac{1}{2} \\ \frac{1}{3}&amp;0&amp;0&amp;\frac{1}{2} \\ \frac{1}{3}&amp;\frac{1}{2}&amp;1&amp;0\end{pmatrix}\)</span></p>
<p>The only way to get to <span class="math inline">\(A\)</span> is by being at <span class="math inline">\(B\)</span>, which depends on being at <span class="math inline">\(A\)</span> or <span class="math inline">\(D\)</span>. This problem is <em>self-referential</em>, as the ranks on all the pages depend on all the others.</p>
<blockquote class="blockquote">
<p>Although we built our matrix from columns of outward links, we can see that the rows describe inward links normalized with respect to their page of origin.</p>
</blockquote>
<p>The vector <span class="math inline">\(R\)</span> will store the rank of all web pages.</p>
<p>To calculate the rank of page A, you need to know three things about all other pages on the Internet.</p>
<ul>
<li>What’s your rank?</li>
<li>Do you link to page A?</li>
<li>How many outoging links do you have in total?</li>
</ul>
<p><span class="math inline">\(r_A = \sum^n_{j=1} L_{A, j} r{j}\)</span></p>
<p><span class="math inline">\(r_A\)</span> is the sum of all the locations * multiplied by their rank</p>
<p>In the expression above the <span class="math inline">\(\sum\)</span> means from where <span class="math inline">\(J = 1\)</span> to <span class="math inline">\(n\)</span>, <span class="math inline">\(n\)</span> is the number of web pages.</p>
<p>All positions in the link matrix relevent to webpage <span class="math inline">\(A\)</span> at location <span class="math inline">\(j\)</span>, multiplied by the rank at location <span class="math inline">\(j\)</span></p>
<p>The rank of <span class="math inline">\(A\)</span> is the sum of all the pages linked to it, weighted by their specific link probability (taken from <span class="math inline">\(L\)</span>)</p>
<p>We can re-write this as a matrix multiplication;</p>
<p><span class="math inline">\(r = Lr\)</span></p>
<blockquote class="blockquote">
<p>Now as we start off not knowing <span class="math inline">\(r\)</span> we assume that all the ranks are equally and normalise them by the total number of webpages in our analysis, in this case is 4</p>
</blockquote>
<p><span class="math inline">\(r = \begin{pmatrix}\frac{1}{4} \\ \frac{1}{4} \\ \frac{1}{4} \\ \frac{1}{4} \end{pmatrix}\)</span></p>
<p>Each time you multiply r by our matrix L, this gives us an updated value for r</p>
<p><span class="math inline">\(r^{i+1} = Lr^i\)</span></p>
<p>We solve the problem by iterating through the matrix <span class="math inline">\(L\)</span> until <span class="math inline">\(r\)</span> stops changing</p>
<p><span class="math inline">\(r\)</span> ban be thought of as an eigenvector of <span class="math inline">\(L\)</span> with an eigenvalue of 1</p>
<p>We can’t use the diagonal method, as it requires us knowing all the eigenvectors, which is what we’re trying to calculate.</p>
<blockquote class="blockquote">
<p>Although there are many approaches for efficiently calculating eigenvectors that have been developed over the years, repeatedly multiplying a randomly selected initial guest vector by a matrix, which is called <em>the power method</em>, is still very effective</p>
<p>The power method gives you one eigenvector, even though there will be <em>n</em> for an <em>n</em> webpage system, because of how we’ve structured our link matrix <span class="math inline">\(L\)</span>, it will always give you an eigenvector with a value of 1 -&gt; this will be the largest eigenvalue</p>
<p>In an example with lots of web pages, most pages won’t link to one another, there will be lots of 0s in <em>L</em>, this is referred to as a sparse matrix, allowing us to perform very efficient multiplication.</p>
</blockquote>
<p>The <strong>damping factor</strong> <span class="math inline">\(d\)</span> adds an additional form to our iterative formula;</p>
<p><span class="math inline">\(r^{i+1} = d(Lr^i) + \frac{1-d}{n}\)</span></p>
<p><span class="math inline">\(d\)</span> is a value between 0 and 1</p>
<p>You can think of it as 1 minus the probability with which Procrastinating Pat suddenly, randomly types in a web address, rather than clicking on a link on his current page.</p>
</section>
<section id="diving-into-the-pagerank-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="diving-into-the-pagerank-algorithm">Diving into the PageRank algorithm</h2>
<p>The PageRank algorithm is based on an ideal random web surfer who, when reaching a page, goes to the next page by clicking on a link. The surfer has equal probability of clicking any link on the page and, when reaching a page with no links, has equal probability of moving to any other page by typing in its URL. In addition, the surfer may occasionally choose to type in a random URL instead of following the links on a page. The PageRank is the ranked order of the pages from the most to the least probable page the surfer will be viewing.</p>
<p><strong>PageRank as a linear algebra problem</strong> Let’s imagine a micro-internet, with just 6 websites (<strong>A</strong>vocado, <strong>B</strong>ullseye, <strong>C</strong>atBabel, <strong>D</strong>romeda, <strong>e</strong>Tings, and <strong>F</strong>aceSpace). Each website links to some of the others, and this forms a network as shown,</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/pagerank2.png" class="img-fluid">{:width=“400”}</p>
<p>Imagine we have 100 <em>Procrastinating Pat</em>s on our micro-internet, each viewing a single website at a time. Each minute the Pats follow a link on their website to another site on the micro-internet. After a while, the websites that are most linked to will have more Pats visiting them, and in the long run, each minute for every Pat that leaves a website, another will enter keeping the total numbers of Pats on each website constant. The PageRank is simply the ranking of websites by how many Pats they have on them at the end of this process.</p>
<p>We represent the number of Pats on each website with the vector, <span class="math display">\[\mathbf{r} = \begin{bmatrix} r_A \\ r_B \\ r_C \\ r_D \\ r_E \\ r_F \end{bmatrix}\]</span> And say that the number of Pats on each website in minute <span class="math inline">\(i+1\)</span> is related to those at minute <span class="math inline">\(i\)</span> by the matrix transformation</p>
<p><span class="math display">\[ \mathbf{r}^{(i+1)} = L \,\mathbf{r}^{(i)}\]</span></p>
<p>with the matrix <span class="math inline">\(L\)</span> taking the form, <span class="math display">\[ L = \begin{bmatrix}
L_{A→A} &amp; L_{B→A} &amp; L_{C→A} &amp; L_{D→A} &amp; L_{E→A} &amp; L_{F→A} \\
L_{A→B} &amp; L_{B→B} &amp; L_{C→B} &amp; L_{D→B} &amp; L_{E→B} &amp; L_{F→B} \\
L_{A→C} &amp; L_{B→C} &amp; L_{C→C} &amp; L_{D→C} &amp; L_{E→C} &amp; L_{F→C} \\
L_{A→D} &amp; L_{B→D} &amp; L_{C→D} &amp; L_{D→D} &amp; L_{E→D} &amp; L_{F→D} \\
L_{A→E} &amp; L_{B→E} &amp; L_{C→E} &amp; L_{D→E} &amp; L_{E→E} &amp; L_{F→E} \\
L_{A→F} &amp; L_{B→F} &amp; L_{C→F} &amp; L_{D→F} &amp; L_{E→F} &amp; L_{F→F} \\
\end{bmatrix}
\]</span></p>
<p>The columns represent the probability of leaving a website for any other website, and sum to one.</p>
<p>The rows determine how likely you are to enter a website from any other, though these need not add to one.</p>
<p>The long time behaviour of this system is when <span class="math inline">\(\mathbf{r}^{(i+1)} = \mathbf{r}^{(i)}\)</span>, so we’ll drop the superscripts here, and that allows us to write, <span class="math display">\[ L \,\mathbf{r} = \mathbf{r}\]</span></p>
<p>This is an eigenvalue equation for the matrix <span class="math inline">\(L\)</span>, with eigenvalue 1 (this is guaranteed by the probabalistic structure of the matrix <span class="math inline">\(L\)</span>).</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pylab notebook</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.linalg <span class="im">as</span> la</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> readonly.PageRankFunctions <span class="im">import</span> <span class="op">*</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(suppress<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co"># column 1 represents the possibilites of going from A to each other site, and so on</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.array([[<span class="dv">0</span>,   <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>,   <span class="dv">0</span> ],</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">0</span> ],</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">0</span>,   <span class="dv">1</span>, <span class="dv">0</span>,   <span class="dv">1</span><span class="op">/</span><span class="dv">2</span> ],</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>,   <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span> ],</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">0</span>, <span class="dv">0</span>,   <span class="dv">0</span> ],</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>,   <span class="dv">0</span> ]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We could use a linear algebra library, as below. But this gets unmanagable for large systems.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>eVals, eVecs <span class="op">=</span> la.eig(L) <span class="co"># Gets the eigenvalues and vectors</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>order <span class="op">=</span> np.absolute(eVals).argsort()[::<span class="op">-</span><span class="dv">1</span>] <span class="co"># Orders them by their eigenvalues</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>eVals <span class="op">=</span> eVals[order]</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>eVecs <span class="op">=</span> eVecs[:,order]</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> eVecs[:, <span class="dv">0</span>] <span class="co"># Sets r to be the principal eigenvector</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="dv">100</span> <span class="op">*</span> np.real(r <span class="op">/</span> np.<span class="bu">sum</span>(r)) <span class="co"># Make this eigenvector sum to one, then multiply by 100 Procrastinating Pats</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="co"># output</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>array([<span class="fl">16.</span>, <span class="fl">5.333</span>, <span class="dv">40</span>, <span class="fl">25.333</span>, <span class="dv">0</span>, <span class="fl">13.333</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>the PageRank of this micro-internet is: <strong>C</strong>atBabel, <strong>D</strong>romeda, <strong>A</strong>vocado, <strong>F</strong>aceSpace, <strong>B</strong>ullseye, <strong>e</strong>Tings</p>
<p>Since we only care about the principal eigenvector (the one with the largest eigenvalue, which will be 1 in this case), we can use the power iteration method which will scale better, and is faster for large systems.</p>
<p>Let’s now try to get the same result using the Power-Iteration method.</p>
<p>First let’s set up our initial vector, <span class="math inline">\(\mathbf{r}^{(0)}\)</span>, so that we have our 100 Procrastinating Pats equally distributed on each of our 6 websites.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> np.ones(<span class="dv">6</span>) <span class="op">/</span> <span class="dv">6</span> <span class="co"># Sets up this vector (6 entries of 1/6 × 100 each)</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="co"># output</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>array([<span class="fl">16.66666667</span>, </span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    <span class="fl">16.66666667</span>, </span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    <span class="fl">16.66666667</span>,  </span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    <span class="fl">16.66666667</span>, </span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="fl">16.66666667</span>, </span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    <span class="fl">16.66666667</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Update the vector to the next minute, with the matrix L</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> np.ones(<span class="dv">6</span>) <span class="op">/</span> <span class="dv">6</span> <span class="co"># Sets up this vector (6 entries of 1/6 × 100 each)</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="dv">100</span>) : <span class="co"># Repeat 100 times</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> L <span class="op">@</span> r</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co"># output</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>array([<span class="fl">16.</span>, <span class="fl">5.333</span>, <span class="dv">40</span>, <span class="fl">25.333</span>, <span class="dv">0</span>, <span class="fl">13.333</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Even better, we can keep running until we get to the required tolerance.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> np.ones(<span class="dv">6</span>) <span class="op">/</span> <span class="dv">6</span> <span class="co"># Sets up this vector (6 entries of 1/6 × 100 each)</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>lastR <span class="op">=</span> r</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> L <span class="op">@</span> r</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> la.norm(lastR <span class="op">-</span> r) <span class="op">&gt;</span> <span class="fl">0.01</span> :</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    lastR <span class="op">=</span> r</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> L <span class="op">@</span> r</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">str</span>(i) <span class="op">+</span> <span class="st">" iterations to convergence."</span>)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="co"># output</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 18 iterations to convergence.</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>array([<span class="fl">16.</span>, <span class="fl">5.333</span>, <span class="dv">40</span>, <span class="fl">25.333</span>, <span class="dv">0</span>, <span class="fl">13.333</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Damping Parameter</strong></p>
<p>Say a new website is added to the micro-internet: <em>Geoff’s</em> Website.</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/pagerank3.png" class="img-fluid">{:width=“400”}</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>L2 <span class="op">=</span> np.array([[<span class="dv">0</span>,   <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>,   <span class="dv">0</span>, <span class="dv">0</span> ],</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>               [<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span> ],</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>               [<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">0</span>,   <span class="dv">1</span>, <span class="dv">0</span>,   <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span> ],</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>               [<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>,   <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span> ],</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>               [<span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">0</span>, <span class="dv">0</span>,   <span class="dv">0</span>, <span class="dv">0</span> ],</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>               [<span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>,   <span class="dv">0</span>, <span class="dv">0</span> ],</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>               [<span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">0</span>, <span class="dv">0</span>,   <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span> ]])</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> np.ones(<span class="dv">7</span>) <span class="op">/</span> <span class="dv">7</span> <span class="co"># Sets up this vector (6 entries of 1/6 × 100 each)</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>lastR <span class="op">=</span> r</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> L2 <span class="op">@</span> r</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> la.norm(lastR <span class="op">-</span> r) <span class="op">&gt;</span> <span class="fl">0.01</span> :</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    lastR <span class="op">=</span> r</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> L2 <span class="op">@</span> r</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>    i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">str</span>(i) <span class="op">+</span> <span class="st">" iterations to convergence."</span>)</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="co"># output</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 131 iterations to convergence.</span></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>array([<span class="fl">0.03046998</span>, <span class="fl">0.01064323</span>, <span class="fl">0.07126612</span>, <span class="fl">0.04423198</span>, <span class="dv">1</span>, <span class="fl">0.02489342</span>, <span class="fl">99.81849527</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This behaviour can be understood, because once a Pat get’s to Geoff’s Website, they can’t leave, as all links head back to Geoff.</p>
<p>To combat this, we can add a small probability that the Procrastinating Pats don’t follow any link on a webpage, but instead visit a website on the micro-internet at random.</p>
<p>We’ll say the probability of them following a link is <span class="math inline">\(d\)</span> and the probability of choosing a random website is therefore <span class="math inline">\(1-d\)</span>. We can use a new matrix to work out where the Pat’s visit each minute. <span class="math display">\[ M = d \, L + \frac{1-d}{n} \, J \]</span> where <span class="math inline">\(J\)</span> is an <span class="math inline">\(n\times n\)</span> matrix where every element is one.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> <span class="fl">0.5</span> <span class="co"># Feel free to play with this parameter after running the code once.</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> d <span class="op">*</span> L2 <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>d)<span class="op">/</span><span class="dv">7</span> <span class="op">*</span> np.ones([<span class="dv">7</span>, <span class="dv">7</span>]) <span class="co"># np.ones() is the J matrix, with ones for each entry.</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> np.ones(<span class="dv">7</span>) <span class="op">/</span> <span class="dv">7</span> <span class="co"># Sets up this vector (6 entries of 1/6 × 100 each)</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>lastR <span class="op">=</span> r</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> M <span class="op">@</span> r</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> la.norm(lastR <span class="op">-</span> r) <span class="op">&gt;</span> <span class="fl">0.01</span> :</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>    lastR <span class="op">=</span> r</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> M <span class="op">@</span> r</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">str</span>(i) <span class="op">+</span> <span class="st">" iterations to convergence."</span>)</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="co"># output</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 8 iterations to convergence.</span></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>array([ <span class="fl">13.68217054</span>, <span class="fl">11.20902965</span>, <span class="fl">22.41964343</span>,  <span class="fl">16.7593433</span>, <span class="fl">7.14285714</span>, <span class="fl">10.87976354</span>, <span class="fl">17.90719239</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="produce-a-function-that-can-calculate-the-pagerank-for-an-arbitrarily-large-probability-matrix" class="level3">
<h3 class="anchored" data-anchor-id="produce-a-function-that-can-calculate-the-pagerank-for-an-arbitrarily-large-probability-matrix">Produce a function that can calculate the PageRank for an arbitrarily large probability matrix</h3>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.linalg <span class="im">as</span> la</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> readonly.PageRankFunctions <span class="im">import</span> <span class="op">*</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(suppress<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pageRank(linkMatrix, d) :</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> linkMatrix.shape[<span class="dv">0</span>]</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> d <span class="op">*</span> linkMatrix <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>d)<span class="op">/</span>n <span class="op">*</span> np.ones([n, n])</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> np.ones(n) <span class="op">/</span> n</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    lastR <span class="op">=</span> r</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> M <span class="op">@</span> r</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> la.norm(lastR <span class="op">-</span> r) <span class="op">&gt;</span> <span class="fl">0.01</span> :</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>        lastR <span class="op">=</span> r</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> M <span class="op">@</span> r</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> r</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="eigenvalues-and-eigenvectors-practice" class="level3">
<h3 class="anchored" data-anchor-id="eigenvalues-and-eigenvectors-practice">Eigenvalues and eigenvectors practice</h3>
<p><code>Numpy</code> can calculate eigenvectors and eigenvalues</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> np.array([[ <span class="dv">4</span>, <span class="op">-</span><span class="dv">5</span>,<span class="dv">6</span>],</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>       [<span class="dv">7</span>, <span class="op">-</span><span class="dv">8</span>, <span class="dv">6</span>],</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>       [<span class="dv">3</span><span class="op">/</span><span class="dv">2</span>,  <span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>]])</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>vals, vecs <span class="op">=</span> np.linalg.eig(M)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>vecs</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="co"># output</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>[[ <span class="fl">3.</span> <span class="op">-</span><span class="fl">2.</span>  <span class="fl">1.</span>]</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a> [ <span class="fl">3.</span> <span class="op">-</span><span class="fl">2.</span> <span class="op">-</span><span class="fl">1.</span>]</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a> [ <span class="fl">1.</span>  <span class="fl">1.</span> <span class="op">-</span><span class="fl">2.</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Q1</strong> Select all the eigenvectors of the matrix above;</p>
<p><span class="math inline">\(\begin{bmatrix} 1/2 \\ -1/2 \\ -1\end{bmatrix}\)</span>, corresponds to the third column in the output</p>
<p><span class="math inline">\(\begin{bmatrix}-3 \\ -3 \\ -1\end{bmatrix}\)</span>, corresponds to the first column in the output</p>
<p><span class="math inline">\(\begin{bmatrix}-2/\sqrt{9} \\ -2/\sqrt{9} \\ 1/\sqrt{9}\end{bmatrix}\)</span>, corresponds to the second column in the output</p>
<p><strong>Q2</strong> In PageRank, we care about the eigenvector of the link matrix, <span class="math inline">\(L\)</span>, that has eigenvalue 1, and that we can find this using power iteration method as this will be the largest eigenvalue.</p>
<p>PageRank can sometimes get into trouble if closed-loop structures appear. A simplified example might look like this,</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/pagerank4.png" class="img-fluid">{:width=“200”}</p>
<p>Therefore the <span class="math inline">\(L\)</span> matrix will look like this;</p>
<p><span class="math inline">\(L=\left[\begin{array}{llll} 0 &amp; 0 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{array}\right]\)</span></p>
<p>What might be going wrong?</p>
<ol type="1">
<li><p>Because of the loop, _Procrastinating Pats that are browsing will go around in a cycle rather than settling on a webpage. The system will never converge using the power iteration method.</p></li>
<li><p>Other eigenvalues are not small compated to 1, and so do not decay away with each power iteration. The other eigenvectors in fact have the same size as 1 (they are <span class="math inline">\(-1, i, -i\)</span>)</p></li>
</ol>
<p>What we can do to overcome this, is to add damping</p>
<p><span class="math inline">\(L^{\prime}=\left[\begin{array}{cccc} 0.1 &amp; 0.1 &amp; 0.1 &amp; 0.7 \\ 0.7 &amp; 0.1 &amp; 0.1 &amp; 0.1 \\ 0.1 &amp; 0.7 &amp; 0.1 &amp; 0.1 \\ 0.1 &amp; 0.1 &amp; 0.7 &amp; 0.1 \end{array}\right]\)</span></p>
<p>There is now a probability to move to any website. Pats are no longer constrained by the loop.</p>
<p><em>The other eigen values get smaller</em></p>
<p>Another issue that arises, is when parts of the system are decoupled.</p>
<p>For example;</p>
<p><img src="{{ site.baseurl }}/images/linear_algebra/pagerank5.png" class="img-fluid">{:width=“200”}</p>
<p>With a link matrix;</p>
<p><span class="math inline">\(L=\left[\begin{array}{llll} 0 &amp; 1 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{array}\right]\)</span></p>
<p>This form is known as block diagonal, as it can be split into square blocks along the main diagonal, i.e.</p>
<p><span class="math inline">\(L= \begin{bmatrix}A &amp; 0 \\ 0 &amp; B\end{bmatrix}\)</span></p>
<p>With <span class="math inline">\(A = B = \begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0\end{bmatrix}\)</span> in this case.</p>
<p>In this system, there are two loops <span class="math inline">\((A \rightleftarrows B)\)</span> and <span class="math inline">\((C \rightleftarrows D)\)</span>.</p>
<p>As the system is disconnected there will not be a unique PageRank. There are two eigenvalues of 1.</p>
<p>The eigensystem is degenerate. Any linear combination of eigenvectors with the same eigenvalue is also an eigenvector.</p>
<p>The power iteration algorithm could settle on multiple values, depending on its starting conditions.</p>
<p>Damping in this setup will produce only one eigenvalue of 1 and PageRank will settle to its’ eigenvector.</p>
<hr>
<p>Good note on the differences between a vecotr, matrix and a tensor <a href="https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c#:~:text=The%20basic%20idea%2C%20though%2C%20is,of%20as%20a%20generalized%20matrix.&amp;text=Any%20rank%2D2%20tensor%20can,really%20a%20rank%2D2%20tensor.">here</a></p>
</section>
</section>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><section id="refs" class="level2">
<h2 class="anchored" data-anchor-id="refs">Refs</h2>
<p>Imperial’s “Mathematics for Machine Learning: Linear Algebra” on <a href="https://www.coursera.org/learn/linear-algebra-machine-learning/">Coursera</a></p>
<p>Joel Grus’ “Data Science from Scratch” <a href="https://github.com/joelgrus/data-science-from-scratch">Github</a></p>
<p>3Blue1Brown’s essence of linear algebra <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Youtube</a></p>
<p>Standford’s CS229 <a href="http://cs229.stanford.edu/summer2019/cs229-linalg.pdf">pdf</a></p>


</section></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>